<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Build CephFS Kernel Module with Latest Codes]]></title>
    <url>%2F2018%2F12%2F03%2Fbuild-cephfs-kernel-module-with-latest-codes%2F</url>
    <content type="text"><![CDATA[概述使用CephFS的时候，若需要获取较高的性能，kernel client是一定会使用到的，但它的每次更新都需要升级Linux内核，这样影响比较大，很多时候是不能接受的。 其实我们可以通过拉取Linux对应版本的源码，把最新的CephFS kernel代码merge进当前系统对应版本的源码上，然后进行编译，成功后替换CephFS模块。 这种方法要求两个Linux Kernel Version相差不要太大，不然代码的merge会很痛苦的，也很容易出错！ 编译安装CephFS模块下载内核源码比如当前Linux的最新稳定版本为：4.19.1，我们使用CephFS kernel client的内核版本为：4.18.5。 则通过wget下载4.18.5和4.19.1内核代码： 12# wget https://mirrors.edge.kernel.org/pub/linux/kernel/v4.x/linux-4.18.5.tar.xz# wget https://mirrors.edge.kernel.org/pub/linux/kernel/v4.x/linux-4.19.1.tar.xz 解压缩12root@ceph1:~# tar -xvf linux-4.18.5.tar.xzroot@ceph1:~# tar -xvf linux-4.19.1.tar.xz 编译根据当前机器的版本信息，修改源码的Makefile文件，并执行编译：1234567891011root@ceph1:~# uname -aLinux ceph1 4.18.5-041805-generic #201808241320 SMP Fri Aug 24 13:22:12 UTC 2018 x86_64 x86_64 x86_64 GNU/Linuxroot@ceph1:~# cd linux-4.18.5root@ceph1:~/linux-4.18.5# vim Makefile...VERSION = 4PATCHLEVEL = 18SUBLEVEL = 5EXTRAVERSION = -041805-genericroot@ceph1:~/linux-4.18.5# make menuconfigroot@ceph1:~/linux-4.18.5# make 上述命令或花费数个小时，请通过screen来执行。 注：根据make的输出安装依赖的packages 以后修改Ceph模块代码后，对于Ceph模块的编译，只需要执行如下命令： 1root@ceph1:~/linux-4.18.5# make m=fs/ceph 安装编译后Ceph相关的模块主要有如下几个，通过insmod安装即可： 123456789root@ceph1:~/linux-4.18.5# insmod net/ceph/libceph.koroot@ceph1:~/linux-4.18.5# insmod drivers/block/rbd.koroot@ceph1:~/linux-4.18.5# insmod fs/fscache/fscache.koroot@ceph1:~/linux-4.18.5# insmod fs/ceph/ceph.koroot@ceph1:~/linux-4.18.5# lsmod | grep cephceph 380928 0fscache 364544 1 cephlibceph 315392 2 ceph,rbdlibcrc32c 16384 5 nf_conntrack,nf_nat,btrfs,raid456,libceph 注：若机器本来已经安装了ceph模块，依据上述安装module的逆序卸载即可(rmmod命令) 代码合并Linux源码中，与ceph相关的代码有以下几个位置： 1234fs/ceph/net/ceph/drivers/block/rbd*include/linux/ceph/ 若要使用较新内核版本里的Ceph代码在4.18.5内核上编译模块，需要把对应上面的所有Ceph代码copy替换掉，然后依据上面的编译步骤开始编译； 可以在如下的链接里查看内核4.19.1和4.18.5的代码修改： v4.19.1 diff with v4.18.5 可以看出里面ceph相关代码有很大的变动！ 注意事项： 因为不同内核版本里的函数名称、接口和代码都可能变化，要依据编译出错和提示修改替换后的代码，使之能够在4.18.5内核上编译通过 编译成功后，使用新的ceph module替换4.18.5里的ceph module即可，要在测试机器上先测试一下，因为很可能会有kernel panic]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>cephfs</tag>
        <tag>kernel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ceph Kernel module dynamic debug]]></title>
    <url>%2F2018%2F11%2F08%2Fceph-kernel-module-dynamic-debug%2F</url>
    <content type="text"><![CDATA[概述在使用CephFS的kernel client后，发现它对比ceph-fuse客户端，不方便找到对应的log了，在dmesg或kern.log里也仅有很少的log输出，这在遇到问题的时候很不方便，需要找到打印Ceph kernel log的方法。 代码定义Ceph的kernel module有两个，如下：12345root@ceph2:~# lsmod | grep cephceph 385024 0libceph 315392 1 cephfscache 368640 1 cephlibcrc32c 16384 8 nf_conntrack,nf_nat,dm_persistent_data,btrfs,xfs,raid456,libceph,ip_vs ceph：CephFS kernel client模块 libceph：Ceph kernel网络通信模块 在Ceph kernel的代码里，有两类log输出机制： pr_debug 输出分为 info / warning / error等好多种，调用 printk 输出； dout 输出Kernel里对应Dynamic Debug； pr_debug通过调用 printk 的指定log级别输出； 文件：include/linux/printk.h123456789101112131415161718192021/* * These can be used to print at the various log levels. * All of these will print unconditionally, although note that pr_debug() * and other debug macros are compiled out unless either DEBUG is defined * or CONFIG_DYNAMIC_DEBUG is set. */#define pr_emerg(fmt, ...) \ printk(KERN_EMERG pr_fmt(fmt), ##__VA_ARGS__)#define pr_alert(fmt, ...) \ printk(KERN_ALERT pr_fmt(fmt), ##__VA_ARGS__)#define pr_crit(fmt, ...) \ printk(KERN_CRIT pr_fmt(fmt), ##__VA_ARGS__)#define pr_err(fmt, ...) \ printk(KERN_ERR pr_fmt(fmt), ##__VA_ARGS__)#define pr_warning(fmt, ...) \ printk(KERN_WARNING pr_fmt(fmt), ##__VA_ARGS__)#define pr_warn pr_warning#define pr_notice(fmt, ...) \ printk(KERN_NOTICE pr_fmt(fmt), ##__VA_ARGS__)#define pr_info(fmt, ...) \ printk(KERN_INFO pr_fmt(fmt), ##__VA_ARGS__) doutceph kernel代码里log大部分使用dout输出，它的定义如下： 文件：include/linux/ceph/ceph_debug.h123456789101112131415161718192021222324252627282930313233343536373839/* SPDX-License-Identifier: GPL-2.0 */#ifndef _FS_CEPH_DEBUG_H#define _FS_CEPH_DEBUG_H#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt#include &lt;linux/string.h&gt;#ifdef CONFIG_CEPH_LIB_PRETTYDEBUG/* * wrap pr_debug to include a filename:lineno prefix on each line. * this incurs some overhead (kernel size and execution time) due to * the extra function call at each call site. */# if defined(DEBUG) || defined(CONFIG_DYNAMIC_DEBUG)# define dout(fmt, ...) \ pr_debug("%.*s %12.12s:%-4d : " fmt, \ 8 - (int)sizeof(KBUILD_MODNAME), " ", \ kbasename(__FILE__), __LINE__, ##__VA_ARGS__)# else/* faux printk call just to see any compiler warnings. */# define dout(fmt, ...) do &#123; \ if (0) \ printk(KERN_DEBUG fmt, ##__VA_ARGS__); \ &#125; while (0)# endif#else/* * or, just wrap pr_debug */# define dout(fmt, ...) pr_debug(" " fmt, ##__VA_ARGS__)#endif#endif 从上面可以看出，如果内核支持了 CONFIG_DYNAMIC_DEBUG，那我们就可以动态获取到这部分debug输出了； 系统Dynamic Debug支持在机器上检查kernel是不是支持 CONFIG_DYNAMIC_DEBUG： 1234root@ceph2:/boot# uname -a Linux ceph2 4.18.5-041805-generic #201808241320 SMP Fri Aug 24 13:22:12 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux root@ceph2:/boot# grep CONFIG_DYNAMIC_DEBUG config-4.18.5-041805-generic CONFIG_DYNAMIC_DEBUG=y 如果输出如上述，则表示使用的linux kernel支持了CONFIG_DYNAMIC_DEBUG，那就可以动态获取Ceph kernel module的输出了； 获取debugfs的目录：1234567root@ceph2:~# mount | grep debugfs debugfs on /sys/kernel/debug type debugfs (rw,relatime) root@ceph2:~# cd /sys/kernel/debug/ root@ceph2:/sys/kernel/debug# cd dynamic_debug/ root@ceph2:/sys/kernel/debug/dynamic_debug# ls control 查看对应Ceph都有哪些Dynamic Debug支持：12345678910root@ceph2:/sys/kernel/debug/dynamic_debug# cat control | grep ceph | wc -l 975 root@ceph2:/sys/kernel/debug/dynamic_debug# cat control | grep ceph | less net/ceph/ceph_common.c:787 [libceph]exit_ceph_lib =_ " exit_ceph_lib\012" net/ceph/ceph_common.c:739 [libceph]ceph_open_session =_ " open_session start\012” ...fs/ceph/cache.c:212 [ceph]ceph_fscache_file_set_cookie =_ " fscache_file_set_cookie %p %p enabling cache\012" fs/ceph/cache.c:204 [ceph]ceph_fscache_file_set_cookie =_ " fscache_file_set_cookie %p %p disabling cache\012" fs/ceph/cache.c:138 [ceph]ceph_fscache_inode_check_aux =_ " ceph inode 0x%p cached okay\012" Dynamic Debug介绍模式分类Linux Kernel里的Dynamic Debug支持好几种配置模式，介绍如下： 1、func输出指定函数的log12echo -n 'func xxx +p' &gt; /sys/kernel/debug/dynamic_debug/control echo -n 'func xxx -p' &gt; /sys/kernel/debug/dynamic_debug/control 比如：ceph_open_session，则会打印这个函数内的所有log 2、file输出指定文件的log12echo -n 'file xxx.c +p' &gt; /sys/kernel/debug/dynamic_debug/control echo -n 'file xxx.c -p' &gt; /sys/kernel/debug/dynamic_debug/control 比如：ceph_common.c，则会打印该文件内的所有log 3、module输出指定模块的log12echo -n 'module xxx.c +p' &gt; /sys/kernel/debug/dynamic_debug/control echo -n 'module xxx.c -p' &gt; /sys/kernel/debug/dynamic_debug/control 比如：libceph/ceph，则会打印libceph或ceph模块的所有log 当前ceph相关的就这两个模块； 4、format输出符合指定格式的log12echo -n 'format xxx +p' &gt; /sys/kernel/debug/dynamic_debug/control echo -n 'format xxx -p' &gt; /sys/kernel/debug/dynamic_debug/control 比如：get_session，则只会打印匹配这个字符串的log 5、line输出指定行号的log12echo -n 'file xxx line xxx +p' &gt; /sys/kernel/debug/dynamic_debug/control echo -n 'file xxx line xxx -p' &gt; /sys/kernel/debug/dynamic_debug/control 比如：100-200，则只会打印指定文件里100行-200行内的log 不同line指定的含义介绍如下：1234line 1603 // exactly line 1603 line 1600-1605 // the six lines from line 1600 to line 1605 line -1605 // the 1605 lines from line 1 to line 1605 line 1600- // all lines from line 1600 to the end of the file Flag说明上述+-号含义如下： 123\- remove the given flags \+ add the given flags = set the flags to the given flags 支持的flag有：123456p enables the pr_debug() callsite. f Include the function name in the printed message l Include line number in the printed message m Include module name in the printed message t Include thread ID in messages not generated from interrupt context _ No flags are set. (Or'd with others on input) 自动配置Dynamic Debug如何自动配置dynamic debug？ 通常有三种办法： 1、添加模块加载的配置2、添加boot args，修改/proc/cmdline3、传递参数给modprobe 下面介绍通过模块加载时候指定参数的办法： 在/etc/modprobe.d/目录创建 .conf 文件，然后在里面添加dyndbg配置： 如下所示：12root@ceph2:/etc/modprobe.d# cat ceph.conf options ceph dyndbg="format get_session +p" 之后就可以通过dmesg或者在kern.log里查看ceph kernel module的log了 ：） 测试如下： 123456789root@ceph2:/etc/modprobe.d# rmmod ceph root@ceph2:/etc/modprobe.d# modprobe ceph root@ceph2:/etc/modprobe.d# mount -t ceph 100.64.0.21,100.64.0.22,100.64.0.26:/ /mnt/cephfs root@ceph2:/etc/modprobe.d# dmesg -T | tail -n 5 [Thu Oct 25 19:54:13 2018] ceph: mdsc get_session 0000000046b3700b 2 -&gt; 3 [Thu Oct 25 19:54:13 2018] ceph: mdsc get_session 0000000096e295d5 1 -&gt; 2 [Thu Oct 25 19:54:13 2018] ceph: mdsc get_session 0000000096e295d5 2 -&gt; 3 [Thu Oct 25 19:54:13 2018] ceph: mdsc get_session 0000000046b3700b 1 -&gt; 2 [Thu Oct 25 19:54:13 2018] ceph: mdsc get_session 0000000046b3700b 2 -&gt; 3 参考文章https://lwn.net/Articles/434856/https://www.kernel.org/doc/html/v4.18/admin-guide/dynamic-debug-howto.html]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>cephfs</tag>
        <tag>kernel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ceph Bluestore RocksDB Analyse]]></title>
    <url>%2F2018%2F10%2F25%2Fceph-bluestore-rocksdb-analyse%2F</url>
    <content type="text"><![CDATA[概述对于Ceph全新的存储引擎BlueStore来说，RocksDB的意义很大，它存储了BlueStore相关的元数据信息，对它的理解有助于更好的理解BlueStore的实现，分析之后遇到的问题； BlueStore架构BlueStore的架构图如下，还是被广泛使用的一张： 如上图所示，BlueStore的几个关键组件中，RocksDB对接了BlueStore的metadata信息，本文抛开别的组件，详细描述RocksDB在这里存储的信息以及其实现； BlueStore结构体定义Ceph里BlueStore的定义和主要数据成员如下： 123456789101112131415161718192021222324class BlueStore : public ObjectStore, public md_config_obs_t &#123;...private: BlueFS *bluefs = nullptr; unsigned bluefs_shared_bdev = 0; ///&lt; which bluefs bdev we are sharing bool bluefs_single_shared_device = true; utime_t bluefs_last_balance; KeyValueDB *db = nullptr; BlockDevice *bdev = nullptr; std::string freelist_type; FreelistManager *fm = nullptr; Allocator *alloc = nullptr; uuid_d fsid; int path_fd = -1; ///&lt; open handle to $path int fsid_fd = -1; ///&lt; open handle (locked) to $path/fsid bool mounted = false; vector&lt;Cache*&gt; cache_shards; std::mutex osr_lock; ///&lt; protect osd_set std::set&lt;OpSequencerRef&gt; osr_set; ///&lt; set of all OpSequencers...&#125;; 几个关键的数据成员如下： 1) BlueFS定义：BlueFS *bluefs = nullptr; 支持RocksDB的定制FS，只实现了RocksEnv需要的API接口； 代码里在_open_db()里对其初始化： 1234567int BlueStore::_open_db(bool create)&#123; rocksdb::Env *env = NULL; if (do_bluefs) &#123; bluefs = new BlueFS(cct); &#125;&#125; 2) RocksDB定义：KeyValueDB *db = nullptr; 在BlueStore的元数据和OMap都通过DB存储，这里使用的是RocksDB，它的初始化也是在_open_db()函数中： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687int BlueStore::_open_db(bool create)&#123; // 获取kv的后端设备 string kv_backend; if (create) &#123; kv_backend = cct-&gt;_conf-&gt;bluestore_kvbackend; &#125; else &#123; r = read_meta("kv_backend", &amp;kv_backend); &#125; // mkfs也会调用这里，create时候根据配置做bluefs的创建 if (create) &#123; do_bluefs = cct-&gt;_conf-&gt;bluestore_bluefs; &#125; else &#123; string s; r = read_meta("bluefs", &amp;s); &#125; rocksdb::Env *env = NULL; // 创建bluefs if (do_bluefs) &#123; bluefs = new BlueFS(cct); bfn = path + "/block.db"; if (::stat(bfn.c_str(), &amp;st) == 0) &#123; r = bluefs-&gt;add_block_device(BlueFS::BDEV_DB, bfn); if (bluefs-&gt;bdev_support_label(BlueFS::BDEV_DB)) &#123; r = _check_or_set_bdev_label( bfn, bluefs-&gt;get_block_device_size(BlueFS::BDEV_DB), "bluefs db", create); &#125; if (create) &#123; bluefs-&gt;add_block_extent( BlueFS::BDEV_DB, SUPER_RESERVED, bluefs-&gt;get_block_device_size(BlueFS::BDEV_DB) - SUPER_RESERVED); &#125; bluefs_shared_bdev = BlueFS::BDEV_SLOW; bluefs_single_shared_device = false; &#125; else &#123; if (::lstat(bfn.c_str(), &amp;st) == -1) &#123; bluefs_shared_bdev = BlueFS::BDEV_DB; &#125; &#125; // shared device bfn = path + "/block"; r = bluefs-&gt;add_block_device(bluefs_shared_bdev, bfn); bfn = path + "/block.wal"; if (::stat(bfn.c_str(), &amp;st) == 0) &#123; r = bluefs-&gt;add_block_device(BlueFS::BDEV_WAL, bfn); if (bluefs-&gt;bdev_support_label(BlueFS::BDEV_WAL)) &#123; r = _check_or_set_bdev_label( bfn, bluefs-&gt;get_block_device_size(BlueFS::BDEV_WAL), "bluefs wal", create); &#125; if (create) &#123; bluefs-&gt;add_block_extent( BlueFS::BDEV_WAL, BDEV_LABEL_BLOCK_SIZE, bluefs-&gt;get_block_device_size(BlueFS::BDEV_WAL) - BDEV_LABEL_BLOCK_SIZE); &#125; cct-&gt;_conf-&gt;set_val("rocksdb_separate_wal_dir", "true"); bluefs_single_shared_device = false; &#125; &#125; // 创建RocksDB db = KeyValueDB::create(cct, kv_backend, fn, static_cast&lt;void*&gt;(env)); FreelistManager::setup_merge_operators(db); db-&gt;set_merge_operator(PREFIX_STAT, merge_op); db-&gt;set_cache_size(cache_size * cache_kv_ratio); if (kv_backend == "rocksdb") options = cct-&gt;_conf-&gt;bluestore_rocksdb_options; db-&gt;init(options); if (create) r = db-&gt;create_and_open(err); else r = db-&gt;open(err);&#125; 3) BlockDevice定义：BlockDevice *bdev = nullptr; 底层存储BlueStore Data / db / wal的块设备，有如下几种： KernelDevice NVMEDevice PMEMDevice 代码中对其初始化如下： 123456789101112131415161718int BlueStore::_open_bdev(bool create)&#123; string p = path + "/block"; bdev = BlockDevice::create(cct, p, aio_cb, static_cast&lt;void*&gt;(this)); int r = bdev-&gt;open(p); if (bdev-&gt;supported_bdev_label()) &#123; r = _check_or_set_bdev_label(p, bdev-&gt;get_size(), "main", create); &#125; // initialize global block parameters block_size = bdev-&gt;get_block_size(); block_mask = ~(block_size - 1); block_size_order = ctz(block_size); r = _set_cache_sizes(); return 0;&#125; 4) FreelistManager定义：FreelistManager *fm = nullptr; 管理BlueStore里空闲blob的； 默认使用的是：BitmapFreelistManager 1234int BlueStore::_open_fm(bool create)&#123; fm = FreelistManager::create(cct, freelist_type, db, PREFIX_ALLOC); int r = fm-&gt;init(bdev-&gt;get_size());&#125; 5) Allocator定义：Allocator *alloc = nullptr; BlueStore的blob分配器，支持如下几种： BitmapAllocator StupidAllocator 默认使用的是 StupidAllocator； 6) 总结：BlueStore的mount过程在BlueStore的 mount过程中，会调用各个函数来初始化其使用的各个组件，顺序如下： 123456789101112131415161718192021222324252627282930313233int BlueStore::_mount(bool kv_only)&#123; int r = read_meta("type", &amp;type); if (type != "bluestore") &#123; return -EIO; &#125; ... int r = _open_path(); r = _open_fsid(false); r = _read_fsid(&amp;fsid); r = _lock_fsid(); r = _open_bdev(false); r = _open_db(false); if (kv_only) return 0; r = _open_super_meta(); r = _open_fm(false); r = _open_alloc(); r = _open_collections(); r = _reload_logger(); if (bluefs) &#123; r = _reconcile_bluefs_freespace(); &#125; _kv_start(); r = _deferred_replay(); mempool_thread.init(); mounted = true; return 0;&#125; RocksDB的定义RocksDB的定义如下，基于KeyValueDB实现接口： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576/** * Uses RocksDB to implement the KeyValueDB interface */class RocksDBStore : public KeyValueDB &#123;... string path; void *priv; rocksdb::DB *db; rocksdb::Env *env; std::shared_ptr&lt;rocksdb::Statistics&gt; dbstats; rocksdb::BlockBasedTableOptions bbt_opts; string options_str; uint64_t cache_size = 0;... // manage async compactions Mutex compact_queue_lock; Cond compact_queue_cond; list&lt; pair&lt;string,string&gt; &gt; compact_queue; bool compact_queue_stop; class CompactThread : public Thread &#123; RocksDBStore *db; public: explicit CompactThread(RocksDBStore *d) : db(d) &#123;&#125; void *entry() override &#123; db-&gt;compact_thread_entry(); return NULL; &#125; friend class RocksDBStore; &#125; compact_thread; ... struct RocksWBHandler: public rocksdb::WriteBatch::Handler &#123; std::string seen ; int num_seen = 0; &#125;; class RocksDBTransactionImpl : public KeyValueDB::TransactionImpl &#123; public: rocksdb::WriteBatch bat; RocksDBStore *db; &#125;; // DB Iterator的具体实现，比较重要 class RocksDBWholeSpaceIteratorImpl : public KeyValueDB::WholeSpaceIteratorImpl &#123; protected: rocksdb::Iterator *dbiter; public: explicit RocksDBWholeSpaceIteratorImpl(rocksdb::Iterator *iter) : dbiter(iter) &#123; &#125; //virtual ~RocksDBWholeSpaceIteratorImpl() &#123; &#125; ~RocksDBWholeSpaceIteratorImpl() override; int seek_to_first() override; int seek_to_first(const string &amp;prefix) override; int seek_to_last() override; int seek_to_last(const string &amp;prefix) override; int upper_bound(const string &amp;prefix, const string &amp;after) override; int lower_bound(const string &amp;prefix, const string &amp;to) override; bool valid() override; int next() override; int prev() override; string key() override; pair&lt;string,string&gt; raw_key() override; bool raw_key_is_prefixed(const string &amp;prefix) override; bufferlist value() override; bufferptr value_as_ptr() override; int status() override; size_t key_size() override; size_t value_size() override; &#125;;...&#125;; 基类 KeyValueDB 的定义如下，只罗列了几个关键的基类定义： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * Defines virtual interface to be implemented by key value store * * Kyoto Cabinet or LevelDB should implement this */class KeyValueDB &#123;public: class TransactionImpl &#123; ... &#125;; typedef ceph::shared_ptr&lt; TransactionImpl &gt; Transaction; class WholeSpaceIteratorImpl &#123; ... &#125;; typedef ceph::shared_ptr&lt; WholeSpaceIteratorImpl &gt; WholeSpaceIterator; class IteratorImpl : public GenericIteratorImpl &#123; const std::string prefix; WholeSpaceIterator generic_iter; ... int seek_to_first() override &#123; return generic_iter-&gt;seek_to_first(prefix); &#125; int seek_to_last() &#123; return generic_iter-&gt;seek_to_last(prefix); &#125; int upper_bound(const std::string &amp;after) override &#123; return generic_iter-&gt;upper_bound(prefix, after); &#125; int lower_bound(const std::string &amp;to) override &#123; return generic_iter-&gt;lower_bound(prefix, to); &#125; bool valid() override &#123; if (!generic_iter-&gt;valid()) return false; return generic_iter-&gt;raw_key_is_prefixed(prefix); &#125; &#125;; typedef ceph::shared_ptr&lt; IteratorImpl &gt; Iterator; WholeSpaceIterator get_iterator() &#123; return _get_iterator(); &#125; Iterator get_iterator(const std::string &amp;prefix) &#123; return std::make_shared&lt;IteratorImpl&gt;(prefix, get_iterator()); &#125;&#125;; 在代码中，使用RocksDB的常用方法如下： 12345678KeyValueDB::Iterator it;it = db-&gt;get_iterator(PREFIX_OBJ); // 设置key的前缀it-&gt;lower_bound(key); / it-&gt;upper_bound(key); // 找到对应key的iterator位置while (it-&gt;valid()) &#123; // 检查iterator是否有效 ... it-&gt;key() / it-&gt;value();; // 获取iterator对应的key或value it-&gt;next(); // 获取下一个iterator位置&#125; RocksDB里KV分类BlueStore里所有的kv数据都可以存储在RocksDB里，实现中通过数据的前缀分类，如下： 123456789// kv store prefixesconst string PREFIX_SUPER = "S"; // field -&gt; valueconst string PREFIX_STAT = "T"; // field -&gt; value(int64 array)const string PREFIX_COLL = "C"; // collection name -&gt; cnode_tconst string PREFIX_OBJ = "O"; // object name -&gt; onode_tconst string PREFIX_OMAP = "M"; // u64 + keyname -&gt; valueconst string PREFIX_DEFERRED = "L"; // id -&gt; deferred_transaction_tconst string PREFIX_ALLOC = "B"; // u64 offset -&gt; u64 length (freelist)const string PREFIX_SHARED_BLOB = "X"; // u64 offset -&gt; shared_blob_t 下面针对每一类前缀做详细介绍： 1) PREFIX_SUPERBlueStore的超级块信息，里面BlueStore自身的元数据信息，比如： 1234567S blobid_maxS bluefs_extentsS freelist_typeS min_alloc_sizeS min_compat_ondisk_formatS nid_maxS ondisk_format 2) PREFIX_STATbluestore_statfs 信息 12345678910111213141516171819202122232425262728293031323334class BlueStore : public ObjectStore, public md_config_obs_t &#123;... struct volatile_statfs &#123; enum &#123; STATFS_ALLOCATED = 0, STATFS_STORED, STATFS_COMPRESSED_ORIGINAL, STATFS_COMPRESSED, STATFS_COMPRESSED_ALLOCATED, STATFS_LAST &#125;; int64_t values[STATFS_LAST];...&#125;; 设置地方：void BlueStore::_txc_update_store_statfs(TransContext *txc)&#123; if (txc-&gt;statfs_delta.is_empty()) return;... &#123; std::lock_guard&lt;std::mutex&gt; l(vstatfs_lock); vstatfs += txc-&gt;statfs_delta; &#125; bufferlist bl; txc-&gt;statfs_delta.encode(bl); txc-&gt;t-&gt;merge(PREFIX_STAT, "bluestore_statfs", bl); txc-&gt;statfs_delta.reset();&#125; 3) PREFIX_COLLCollection的元数据信息，Collection对应逻辑上的PG，每个ObjectStore都会实现自己的Collection； BlueStore存储一个PG，就会存储一个Collection的kv到RocksDB； 12345678910111213141516171819202122232425class BlueStore : public ObjectStore, public md_config_obs_t &#123; ... typedef boost::intrusive_ptr&lt;Collection&gt; CollectionRef; struct Collection : public CollectionImpl &#123; BlueStore *store; Cache *cache; ///&lt; our cache shard coll_t cid; bluestore_cnode_t cnode; RWLock lock; bool exists; SharedBlobSet shared_blob_set; ///&lt; open SharedBlobs // cache onodes on a per-collection basis to avoid lock // contention. OnodeSpace onode_map; //pool options pool_opts_t pool_opts; ... &#125;;&#125; 4) PREFIX_OBJObject的元数据信息，对于存在BlueStore里的任何一个Object，都会把其的struct Onode信息（+其他）作为value写入RocksDB； 需要访问该Object时，先查询RocksDB，构造出其内存数据结构Onode，再访问之； 1234567891011121314151617181920212223class BlueStore : public ObjectStore, public md_config_obs_t &#123; ... /// an in-memory object struct Onode &#123; std::atomic_int nref; ///&lt; reference count Collection *c; ghobject_t oid; /// key under PREFIX_OBJ where we are stored mempool::bluestore_cache_other::string key; boost::intrusive::list_member_hook&lt;&gt; lru_item; bluestore_onode_t onode; ///&lt; metadata stored as value in kv store bool exists; ///&lt; true if object logically exists ExtentMap extent_map; ... &#125;; typedef boost::intrusive_ptr&lt;Onode&gt; OnodeRef;&#125; 5) PREFIX_OMAPObject的OMap信息，之前存储在Object的attr和Map信息，都通过PREFIX_OMAP前缀保存在RocksDB里； 6) PREFIX_DEFERREDBlueStore Deferred transaction的信息，对应数据结构定义如下： 12345678910111213141516171819/// writeahead-logged transactionstruct bluestore_deferred_transaction_t &#123; uint64_t seq = 0; list&lt;bluestore_deferred_op_t&gt; ops; interval_set&lt;uint64_t&gt; released; ///&lt; allocations to release after tx bluestore_deferred_transaction_t() : seq(0) &#123;&#125; DENC(bluestore_deferred_transaction_t, v, p) &#123; DENC_START(1, 1, p); denc(v.seq, p); denc(v.ops, p); denc(v.released, p); DENC_FINISH(p); &#125; void dump(Formatter *f) const; static void generate_test_instances(list&lt;bluestore_deferred_transaction_t*&gt;&amp; o);&#125;;WRITE_CLASS_DENC(bluestore_deferred_transaction_t) 7) PREFIX_ALLOCFreelistManager相关，默认使用BitmapFreelistManager； 1234B blocksB blocks_per_keyB bytes_per_blockB size 8) PREFIX_SHARED_BLOBShared blob的元数据信息，因为blob的size比较大，有可能上面的多个extent maps映射下来； RocksDB toolceph提供了一个命令来获取一个kvstore里的数据：ceph-kvstore-tool，help如下： 12345678910111213141516171819root@ceph6:~# ceph-kvstore-tool -hUsage: ceph-kvstore-tool &lt;leveldb|rocksdb|bluestore-kv&gt; &lt;store path&gt; command [args...] Commands: list [prefix] list-crc [prefix] exists &lt;prefix&gt; [key] get &lt;prefix&gt; &lt;key&gt; [out &lt;file&gt;] crc &lt;prefix&gt; &lt;key&gt; get-size [&lt;prefix&gt; &lt;key&gt;] set &lt;prefix&gt; &lt;key&gt; [ver &lt;N&gt;|in &lt;file&gt;] rm &lt;prefix&gt; &lt;key&gt; rm-prefix &lt;prefix&gt; store-copy &lt;path&gt; [num-keys-per-tx] [leveldb|rocksdb|...] store-crc &lt;path&gt; compact compact-prefix &lt;prefix&gt; compact-range &lt;prefix&gt; &lt;start&gt; &lt;end&gt; repair 使用示例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950root@ceph6:~# systemctl stop ceph-osd@20.service root@ceph6:~# ceph-kvstore-tool bluestore-kv /var/lib/ceph/osd/ceph-20/ list B &gt; list-B2018-09-21 11:43:42.679 7f4ec14deb80 1 bluestore(/var/lib/ceph/osd/ceph-20/) _mount path /var/lib/ceph/osd/ceph-20/2018-09-21 11:43:42.679 7f4ec14deb80 1 bdev create path /var/lib/ceph/osd/ceph-20//block type kernel2018-09-21 11:43:42.679 7f4ec14deb80 1 bdev(0x55ddf4e58000 /var/lib/ceph/osd/ceph-20//block) open path /var/lib/ceph/osd/ceph-20//block2018-09-21 11:43:42.679 7f4ec14deb80 1 bdev(0x55ddf4e58000 /var/lib/ceph/osd/ceph-20//block) open size 4000783007744 (0x3a381400000, 3.6 TiB) block_size 4096 (4 KiB) rotational2018-09-21 11:43:42.679 7f4ec14deb80 1 bluestore(/var/lib/ceph/osd/ceph-20/) _set_cache_sizes cache_size 1073741824 meta 0.5 kv 0.5 data 02018-09-21 11:43:42.679 7f4ec14deb80 1 bdev create path /var/lib/ceph/osd/ceph-20//block.db type kernel2018-09-21 11:43:42.679 7f4ec14deb80 1 bdev(0x55ddf4e58380 /var/lib/ceph/osd/ceph-20//block.db) open path /var/lib/ceph/osd/ceph-20//block.db2018-09-21 11:43:42.679 7f4ec14deb80 1 bdev(0x55ddf4e58380 /var/lib/ceph/osd/ceph-20//block.db) open size 3221225472 (0xc0000000, 3 GiB) block_size 4096 (4 KiB) non-rotational2018-09-21 11:43:42.679 7f4ec14deb80 1 bluefs add_block_device bdev 1 path /var/lib/ceph/osd/ceph-20//block.db size 3 GiB2018-09-21 11:43:42.679 7f4ec14deb80 1 bdev create path /var/lib/ceph/osd/ceph-20//block type kernel2018-09-21 11:43:42.679 7f4ec14deb80 1 bdev(0x55ddf4e58700 /var/lib/ceph/osd/ceph-20//block) open path /var/lib/ceph/osd/ceph-20//block2018-09-21 11:43:42.683 7f4ec14deb80 1 bdev(0x55ddf4e58700 /var/lib/ceph/osd/ceph-20//block) open size 4000783007744 (0x3a381400000, 3.6 TiB) block_size 4096 (4 KiB) rotational2018-09-21 11:43:42.683 7f4ec14deb80 1 bluefs add_block_device bdev 2 path /var/lib/ceph/osd/ceph-20//block size 3.6 TiB2018-09-21 11:43:42.683 7f4ec14deb80 1 bdev create path /var/lib/ceph/osd/ceph-20//block.wal type kernel2018-09-21 11:43:42.683 7f4ec14deb80 1 bdev(0x55ddf4e58a80 /var/lib/ceph/osd/ceph-20//block.wal) open path /var/lib/ceph/osd/ceph-20//block.wal2018-09-21 11:43:42.683 7f4ec14deb80 1 bdev(0x55ddf4e58a80 /var/lib/ceph/osd/ceph-20//block.wal) open size 3221225472 (0xc0000000, 3 GiB) block_size 4096 (4 KiB) non-rotational2018-09-21 11:43:42.683 7f4ec14deb80 1 bluefs add_block_device bdev 0 path /var/lib/ceph/osd/ceph-20//block.wal size 3 GiB2018-09-21 11:43:42.683 7f4ec14deb80 1 bluefs mount2018-09-21 11:43:42.691 7f4ec14deb80 0 set rocksdb option compaction_readahead_size = 20971522018-09-21 11:43:42.691 7f4ec14deb80 0 set rocksdb option compression = kNoCompression2018-09-21 11:43:42.691 7f4ec14deb80 0 set rocksdb option max_write_buffer_number = 42018-09-21 11:43:42.691 7f4ec14deb80 0 set rocksdb option min_write_buffer_number_to_merge = 12018-09-21 11:43:42.691 7f4ec14deb80 0 set rocksdb option recycle_log_file_num = 42018-09-21 11:43:42.691 7f4ec14deb80 0 set rocksdb option writable_file_max_buffer_size = 02018-09-21 11:43:42.691 7f4ec14deb80 0 set rocksdb option write_buffer_size = 2684354562018-09-21 11:43:42.691 7f4ec14deb80 0 set rocksdb option compaction_readahead_size = 20971522018-09-21 11:43:42.691 7f4ec14deb80 0 set rocksdb option compression = kNoCompression2018-09-21 11:43:42.691 7f4ec14deb80 0 set rocksdb option max_write_buffer_number = 42018-09-21 11:43:42.691 7f4ec14deb80 0 set rocksdb option min_write_buffer_number_to_merge = 12018-09-21 11:43:42.691 7f4ec14deb80 0 set rocksdb option recycle_log_file_num = 42018-09-21 11:43:42.691 7f4ec14deb80 0 set rocksdb option writable_file_max_buffer_size = 02018-09-21 11:43:42.691 7f4ec14deb80 0 set rocksdb option write_buffer_size = 2684354562018-09-21 11:43:42.691 7f4ec14deb80 1 rocksdb: do_open column families: [default]2018-09-21 11:43:42.699 7f4ec14deb80 1 bluestore(/var/lib/ceph/osd/ceph-20/) _open_db opened rocksdb path db options compression=kNoCompression,max_write_buffer_number=4,min_write_buffer_number_to_merge=1,recycle_log_file_num=4,write_buffer_size=268435456,writable_file_max_buffer_size=0,compaction_readahead_size=20971522018-09-21 11:43:42.703 7f4ec14deb80 1 bluestore(/var/lib/ceph/osd/ceph-20/) umount2018-09-21 11:43:42.703 7f4ec14deb80 1 bluefs umount2018-09-21 11:43:42.703 7f4ec14deb80 1 stupidalloc 0x0x55ddf4a92a70 shutdown2018-09-21 11:43:42.703 7f4ec14deb80 1 stupidalloc 0x0x55ddf4a92ae0 shutdown2018-09-21 11:43:42.703 7f4ec14deb80 1 stupidalloc 0x0x55ddf4a92b50 shutdown2018-09-21 11:43:42.703 7f4ec14deb80 1 bdev(0x55ddf4e58a80 /var/lib/ceph/osd/ceph-20//block.wal) close2018-09-21 11:43:42.991 7f4ec14deb80 1 bdev(0x55ddf4e58380 /var/lib/ceph/osd/ceph-20//block.db) close2018-09-21 11:43:43.227 7f4ec14deb80 1 bdev(0x55ddf4e58700 /var/lib/ceph/osd/ceph-20//block) close2018-09-21 11:43:43.463 7f4ec14deb80 1 bdev(0x55ddf4e58000 /var/lib/ceph/osd/ceph-20//block) close root@ceph6:~# systemctl start ceph-osd@20.service]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>Bluestore</tag>
        <tag>RocksDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CephFS client evict子命令使用]]></title>
    <url>%2F2018%2F09%2F28%2Fcephfs-client-evict-intro%2F</url>
    <content type="text"><![CDATA[概述在使用Ceph的CephFS时，每个client都会建立与MDS的连接，以获取CephFS的元数据信息。如果有多个Active的MDS，则一个client可能会与多个MDS都建立连接。 Ceph提供了client/session子命令来查询和管理这些连接，在这些子命令中，有一个命令来处理当CephFS的client有问题时，如何手动来断开这些client的连接，比如执行命令：# ceph tell mds.2 client evict，则会把与mds rank 2 连接的所有clients都断开。 那么执行client evict的影响是什么？是否可以恢复呢？本文将重点介绍一下这些。 命令格式参考：http://docs.ceph.com/docs/master/cephfs/eviction/ 测试环境：Ceph Mimic 13.2.1 1. 查看所有client/session可以通过命令 client/session ls查看与ms rank [id] 建立connection的所有clients； 123456789101112131415161718192021222324# ceph tell mds.0 client ls2018-09-05 10:00:15.986 7f97f0ff9700 0 client.25196 ms_handle_reset on 192.168.0.26:6800/18568127612018-09-05 10:00:16.002 7f97f1ffb700 0 client.25199 ms_handle_reset on 192.168.0.26:6800/1856812761[ &#123; "id": 25085, "num_leases": 0, "num_caps": 5, "state": "open", "replay_requests": 0, "completed_requests": 0, "reconnecting": false, "inst": "client.25085 192.168.0.26:0/265326503", "client_metadata": &#123; "ceph_sha1": "5533ecdc0fda920179d7ad84e0aa65a127b20d77", "ceph_version": "ceph version 13.2.1 (5533ecdc0fda920179d7ad84e0aa65a127b20d77) mimic (stable)", "entity_id": "admin", "hostname": "mimic3", "mount_point": "/mnt/cephfuse", "pid": "44876", "root": "/" &#125; &#125;] 比较重要的信息有： id：client唯一id num_caps：client获取的caps inst：client端的ip和端口链接信息 ceph_version：client端的ceph-fuse版本，若使用kernel client，则为kernel_version hostname：client端的主机名 mount_point：client在主机上对应的mount point pid：client端ceph-fuse进程的pid 2. evict指定client可以通过指定id来evict特定的client链接； 若有多个Active MDS，单个MDS Rank的evict也会传播到别的Active MDS 1# ceph tell mds.0 client evict id=25085 evict client后，在对应的host上检查client的mountpoint已经不能访问： 1234567root@mimic3:/mnt/cephfuse# lsls: cannot open directory '.': Cannot send after transport endpoint shutdown root@mimic3:~# vim /var/log/ceph/ceph-client.admin.log...2018-09-05 10:02:54.829 7fbe732d7700 -1 client.25085 I was blacklisted at osd epoch 519 3. 查看ceph osd的blacklistevict client后，会把client加入到osd blacklist中（后续有代码分析）； 123root@mimic1:~# ceph osd blacklist lslisted 1 entries192.168.0.26:0/265326503 2018-09-05 11:02:54.696345 加入到osd blacklist后，防止evict client的in-flight数据写下去，影响数据一致性；有效时间为1个小时； 4. 尝试恢复evict client把ceph osd blacklist里与刚evict client相关的记录删除； 12root@mimic1:~# ceph osd blacklist rm 192.168.0.26:0/265326503un-blacklisting 192.168.0.26:0/265326503 在对应的host上检查client是否正常？发现client变得正常了！！ 123root@mimic3:~# cd /mnt/cephfuseroot@mimic3:/mnt/cephfuse# lsperftest 而测试 Ceph Luminous 12.2.7 版本时，evcit client后无法立刻恢复，等一段时间后恢复！！ （ “mds_session_autoclose”: “300.000000”,） 12345678root@luminous2:~# ceph osd blacklist rm 192.168.213.25:0/1534097905un-blacklisting 192.168.213.25:0/1534097905root@luminous2:~# ceph osd blacklist lslisted 0 entriesroot@luminous2:/mnt/cephfuse# lsls: cannot open directory '.': Cannot send after transport endpoint shutdown 等待一段时间（300s）后，session变得正常！ 12root@luminous2:/mnt/cephfuse# lsperftest 测试cephfs kernel client的evcit，client无法恢复！！ 12root@mimic3:~# cd /mnt/cephfs-bash: cd: /mnt/cephfs: Permission denied 5. evict所有的client若在evict命令后不指定具体的client id，则会把与该MDS Rank链接的所有client evict掉； 若有多个Active MDS，单个MDS Rank的evict也会传播到别的Active MDS 1# ceph tell mds.0 client evict 这个命令慎用，也一定不要误用，影响比较大！！！ 6. session kill命令session子命令里还有一个kill命令，它比evict命令更彻底； 1234567891011121314root@mimic1:~# ceph tell mds.0 session kill 1047042018-09-05 15:57:45.897 7ff2157fa700 0 client.25742 ms_handle_reset on 192.168.0.26:6800/18568127612018-09-05 15:57:45.917 7ff2167fc700 0 client.25745 ms_handle_reset on 192.168.0.26:6800/1856812761 root@mimic1:~# ceph tell mds.0 session ls2018-09-05 15:57:50.709 7f44eeffd700 0 client.95370 ms_handle_reset on 192.168.0.26:6800/18568127612018-09-05 15:57:50.725 7f44effff700 0 client.95376 ms_handle_reset on 192.168.0.26:6800/1856812761[] root@mimic1:~# ceph osd blacklist lslisted 1 entries192.168.0.26:0/1613295381 2018-09-05 16:57:45.920138 删除 osd blacklist entry： 1234root@mimic1:~# ceph osd blacklist rm 192.168.0.26:0/1613295381un-blacklisting 192.168.0.26:0/1613295381root@mimic1:~# ceph osd blacklist lslisted 0 entries 之后client链接没有再恢复！！！ 123root@mimic3:~# cd /mnt/cephfuseroot@mimic3:/mnt/cephfuse# lsls: cannot open directory '.': Cannot send after transport endpoint shutdown session kill后，这个session无法再恢复！！！也要慎用！！！ 代码分析基于Ceph Mimic 13.2.1代码； 执行client evict的代码如下，可以看出里面会添加osd blacklist： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172bool MDSRank::evict_client(int64_t session_id, bool wait, bool blacklist, std::stringstream&amp; err_ss, Context *on_killed)&#123; ... // 获取指定id的session Session *session = sessionmap.get_session( entity_name_t(CEPH_ENTITY_TYPE_CLIENT, session_id)); // 定义kill mds session的函数 auto kill_mds_session = [this, session_id, on_killed]() &#123; assert(mds_lock.is_locked_by_me()); Session *session = sessionmap.get_session( entity_name_t(CEPH_ENTITY_TYPE_CLIENT, session_id)); if (session) &#123; if (on_killed) &#123; server-&gt;kill_session(session, on_killed); &#125; else &#123; C_SaferCond on_safe; server-&gt;kill_session(session, &amp;on_safe); mds_lock.Unlock(); on_safe.wait(); mds_lock.Lock(); &#125; &#125; ... &#125;; // 定义添加OSD blacklist的函数 auto background_blacklist = [this, session_id, cmd](std::function&lt;void ()&gt; fn) &#123; ... Context *on_blacklist_done = new FunctionContext([this, session_id, fn](int r) &#123; objecter-&gt;wait_for_latest_osdmap( new C_OnFinisher( new FunctionContext(...), finisher) ); &#125;); ... monc-&gt;start_mon_command(cmd, &#123;&#125;, nullptr, nullptr, on_blacklist_done); &#125;; auto blocking_blacklist = [this, cmd, &amp;err_ss, background_blacklist]() &#123; C_SaferCond inline_ctx; background_blacklist([&amp;inline_ctx]() &#123; inline_ctx.complete(0); &#125;); mds_lock.Unlock(); inline_ctx.wait(); mds_lock.Lock(); &#125;; // 根据参数执行kill mds session和添加OSD的blacklist if (wait) &#123; if (blacklist) &#123; blocking_blacklist(); &#125; // We dropped mds_lock, so check that session still exists session = sessionmap.get_session(entity_name_t(CEPH_ENTITY_TYPE_CLIENT, session_id)); ... kill_mds_session(); &#125; else &#123; if (blacklist) &#123; background_blacklist(kill_mds_session); &#125; else &#123; kill_mds_session(); &#125; &#125; ...&#125; 调用该函数的地方有： 12345678910Cscope tag: evict_client # line filename / context / line 1 1965 mds/MDSRank.cc &lt;&lt;handle_asok_command&gt;&gt; bool evicted = evict_client(strtol(client_id.c_str(), 0, 10), true, 2 2120 mds/MDSRank.cc &lt;&lt;evict_clients&gt;&gt; evict_client(s-&gt;info.inst.name.num(), false, 3 782 mds/Server.cc &lt;&lt;find_idle_sessions&gt;&gt; mds-&gt;evict_client(session-&gt;info.inst.name.num(), false, true, 4 1058 mds/Server.cc &lt;&lt;reconnect_tick&gt;&gt; mds-&gt;evict_client(session-&gt;info.inst.name.num(), false, true, ss, 1、handle_asok_command：命令行处理client evict 2、evict_clients：批量evict clients 3、find_idle_sessions：对于stale状态的session，执行evict client 4、reconnect_tick：mds恢复后等待client reconnect，45s超时后evict clients 相关参数于mds session相关的配置参数有： 12345# ceph daemon mgr.luminous2 config show | grep mds_session_ "mds_session_autoclose": "300.000000", "mds_session_blacklist_on_evict": "true", "mds_session_blacklist_on_timeout": "true", "mds_session_timeout": "60.000000", 还有一些client相关的： 1234"client_reconnect_stale": "false","client_tick_interval": "1.000000","mon_client_ping_interval": "10.000000","mon_client_ping_timeout": "30.000000", evict client后的处理从上面的实践可以看出，evcit client后，client会被添加到osd blacklist里，超时时间为1小时；在这个时间段内，client是不能访问CephFS的； 但是通过命令：ceph osd blacklist rm &lt;entry&gt; 删除osd的blacklist后，client端立刻就能继续访问CephFS，一切都跟之前正常时候一样！ 方法1：rm blacklist12345678910111213141516171819root@mimic1:~# ceph tell mds.0 client evict id=250852018-09-05 11:07:43.580 7f80d37fe700 0 client.25364 ms_handle_reset on 192.168.0.26:6800/18568127612018-09-05 11:07:44.292 7f80e8ff9700 0 client.25370 ms_handle_reset on 192.168.0.26:6800/1856812761 root@mimic1:~# ceph tell mds.0 client ls2018-09-05 11:05:23.527 7f5005ffb700 0 client.25301 ms_handle_reset on 192.168.0.26:6800/18568127612018-09-05 11:05:23.539 7f5006ffd700 0 client.94941 ms_handle_reset on 192.168.0.26:6800/1856812761[] root@mimic1:~# ceph osd blacklist rm 192.168.0.26:0/265326503un-blacklisting 192.168.0.26:0/265326503 root@mimic1:~# ceph tell mds.0 client ls2018-09-05 11:07:57.884 7fe07b7f6700 0 client.95022 ms_handle_reset on 192.168.0.26:6800/18568127612018-09-05 11:07:57.900 7fe07c7f8700 0 client.25400 ms_handle_reset on 192.168.0.26:6800/1856812761[] 然后在client host重新访问以下挂载点目录后，session变为正常 123456789101112131415161718192021222324root@mimic1:~# ceph tell mds.0 client ls2018-09-05 11:06:31.484 7f6c6bfff700 0 client.94971 ms_handle_reset on 192.168.0.26:6800/18568127612018-09-05 11:06:31.496 7f6c717fa700 0 client.94977 ms_handle_reset on 192.168.0.26:6800/1856812761[ &#123; "id": 25085, "num_leases": 0, "num_caps": 4, "state": "open", "replay_requests": 0, "completed_requests": 0, "reconnecting": false, "inst": "client.25085 192.168.0.26:0/265326503", "client_metadata": &#123; "ceph_sha1": "5533ecdc0fda920179d7ad84e0aa65a127b20d77", "ceph_version": "ceph version 13.2.1 (5533ecdc0fda920179d7ad84e0aa65a127b20d77) mimic (stable)", "entity_id": "admin", "hostname": "mimic3", "mount_point": "/mnt/cephfuse", "pid": "44876", "root": "/" &#125; &#125;] 方法2：wait 1小时默认evict client后，添加osd blacklist的超时时间为1小时，考察1小时过后，session可以变为正常： 12root@mimic1:~# ceph osd blacklist lslisted 0 entries 然后在client host重新访问以下挂载点目录后，session变为正常 123root@mimic3:~# cd /mnt/cephfuse/root@mimic3:/mnt/cephfuse# lsperftest 查看mds的sessions： 123456789101112131415161718192021222324root@mimic1:~# ceph tell mds.0 session ls2018-09-05 13:56:26.630 7fae7f7fe700 0 client.95118 ms_handle_reset on 192.168.0.26:6801/15417447462018-09-05 13:56:26.642 7fae94ff9700 0 client.25496 ms_handle_reset on 192.168.0.26:6801/1541744746[ &#123; "id": 25085, "num_leases": 0, "num_caps": 1, "state": "open", "replay_requests": 0, "completed_requests": 0, "reconnecting": false, "inst": "client.25085 192.168.0.26:0/265326503", "client_metadata": &#123; "ceph_sha1": "5533ecdc0fda920179d7ad84e0aa65a127b20d77", "ceph_version": "ceph version 13.2.1 (5533ecdc0fda920179d7ad84e0aa65a127b20d77) mimic (stable)", "entity_id": "admin", "hostname": "mimic3", "mount_point": "/mnt/cephfuse", "pid": "44876", "root": "/" &#125; &#125;]]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>CephFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ceph Mimic测试Dashboard功能]]></title>
    <url>%2F2018%2F09%2F15%2Fceph-mimic-dashboard-intro%2F</url>
    <content type="text"><![CDATA[系统概述Ceph从Luminous开始，提供了原生的Dashboard功能，通过Dashboard可以获取Ceph集群的各种状态信息； 相比之前的第三方实现，原生的Dashboard还比较简洁，但是部署和使用方便，后续值得期待！ Ceph从Mimic里实现了Dashboard V2版本，提供了更全面的Ceph展示和管理功能，值得一试； 下面基于Ceph：Mimic 13.2.1 版本实验下其Dashboard功能。 参考文档： http://docs.ceph.com/docs/mimic/mgr/dashboard/ https://www.openattic.org/posts/ceph-manager-dashboard-v2/ 注：Mimic 13.2.1还不支持非ssl的http访问，而最新Master版本里已经有非ssl的支持 操作步骤按照官方文档的步骤，可以很轻松的部署起来Dashboard，如下： 1、查看ceph状态，找出active的mgr 12345678910111213141516root@ceph1:~# ceph -s cluster: id: cdd6456d-9291-4ec9-b138-e073aea1cdbe health: HEALTH_OK services: mon: 3 daemons, quorum ceph1,ceph2,ceph3 mgr: ceph3(active), standbys: ceph1, ceph2 mds: kirkfs-2/2/2 up &#123;0=mds-daemon-ceph3=up:active,1=mds-ceph3=up:active&#125;, 6 up:standby osd: 24 osds: 24 up, 24 in data: pools: 2 pools, 1536 pgs objects: 25.64 k objects, 100 GiB usage: 280 GiB used, 68 TiB / 68 TiB avail pgs: 1536 active+clean mgr: ceph3(active) 2、生成并安装自签名的证书 12root@ceph1:~# ceph dashboard create-self-signed-certSelf-signed certificate created 3、生成key pair，并配置给ceph mgr 12345678910111213141516root@ceph1:~/yangguanjun# mkdir mgr-dashboardroot@ceph1:~/yangguanjun# cd mgr-dashboard/root@ceph1:~/yangguanjun/mgr-dashboard# openssl req -new -nodes -x509 \&gt; -subj "/O=IT/CN=ceph-mgr-dashboard" -days 3650 \&gt; -keyout dashboard.key -out dashboard.crt -extensions v3_caGenerating a 2048 bit RSA private key.......................+++............+++writing new private key to 'dashboard.key'----- root@ceph1:~/yangguanjun/mgr-dashboard# lsdashboard.crt dashboard.key root@ceph1:~/yangguanjun/mgr-dashboard# ceph mgr module disable dashboardroot@ceph1:~/yangguanjun/mgr-dashboard# ceph mgr module enable dashboard 4、在ceph active mgr上配置server addr和port 若使用默认的8443端口，则可跳过该步骤！ 12345678root@ceph3:~# ceph config set mgr mgr/dashboard/server_addr 192.168.0.26root@ceph3:~# ceph config set mgr mgr/dashboard/server_port 8080 root@ceph3:~# ceph mgr services&#123; "dashboard": "https://192.168.0.26:8080/",&#125; 5、生成登陆认证的用户名和密码 123root@ceph1:~/yangguanjun/mgr-dashboard# ceph dashboard set-login-credentials admin admin@2018Username and password updated 6、若需通过外网访问，则在有外网IP的机器上配置nginx服务做https转发 123456789101112131415161718192021root@ceph1:/etc/nginx/ssr# scp ceph1:~/yangguanjun/mgr-dashboard/* ./dashboard.crt 100% 1155 1.1KB/s 00:00dashboard.keyroot@ceph1:/etc/nginx/ssl# vim /etc/nginx/nginx.conf... server &#123; listen 8080 ssl; ssl_certificate /etc/nginx/ssl/dashboard.crt; ssl_certificate_key /etc/nginx/ssl/dashboard.key; ssl_session_cache shared:SSL:1m; ssl_session_timeout 5m; ssl_protocols SSLv2 SSLv3 TLSv1.2; ssl_ciphers HIGH:!aNULL:!MD5; ssl_prefer_server_ciphers on; location / &#123; proxy_pass https://192.168.0.26:8080; proxy_set_header Host $host; &#125; &#125;...root@ceph1:/etc/nginx/ssl# systemctl restart nginx.service 示例截图然后就可以通过ceph1的外网IP来访问ceph的dashboard了； https://external-ip:8080/ 使用之前配置的admin账号密码登陆后，看到Dashboard界面如下，然后截图几个，展示一下：]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>mgr</tag>
        <tag>Dashboard</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ceph BlueStore Write Analyse]]></title>
    <url>%2F2018%2F09%2F06%2Fceph-blueStore-write-analyse%2F</url>
    <content type="text"><![CDATA[概述Ceph从Luminous开始，默认使用了全新的存储引擎BlueStore，来替代之前提供存储服务的FileStore，与FileStore不一样的是，BlueStore直接管理裸盘，分配数据块来存储RADOS里的objects。 Bluestore比较复杂，学习BlueStore的关键就是查看其实现的 ”object → block device“ 映射，所以本文先重点分析下该部分：一个Object写到Bluestore的处理过程。 Bluestore整体架构下面借用Sage Weil的图描述下BlueStore的整体架构： 里面的几个关键组件介绍： RocksDB：存储元数据信息 BlueRocksEnv：提供RocksDB的访问接口 BlueFS：实现BlueRocksEnv里的访问接口 Allocator：磁盘分配器 针对BlueStore的整体架构不做展开，大家只需对其几大组件有些了解即可，本文继续介绍Object写操作到底层BlockDevice的过程。 BlueStore中Object到底层Device的映射关系这里先从整体上给出在BlueStore中，一个Object的数据映射到底层BlockDevice的实现，如下图： 首先详细介绍下与上诉映射关系相关的数据结构。 Bluestore相关数据结构BlueStore里与Object 数据映射相关的数据结构罗列如下： Onode任何RADOS里的一个Object都对应Bluestore里的一个Onode（内存结构），定义如下： 123456struct Onode &#123; Collection *c; // 对应的Collection，对应PG ghobject_t oid; // Object信息 bluestore_onode_t onode; // Object存到kv DB的元数据信息 ExtentMap extent_map; // 映射lextents到blobs&#125;; 通过Onode里的ExtentMap来查询Object数据到底层的映射。 ExtentMapExtentMap是Extent的set集合，是有序的，定义如下： 12345678910111213struct ExtentMap &#123; Onode *onode; // 指向Onode指针 extent_map_t extent_map; // Extents到Blobs的map blob_map_t spanning_blob_map; // 跨越shards的blobs struct Shard &#123; bluestore_onode_t::shard_info *shard_info = nullptr; unsigned extents = 0; ///&lt; count extents in this shard bool loaded = false; ///&lt; true if shard is loaded bool dirty = false; ///&lt; true if shard is dirty and needs reencoding &#125;; mempool::bluestore_cache_other::vector&lt;Shard&gt; shards; ///&lt; shards&#125;; ExtentMap还提供了分片功能，防止在文件碎片化严重，ExtentMap很大时，影响写RocksDB的性能。 ExtentMap会随着写入数据的变化而变化； ExtentMap的连续小段会合并为大； 覆盖写也会导致ExtentMap分配新的Blob； ExtentExtent是实现object的数据映射的关键数据结构，定义如下： 123456struct Extent : public ExtentBase &#123; uint32_t logical_offset = 0; // 对应Object的逻辑偏移 uint32_t blob_offset = 0; // 对应Blob上的偏移 uint32_t length = 0; // 数据段长度 BlobRef blob; // 指向对应Blob的指针&#125;; 每个Extent都会映射到下一层的Blob上，Extent会依据 block_size 对齐，没写的地方填充全零。 Extent中的length值，最小：block_size，最大：max_blob_size BlobBlob是Bluestore里引入的处理块设备数据映射的中间层，定义如下： 12345678910struct Blob &#123; int16_t id = -1; SharedBlobRef shared_blob; // 共享的blob状态 mutable bluestore_blob_t blob; // blob的元数据&#125;;struct bluestore_blob_t &#123; PExtentVector extents; // 对应磁盘上的一组数据段 uint32_t logical_length = 0; // blob的原始数据长度 uint32_t compressed_length = 0; // 压缩的数据长度&#125;; 每个Blob会对应一组 PExtentVector，它就是 bluestore_pextent_t 的一个数组，指向从Disk中分配的物理空间。 Blob里可能对应一个磁盘pextent，也可能对应多个pextent； Blob里的pextent个数最多为：max_blob_size / min_alloc_size； Blob里的多个pextent映射的Blob offset可能不连续，中间有空洞； AllocExtentAllocExtent是管理物理磁盘上的数据段的，定义如下： 123456789struct bluestore_pextent_t : public AllocExtent &#123; ...&#125;;class AllocExtent &#123;public: uint64_t offset; // 磁盘上的物理偏移 uint32_t length; // 数据段的长度...&#125;; AllocExtent的 length值，最小：min_alloc_size，最大：max_blob_size BlueStore写数据流程BlueStore里的写数据入口是BlueStore::_do_write()，它会根据 min_alloc_size 来切分 [offset, length] 的写，然后分别依据 small write 和 big write 来处理，如下： 12345678910111213141516171819202122232425262728// 按照min_alloc_size大小切分，把写数据映射到不同的块上 [offset, length] |==p1==|=======p2=======|=p3=||----------------|----------------|----------------|| min_alloc_size | min_alloc_size | min_alloc_size ||----------------|----------------|----------------|small write: p1, p3big write: p2 BlueStore::_do_write()|-- BlueStore::_do_write_data()| // 依据`min_alloc_size`把写切分为`small/big`写| | -- BlueStore::_do_write_small()| | | -- BlueStore::ExtentMap::seek_lextent()| | | -- BlueStore::Blob::can_reuse_blob()| | reuse blob? or new blob?| | | -- insert to struct WriteContext &#123;&#125;;| | -- BlueStore::_do_write_big()| | | -- BlueStore::ExtentMap::punch_hole()| | | -- BlueStore::Blob::can_reuse_blob()| | reuse blob? or new blob?| | | -- insert to struct WriteContext &#123;&#125;;|-- BlueStore::_do_alloc_write()| | -- StupidAllocator::allocate()| | -- BlueStore::ExtentMap::set_lextent()| | -- BlueStore::_buffer_cache_write()|-- BlueStore::_wctx_finish() BlueStore Log分析可以通过开启Ceph bluestore debug来抓取其写过程中对数据的映射，具体步骤如下。 下面通过在CephFS上测试为例： 1234567891011121314151617181920212223241. 创建一个文件# touch tstfile 2. 查看该文件的inode numer# ls -i2199023255554 tstfile 3. 获取该文件的映射信息上诉inode number转换为16进制：20000000002查看文件的第一个默认4M Object的映射信息# ceph osd map cephfs_data_ssd 20000000002.00000000osdmap e2649 pool 'cephfs_data_ssd' (3) object '20000000002.00000000' -&gt; pg 3.3ff3fe94 (3.94) -&gt; up ([12,0], p12) acting ([12,0], p12) 4. 在osd 12上开启bluestroe debug信息# ceph daemon /var/run/ceph/ceph-osd.12.asok config set debug_bluestore "30" // 开启debug# ceph daemon /var/run/ceph/ceph-osd.12.asok config set debug_bluestore "1/5" // 恢复默认 5. 对测试文件的前4M内进行dd操作，收集log# dd if=/dev/zero of=tstfile bs=4k count=1 oflag=direct# grep -v "trim shard target" /var/log/ceph/ceph-osd.12.log | grep -v "collection_list" &gt; bluestore-write-0-4k.log 通过上述方式可以搜集到Bluestore在写入数据时，object的数据分配和映射过程，可以帮助理解其实现。 BlueStore dd write各种case为了更好的理解BlueStore里一个write的过程，我们通过dd命令写一个Object，然后抓取log后分析不同情况下的Object数据块映射情况，最后结果如下图所示： 注释：上图的数据块映射关系是通过抓取log后获取的。 最后一图中，写[100k, 200)的区域，查看Object对应的ExtentMap并不是与 min_alloc_size（16k）对齐的，只是保证是block_size（4k）对齐而已。]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>bluestore</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes上部署rabbitmq集群]]></title>
    <url>%2F2018%2F08%2F06%2Frabbitmq-deploy-with-k8s%2F</url>
    <content type="text"><![CDATA[概述之前文章讲述了如何通过docker命令来手动创建一个rabbitmq集群；可以通过其了解rabbitmq构建的步骤和基本原理； 随着Kubernetes的逐渐流行，如何通过Kubernetes的方式来部署一个rabbitmq集群呢？下面讲述其步骤。 创建rabbitmq集群这里我们使用的docker image有一个： rabbitmq:3.7-rc-management 创建rabbitmq集群的yaml文件下面yaml中涉及的Kubernetes组件有： Service 提供http服务的service 提供StatfulSet的headless service StatefulSet 有状态pods的集合，真正提供rabbitmq的服务 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485root@kmaster:~/yangguanjun# cat rabbitmq.yaml---apiVersion: v1kind: Servicemetadata: # Expose the management HTTP port on each node name: rabbitmq-management labels: app: rabbitmqspec: ports: - port: 15672 name: http selector: app: rabbitmq type: NodePort # Or LoadBalancer in production w/ proper security---apiVersion: v1kind: Servicemetadata: # The required headless service for StatefulSets name: rabbitmq labels: app: rabbitmqspec: ports: - port: 5672 name: amqp - port: 4369 name: epmd - port: 25672 name: rabbitmq-dist clusterIP: None selector: app: rabbitmq---apiVersion: apps/v1beta1kind: StatefulSetmetadata: name: rabbitmqspec: serviceName: "rabbitmq" replicas: 3 template: metadata: labels: app: rabbitmq spec: containers: - name: rabbitmq image: rabbitmq:3.7-rc-management lifecycle: postStart: exec: command: - /bin/sh - -c - &gt; if [ -z "$(grep rabbitmq /etc/resolv.conf)" ]; then sed "s/^search \([^ ]\+\)/search rabbitmq.\1 \1/" /etc/resolv.conf &gt; /etc/resolv.conf.new; cat /etc/resolv.conf.new &gt; /etc/resolv.conf; rm /etc/resolv.conf.new; fi; until rabbitmqctl node_health_check; do sleep 1; done; if [ -z "$(rabbitmqctl cluster_status | grep rabbitmq-0)" ]; then touch /gotit rabbitmqctl stop_app; rabbitmqctl reset; rabbitmqctl join_cluster rabbit@rabbitmq-0; rabbitmqctl start_app; else touch /notget fi; env: - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: RABBITMQ_ERLANG_COOKIE value: "YZSDHWMFSMKEMBDHSGGZ" - name: RABBITMQ_NODENAME value: "rabbit@$(MY_POD_NAME)" ports: - containerPort: 5672 name: amqp 通过Pod的postStart来执行命令，把后面的两个Pod加入rabbitmq集群。 Pod的生命周期里有两个个hook部分：PostStart，PreStop 创建不同的rabbitmq集群，需要把yaml文件的很多处rabbitmq关键字替换掉。。。 也可以通过参数来生成yaml文件 创建 rabbitmq 集群通过kubectl命令，依据上章节的rabbitmq.yaml文件创建rabbitmq集群； 1234root@kmaster:~/yangguanjun# kubectl create -f rabbitmq.yamlservice "rabbitmq-management" createdservice "rabbitmq" createdstatefulset "rabbitmq" created 检查rabbitmq 集群检查刚才创建的rabbitmq集群是否正常？出于什么状态？可以使用如下命令： 12345678910111213141516171819202122232425root@kmaster:~/yangguanjun# kubectl get serviceNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes 10.96.0.1 &lt;none&gt; 443/TCP 9drabbitmq None &lt;none&gt; 5672/TCP,4369/TCP,25672/TCP 21mrabbitmq-management 10.96.158.11 &lt;nodes&gt; 15672:32219/TCP 21m root@kmaster:~/yangguanjun# kubectl get podsNAME READY STATUS RESTARTS AGErabbitmq-0 1/1 Running 0 21mrabbitmq-1 1/1 Running 0 21mrabbitmq-2 1/1 Running 0 21m root@kmaster:~/yangguanjun# kubectl exec -it rabbitmq-0 bashroot@rabbitmq-0:/# rabbitmqctl cluster_statusCluster status of node rabbit@rabbitmq-0 ...[&#123;nodes,[&#123;disc,['rabbit@rabbitmq-0','rabbit@rabbitmq-1', 'rabbit@rabbitmq-2']&#125;]&#125;, &#123;running_nodes,['rabbit@rabbitmq-2','rabbit@rabbitmq-1','rabbit@rabbitmq-0']&#125;, &#123;cluster_name,&lt;&lt;"rabbit@rabbitmq-0.rabbitmq.default.svc.cluster.local"&gt;&gt;&#125;, &#123;partitions,[]&#125;, &#123;alarms,[&#123;'rabbit@rabbitmq-2',[]&#125;, &#123;'rabbit@rabbitmq-1',[]&#125;, &#123;'rabbit@rabbitmq-0',[]&#125;]&#125;] rabbitmq集群的域名上面我们通过Statefulset来创建的rabbitmq集群，但是我们并没有在镜像中指定几个Pods之间如何访问，它们之间的解析是通过Headless service提供的，可以通过下面命令获取Pod里的域名解析： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455root@rabbitmq-0:/# cat /etc/resolv.confnameserver 10.96.0.10search rabbitmq.default.svc.cluster.local default.svc.cluster.local svc.cluster.local cluster.localoptions ndots:5 root@rabbitmq-0:/# dig rabbitmq.default.svc.cluster.local ; &lt;&lt;&gt;&gt; DiG 9.10.3-P4-Debian &lt;&lt;&gt;&gt; rabbitmq.default.svc.cluster.local;; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 55697;; flags: qr aa rd ra; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 0 ;; QUESTION SECTION:;rabbitmq.default.svc.cluster.local. IN A ;; ANSWER SECTION:rabbitmq.default.svc.cluster.local. 30 IN A 192.168.239.175rabbitmq.default.svc.cluster.local. 30 IN A 192.168.150.235rabbitmq.default.svc.cluster.local. 30 IN A 192.168.152.171 ;; Query time: 0 msec;; SERVER: 10.96.0.10#53(10.96.0.10);; WHEN: Fri May 04 08:56:40 UTC 2018;; MSG SIZE rcvd: 100 root@rabbitmq-0:/# nslookup rabbitmq-1Server: 10.96.0.10Address: 10.96.0.10#53 Name: rabbitmq-1.rabbitmq.default.svc.cluster.localAddress: 192.168.239.175 root@rabbitmq-0:/# nslookup rabbitmq.default.svc.cluster.localServer: 10.96.0.10Address: 10.96.0.10#53 Name: rabbitmq.default.svc.cluster.localAddress: 192.168.150.235Name: rabbitmq.default.svc.cluster.localAddress: 192.168.152.171Name: rabbitmq.default.svc.cluster.localAddress: 192.168.239.175 root@rabbitmq-0:/# nslookup rabbitmq-management.default.svc.cluster.localServer: 10.96.0.10Address: 10.96.0.10#53 Name: rabbitmq-management.default.svc.cluster.localAddress: 10.96.158.11 访问rabbitmq集群创建好rabbitmq集群后，如何访问它呢？ 如之前我们介绍使用的image为：rabbitmq:3.7-rc-management，它包含了rabbitmq的management组件，另外我们也创建了rabbitmq集群对应的service服务，提供了基于http端口的访问，通过下面命令获取其信息： 12345678910111213root@kmaster:~/yangguanjun# kubectl describe service/rabbitmq-managementName: rabbitmq-managementNamespace: defaultLabels: app=rabbitmqAnnotations: &lt;none&gt;Selector: app=rabbitmqType: NodePortIP: 10.96.158.11Port: http 15672/TCPNodePort: http 32219/TCPEndpoints: 192.168.150.235:15672,192.168.152.171:15672,192.168.239.175:15672Session Affinity: NoneEvents: &lt;none&gt; 则可以通过http指定IP和Port直接访问：http://10.96.158.11:15672 当然这里的IP是内网IP，若公司内外不能访问的话，可以绑定外网IP后再通过公网+port访问。 参考文档https://blog.csdn.net/zhaohuabing/article/details/78673329https://wesmorgan.svbtle.com/rabbitmq-cluster-on-kubernetes-with-statefulsetshttps://github.com/xarg/rabbitmq-statefulset]]></content>
      <categories>
        <category>other</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>rabbitmq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker部署rabbitmq集群]]></title>
    <url>%2F2018%2F07%2F30%2Frabbitmq-cluster-deploy-with-docker%2F</url>
    <content type="text"><![CDATA[拉取rabbitmq镜像拉取rabbitmq management镜像1docker pull rabbitmq:3.7-rc-management 若不使用Rabbitmq的management功能，可以拉取镜像：rabbitmq:3.7-rc 参考：https://hub.docker.com/_/rabbitmq/ 创建网络创建rabbitmq私有网络 12345# docker network create rabbitmqnet# docker network lsNETWORK ID NAME DRIVER SCOPE65b44ea8847c rabbitmqnet bridge local... 创建节点通过docker命令创建三个Rabbitmq nodes； 注意这里使用相同的 RABBITMQ_ERLANG_COOKIE 值 1234567891011121314151617181920212223242526272829# docker run -d \--name=rabbitmq1 \-p 5672:5672 \-p 15672:15672 \-e RABBITMQ_NODENAME=rabbitmq1 \-e RABBITMQ_ERLANG_COOKIE='YZSDHWMFSMKEMBDHSGGZ' \-h rabbitmq1 \--net=rabbitmqnet \rabbitmq:3.7-rc-management# docker run -d \--name=rabbitmq2 \-p 5673:5672 \-p 15673:15672 \-e RABBITMQ_NODENAME=rabbitmq2 \-e RABBITMQ_ERLANG_COOKIE='YZSDHWMFSMKEMBDHSGGZ' \-h rabbitmq2 \--net=rabbitmqnet \rabbitmq:3.7-rc-management# docker run -d \--name=rabbitmq3 \-p 5674:5672 \-p 15674:15672 \-e RABBITMQ_NODENAME=rabbitmq3 \-e RABBITMQ_ERLANG_COOKIE='YZSDHWMFSMKEMBDHSGGZ' \-h rabbitmq3 \--net=rabbitmqnet \rabbitmq:3.7-rc-management 组建rabbitmq集群登陆Rabbitmq的后两个节点，执行命令加入第一个Rabbitmq节点集群 12345678910111213### Disk Node# docker exec rabbitmq2 bash -c \"rabbitmqctl stop_app &amp;&amp; \rabbitmqctl reset &amp;&amp; \rabbitmqctl join_cluster rabbitmq1@rabbitmq1 &amp;&amp; \rabbitmqctl start_app"### Ram Node# docker exec rabbitmq3 bash -c \"rabbitmqctl stop_app &amp;&amp; \rabbitmqctl reset &amp;&amp; \rabbitmqctl join_cluster --ram rabbitmq1@rabbitmq1 &amp;&amp; \rabbitmqctl start_app" 退出集群1234# docker exec rabbitmq3 bash -c \"rabbitmqctl stop_app &amp;&amp; \rabbitmqctl reset &amp;&amp; \rabbitmqctl start_app" 拉取haproxy镜像拉取haproxy镜像 1# docker pull haproxy 启动haproxy 123456789# cat haproxy-create.sh#! /bin/bashdocker run -d \ --name rabbitmq-haproxy \ -p 1080:80 -p 5677:5677 -p 8001:8001 \ --net=rabbitmqnet \ -v /root/rabbitmq/haproxy-etc:/usr/local/etc/haproxy:ro \ haproxy:latest haproxy的配置文件如下： 123456789101112131415161718192021222324252627282930313233root@node0:~/rabbitmq# cat haproxy-etc/haproxy.cfg# Simple configuration for an HTTP proxy listening on port 80 on all# interfaces and forwarding requests to a single backend "servers" with a# single server "server1" listening on 127.0.0.1:8000global daemon maxconn 256defaults mode http timeout connect 5000ms timeout client 5000ms timeout server 5000mslisten rabbitmq_cluster bind 0.0.0.0:5677 option tcplog mode tcp balance leastconn server rabbit1 rabbitmq1:5672 check inter 2s rise 2 fall 3 server rabbit2 rabbitmq2:5672 check inter 2s rise 2 fall 3 server rabbit3 rabbitmq3:5672 check inter 2s rise 2 fall 3listen http_front bind 0.0.0.0:80 stats uri /haproxy?statslisten rabbitmq_admin bind 0.0.0.0:8001 server rabbit1 rabbitmq1:15672 server rabbit2 rabbitmq2:15672 server rabbit3 rabbitmq3:15672 启动haproxy后，可以通过haproxy来访问rabbitmq集群： http://external-ip:8001 获取haproxy的状态： http://external-ip:1080/haproxy?stats rabbitmq exporter部署要收集rabbitmq的metrics给prometheus使用的话，可以使用开源的rabbitmq-exporter 参考如下： https://github.com/kbudde/rabbitmq_exporter https://hub.docker.com/r/kbudde/rabbitmq-exporter/ 拉取镜像 1# docker pull kbudde/rabbitmq-exporter 启动rabbitmq实例 1# docker run -d --name=rabbitmq1 -p 5672:5672 -p 15672:15672 -e RABBITMQ_NODENAME=rabbitmq1 -e RABBITMQ_ERLANG_COOKIE='YZSDHWMFSMKEMBDHSGGZ' -h rabbitmq1 --net=rabbitmqnet -p 9090:9090 rabbitmq:3.7-rc-management 开启9090端口，这个是rabbitmq exporter的默认PUBLISH_PORT 启动rabbitmq exporter实例 1# docker run -d --net=container:rabbitmq1 kbudde/rabbitmq-exporter 获取rabbitmq的metrics 1# wget http://localhost:9090/metrics rabbitmq相关端口 4369 (epmd), 25672 (Erlang distribution) Epmd 是 Erlang Port Mapper Daemon 的缩写，在 Erlang 集群中相当于 dns 的作用，供给节点名称到端口的查询，epmd绑定在总所周知的4369端口上。 5672, 5671 (AMQP 0-9-1 without and with TLS) AMQP，即 Advanced Message Queuing Protocol，一个提供统一消息服务的应用层标准高级消息队列协议，是应用层协议的一个开放标准,为面向消息的中间件设计。基于此协议的客户端与消息中间件可传递消息，并不受客户端/中间件不同产品，不同的开发语言等条件的限制。Erlang 中的实现有 RabbitMQ 等。 15672 (if management plugin is enabled) 通过 http://serverip:15672 访问 RabbitMQ 的 Web 管理界面，默认用户名密码都是 guest。（注意：RabbitMQ 3.0之前的版本默认端口是55672，下同） 25672 管理插件端口，默认是AMQP端口+20000 61613, 61614 (if STOMP is enabled) Stomp 是一个简单的消息文本协议，它的设计核心理念就是简单与可用性，官方文档，现在我们就来实践一下 Stomp 协议，你需要的是： 一个支持 stomp 消息协议的 messaging server (譬如activemq，rabbitmq） 一个终端（譬如linux shell） 一些基本命令与操作（譬如nc，telnet） 1883, 8883 (if MQTT is enabled) MQTT 只是 IBM 推出的一个消息协议，基于 TCP/IP 的。两个 App 端发送和接收消息需要中间人，这个中间人就是消息服务器（比如ActiveMQ/RabbitMQ），三者通信协议就是 MQTT]]></content>
      <categories>
        <category>other</category>
      </categories>
      <tags>
        <tag>rabbitmq</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux Ext文件系统恢复误删文件]]></title>
    <url>%2F2018%2F07%2F18%2Fext-fs-undelete-intro%2F</url>
    <content type="text"><![CDATA[需求描述生产中会遇到用户误操作删除文件系统中文件的情况，而我们经常用的就是Ext文件系统，Ext3/4居多。 本篇文章大致说明下Ext文件系统文件误删后应该如何操作？如何恢复？ 工具介绍网上针对Ext文件系统中文件恢复的文章比较多，用的工具大部分是：extundelete extundelete介绍：http://extundelete.sourceforge.net/ 原理Linux上删除文件的时候，只会收回其inode，inode表中inode数据不会删除，inode中文件分配的数据块也不会删除，所以可以通过工具扫描inode表，解析之后恢复数据。 恢复前提 删除文件的inode没有被重新分配 删除文件的inode里分配的数据块没有重新分配给别的inode 所以遇到误删文件后的紧急处理如下： 立即停止所有写入； 最保险的就是卸载挂载的云硬盘；或者重新挂载为只读模式； 参考https://blog.csdn.net/u012843189/article/details/80143998 http://blog.51cto.com/ixdba/1566856 实践步骤下面描述如何通过工具extundelete来恢复Ext文件系统里误删的文件。 安装extundeleteCentOS环境：12345678# yum install -y e2fsprogs* e2fslibs*# wget //nchc.dl.sourceforge.net/project/extundelete/extundelete/0.2.4/extundelete-0.2.4.tar.bz2# yum install bzip2# tar -jxvf extundelete-0.2.4.tar.bz2# cd extundelete-0.2.4# ./configure# make# make install extundelete的help如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# extundelete --helpUsage: extundelete [options] [--] device-fileOptions: --version, -[vV] Print version and exit successfully. --help, Print this help and exit successfully. --superblock Print contents of superblock in addition to the rest. If no action is specified then this option is implied. --journal Show content of journal. --after dtime Only process entries deleted on or after 'dtime'. --before dtime Only process entries deleted before 'dtime'.Actions: --inode ino Show info on inode 'ino'. --block blk Show info on block 'blk'. --restore-inode ino[,ino,...] Restore the file(s) with known inode number 'ino'. The restored files are created in ./RECOVERED_FILES with their inode number as extension (ie, file.12345). --restore-file 'path' Will restore file 'path'. 'path' is relative to root of the partition and does not start with a '/' The restored file is created in the current directory as 'RECOVERED_FILES/path'. --restore-files 'path' Will restore files which are listed in the file 'path'. Each filename should be in the same format as an option to --restore-file, and there should be one per line. --restore-directory 'path' Will restore directory 'path'. 'path' is relative to the root directory of the file system. The restored directory is created in the output directory as 'path'. --restore-all Attempts to restore everything. -j journal Reads an external journal from the named file. -b blocknumber Uses the backup superblock at blocknumber when opening the file system. -B blocksize Uses blocksize as the block size when opening the file system. The number should be the number of bytes. --log 0 Make the program silent. --log filename Logs all messages to filename.--log D1=0,D2=filename Custom control of log messages with comma-separated Examples below: list of options. Dn must be one of info, warn, or --log info,error error. Omission of the '=name' results in messages --log warn=0 with the specified level to be logged to the console. --log error=filename If the parameter is '=0', logging for the specified level will be turned off. If the parameter is '=filename', messages with that level will be written to filename. -o directory Save the recovered files to the named directory. The restored files are created in a directory named 'RECOVERED_FILES/' by default. extundelete 常用命令1234# extundelete --inode 2 /dev/rbd0 ## 查看根目录下文件信息# extundelete /dev/rbd0 --restore-file hosts ## 恢复单个文件，恢复名为hosts的误删文件# extundelete /dev/rbd0 --restore-files test/ ## 恢复一个目录，恢复名为test的误删目录# extundelete /dev/rbd0 –-restore-all ## 恢复整个设备 操作实例这里以ceph rbd提供的块设备为例讲解如何扫描恢复误删的文件。 首先扫描根目录下的文件信息： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# extundelete --inode 2 /dev/rbd0NOTICE: Extended attributes are not restored.WARNING: EXT3_FEATURE_INCOMPAT_RECOVER is set.The partition should be unmounted to undelete any files without further data loss.If the partition is not currently mounted, this message indicatesit was improperly unmounted, and you should run fsck before continuing.If you decide to continue, extundelete may overwrite some of the deletedfiles and make recovering those files impossible. You should unmount thefile system and check it with fsck before using extundelete.Would you like to continue? (y/n)yLoading filesystem metadata ... 4096 groups loaded.Group: 0Contents of inode 2:0000 | ed 41 00 00 00 10 00 00 83 ac f3 5a cb ac f3 5a | .A.........Z...Z0010 | cb ac f3 5a 00 00 00 00 00 00 08 00 08 00 00 00 | ...Z............0020 | 00 00 08 00 65 17 06 00 0a f3 01 00 04 00 00 00 | ....e...........0030 | 00 00 00 00 00 00 00 00 01 00 00 00 21 24 00 00 | ............!$..0040 | 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 | ................0050 | 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 | ................0060 | 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 | ................0070 | 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 | ................0080 | 20 00 00 00 c8 9b 8a df c8 9b 8a df 10 32 46 a7 | ............2F.0090 | 90 da 3c 5a 00 00 00 00 00 00 00 00 00 00 00 00 | ..&lt;Z............00a0 | 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 | ................00b0 | 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 | ................00c0 | 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 | ................00d0 | 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 | ................00e0 | 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 | ................00f0 | 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 | ................ Inode is AllocatedFile mode: 16877Low 16 bits of Owner Uid: 0Size in bytes: 4096Access time: 1525918851Creation time: 1525918923Modification time: 1525918923Deletion Time: 0Low 16 bits of Group Id: 0Links count: 8Blocks count: 8File flags: 524288File version (for NFS): 0File ACL: 0Directory ACL: 0Fragment address: 0Direct blocks: 127754, 4, 0, 0, 1, 9249, 0, 0, 0, 0, 0, 0Indirect block: 0Double indirect block: 0Triple indirect block: 0 File name | Inode number | Deleted status. 2.. 2log-test 22020097backup 31981569model 32768001deploy 6029313configs 1703937log-train 2359297labels.lst 11 Deletedlabels_bt.lst 12 Deleted 然后可以执行恢复： 恢复单个文件 1# extundelete /dev/rbd0 --restore-file labels.lst 恢复一个目录 1# extundelete /dev/rbd0 --restore-directory log-train 恢复所有 1# extundelete /dev/rbd0 --restore-all]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>ext</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Luminous版本CephFS的file location]]></title>
    <url>%2F2018%2F06%2F25%2Fcephfs-digout-file-location%2F</url>
    <content type="text"><![CDATA[需求在Ceph Luminous里部署了CephFS，想查看上面的一个file到Rados，再到OSDs的映射关系。 之前在Ceph Jewel版本里，有个cephfs的工具，可以获取file的location信息，如下： 12345678910# cephfs /mnt/tstfs2/mike512K/tstfile show_locationWARNING: This tool is deprecated. Use the layout.* xattrs to query and modify layouts.location.file_offset: 0 // file的偏移location.object_offset:0 // object的偏移location.object_no: 0 // object的numberlocation.object_size: 4194304 // object size为4Mlocation.object_name: 10000002356.00000000 // object的namelocation.block_offset: 0 // block的偏移location.block_size: 524288 // block size为512klocation.osd: 0 // 存储在osd 0 上 但如上面的WARNING所述，这个命令被遗弃了，在Ceph Luminous里没找到相关的替换命令。。。 在Ceph的官方文档里也没找到相关说法：http://docs.ceph.com/docs/master/cephfs/file-layouts/ 那只能自己看代码分析了 ;( 代码分析Jewel版本在Jewel版本里是有cephfs这个工具的，那先看看它是如何工作的？ 123456789101112131415161718192021222324文件：src/cephfs.ccint main (int argc, char **argv)&#123;... if (CMD_SHOW_LAYOUT == cmd) &#123; ... &#125; else if (CMD_SHOW_LOC == cmd) &#123; struct ceph_ioctl_dataloc location; location.file_offset = file_offset; err = ioctl(fd, CEPH_IOC_GET_DATALOC, (unsigned long)&amp;location); if (err) &#123; cerr &lt;&lt; "Error getting location: " &lt;&lt; cpp_strerror(err) &lt;&lt; endl; return 1; &#125; cout &lt;&lt; "location.file_offset: " &lt;&lt; location.file_offset &lt;&lt; endl; cout &lt;&lt; "location.object_offset:" &lt;&lt; location.object_offset &lt;&lt; endl; cout &lt;&lt; "location.object_no: " &lt;&lt; location.object_no &lt;&lt; endl; cout &lt;&lt; "location.object_size: " &lt;&lt; location.object_size &lt;&lt; endl; cout &lt;&lt; "location.object_name: " &lt;&lt; location.object_name &lt;&lt; endl; cout &lt;&lt; "location.block_offset: " &lt;&lt; location.block_offset &lt;&lt; endl; cout &lt;&lt; "location.block_size: " &lt;&lt; location.block_size &lt;&lt; endl; cout &lt;&lt; "location.osd: " &lt;&lt; location.osd &lt;&lt; endl;// cout &lt;&lt; "osd address: " &lt;&lt; location.osd_addr &lt;&lt; endl; &#125; 上面代码段就是与命令cephfs &lt;file_path&gt; show_location相关的代码。 123文件：src/client/ioctl.h#define CEPH_IOC_GET_DATALOC _IOWR(CEPH_IOCTL_MAGIC, 3, \ struct ceph_ioctl_dataloc) 查看ceph代码里的ioctl，如下，这里只是ceph-fuse客户端实现： 1234567891011121314151617181920212223文件：src/client/fuse_ll.cc#ifdef FUSE_IOCTL_COMPATstatic void fuse_ll_ioctl(fuse_req_t req, fuse_ino_t ino, int cmd, void *arg, struct fuse_file_info *fi, unsigned flags, const void *in_buf, size_t in_bufsz, size_t out_bufsz)&#123;... switch(cmd) &#123; case CEPH_IOC_GET_LAYOUT: &#123; file_layout_t layout; struct ceph_ioctl_layout l; Fh *fh = (Fh*)fi-&gt;fh; cfuse-&gt;client-&gt;ll_file_layout(fh, &amp;layout); l.stripe_unit = layout.stripe_unit; l.stripe_count = layout.stripe_count; l.object_size = layout.object_size; l.data_pool = layout.pool_id; fuse_reply_ioctl(req, 0, &amp;l, sizeof(struct ceph_ioctl_layout)); &#125; break; default: fuse_reply_err(req, EINVAL); &#125;&#125; 看到ceph-fuse仅仅支持CEPH_IOC_GET_LAYOUTioctl命令。 查看Linux的kernel代码，看相关cephfs的ioctl部分： 12345678910111213141516171819202122文件：fs/ceph/ioctl.c/* * Return object name, size/offset information, and location (OSD * number, network address) for a given file offset. */static long ceph_ioctl_get_dataloc(struct file *file, void __user *arg)&#123; struct ceph_ioctl_dataloc dl; struct inode *inode = file_inode(file);... dl.file_offset -= dl.object_offset; dl.object_size = ci-&gt;i_layout.object_size; dl.block_size = ci-&gt;i_layout.stripe_unit; /* block_offset = object_offset % block_size */ tmp = dl.object_offset; dl.block_offset = do_div(tmp, dl.block_size); snprintf(dl.object_name, sizeof(dl.object_name), "%llx.%08llx", ceph_ino(inode), dl.object_no);...&#125; 看出file的location信息是根据其layout信息和inode规则生成的。 Luminous版本在Luminous版本里，没有找到src/cephfs.cc文件，那就查查其它相关代码。 想象一个file的什么过程会要求获取到Rados的映射信息？首先想到的就是read/write，那就看Ceph Luminous版本里的相关代码吧~ 1234567891011121314151617181920212223242526272829303132333435363738394041424344文件：src/client/Client.ccint Client::_write(Fh *f, int64_t offset, uint64_t size, const char *buf, const struct iovec *iov, int iovcnt)&#123;... // async, caching, non-blocking. r = objectcacher-&gt;file_write(&amp;in-&gt;oset, &amp;in-&gt;layout, in-&gt;snaprealm-&gt;get_snap_context(), offset, size, bl, ceph::real_clock::now(), 0);...&#125;文件：src/osdc/ObjectCacher.hclass ObjectCacher &#123;... int file_write(ObjectSet *oset, file_layout_t *layout, const SnapContext&amp; snapc, loff_t offset, uint64_t len, bufferlist&amp; bl, ceph::real_time mtime, int flags) &#123; OSDWrite *wr = prepare_write(snapc, bl, mtime, flags, 0); Striper::file_to_extents(cct, oset-&gt;ino, layout, offset, len, oset-&gt;truncate_size, wr-&gt;extents); return writex(wr, oset, NULL); &#125;...&#125;;文件：osdc/Striper.hclass Striper &#123;... static void file_to_extents(CephContext *cct, inodeno_t ino, const file_layout_t *layout, uint64_t offset, uint64_t len, uint64_t trunc_size, vector&lt;ObjectExtent&gt;&amp; extents) &#123; // generate prefix/format char buf[32]; snprintf(buf, sizeof(buf), "%llx.%%08llx", (long long unsigned)ino); file_to_extents(cct, buf, layout, offset, len, trunc_size, extents); &#125;...&#125;;可以看出file到extents的转换格式为：&lt;ino.%%08llx&gt; 也就是说在CephFS中file到Rados里object的映射关系如下。 object命名规则：&lt;file inode number&gt;.&lt;slice number&gt; 验证12345678910111213141516171819root@ceph0:/mnt/cephfs# dd if=/dev/zero of=4Mfile bs=4M count=11+0 records in1+0 records out4194304 bytes (4.2 MB, 4.0 MiB) copied, 0.00866722 s, 484 MB/sroot@ceph0:/mnt/cephfs# ll -ihtotal 4.1M 1 drwxr-xr-x 1 root root 40G Jun 7 17:33 ./ 15466497 drwxr-xr-x 3 root root 4.0K Jun 4 15:19 ../1099511628901 -rw-r--r-- 1 root root 4.0M Jun 7 17:33 4Mfileroot@ceph0:/mnt/cephfs# stat 4Mfile File: '4Mfile' Size: 4194304 Blocks: 8192 IO Block: 4194304 regular fileDevice: 10006bh/1048683d Inode: 1099511628901 Links: 1Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root)Access: 2018-06-07 17:33:12.451473976 +0800Modify: 2018-06-07 17:44:11.141674057 +0800Change: 2018-06-07 17:44:11.141674057 +0800 Birth: - 1099511628901转换为16进制为：0x10000000465 查看文件的layout信息： 123root@ceph0:/mnt/cephfs# getfattr -n ceph.file.layout 4Mfile# file: 4Mfileceph.file.layout="stripe_unit=4194304 stripe_count=1 object_size=4194304 pool=cephfs_data" 查看Rados里的object和其map信息： 1234567root@ceph0:/mnt/cephfs# rados ls -p cephfs_data | grep -i 1000000046510000000465.00000000root@ceph0:/mnt/cephfs# rados -p cephfs_data stat 10000000465.00000000cephfs_data/10000000465.00000000 mtime 2018-06-07 17:33:12.000000, size 4194304root@ceph0:/mnt/cephfs# ceph osd map cephfs_data 10000000465.00000000osdmap e5770 pool 'cephfs_data' (2) object '10000000465.00000000' -&gt; pg 2.3137aa5e (2.5e) -&gt; up ([2,6], p2) acting ([2,6], p2)]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>cephfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph自动化部署]]></title>
    <url>%2F2018%2F06%2F02%2Fceph-automatic-deploy%2F</url>
    <content type="text"><![CDATA[一、概述Ceph当前的自动化部署有两个，分别是ceph-deploy和ceph-ansible，语言都是python，对应的github库地址为：ceph-deploy: https://github.com/ceph/ceph-deployceph-ansible: https://github.com/ceph/ceph-ansible ceph-deploy在手动部署中用的比较多，要实现自动化部署，必须再写脚本执行；ceph-ansible基于ansible，可以实现根据配置文件的ceph自动化部署； 我在之前写过一个基于ceph-deploy部署ceph的自动化脚本，用起来比较方便熟练；所以这里我写的自动化部署是基于自写脚本调用ceph-deploy的自动化部署； 二、脚本这里有两个文件： 12root@ceph0:~/ceph-deploy# lsceph-auto-deploy.py deploy.conf ceph-auto-deploy.py: 执行自动化部署的python脚本 deploy.conf：ceph deploy的配置文件，里面包括了ceph cluster的配置信息，ceph集群的参数优化信息等； deploy.conf该文件里需要用户配置的信息如下：（在文件底部） 12345678910111213141516171819#################### host-specific## Put your settings for each node here, these settings will be removed## from the ultimate ceph.conf[host-specific]mon_hosts = ceph0, ceph1, ceph2osd_hosts = ceph0, ceph1rgw_hosts = ceph0 # Example configuration use file system path#osd.ceph0.paths = /var/lib/ceph/osd/ceph-0, /var/lib/ceph/osd/ceph-1#osd.ceph1.paths = /var/lib/ceph/osd/ceph-2, /var/lib/ceph/osd/ceph-3osd.ceph0.devs = vdb:vde, vdc:vde, vdd:vdeosd.ceph1.devs = vdb, vdc, vdd ## for multi network settings## public network 1;cluster network1 = Host1, Host2, Host3## public network 2;cluster network2 = Host4, Host5192.10.4.0/24;192.10.4.0/24 = ceph0, ceph1, ceph2 指定Ceph组件部署在哪些Host上： mon_hosts = ceph0, ceph1, ceph2 Ceph Monitors部署ceph0, ceph1, ceph2上 osd_hosts = ceph0, ceph1, ceph2 Ceph OSDs部署ceph0, ceph1上 rgw_hosts = ceph0 Ceph RadosGW部署ceph0上（若不配置RadosGW，可注释掉改行） 指定每个部署OSDs的host上，哪些磁盘来启动osd：（于上述配置osd_hosts对应） 1osd.ceph0.devs = vdb:vde, vdc:vde, vdd:vde 在ceph0上，启动 vdb, vdc, vde三个osd，vde作为这三个osd的journal盘 1osd.ceph1.devs = vdb, vdc, vdd 在ceph1上，启动 vdb, vdc, vde三个osd，每个盘会自动格式化一个分区作为journal 注: 在deploy.conf里可配置journal size ceph-auto-deploy.py该文件是自动化部署ceph的执行脚本，支持的命令有： 12345678910root@ceph0:~/ceph-deploy# python ceph-auto-deploy.py --helpceph-auto-deploy.py usage: python ceph-auto-deploy.py run this script with default configuration -h, --help print help message -v, --version print script version -c [configure file] specify the configure file -p just purge older ceph and ceph data -r do ceph purge and ceph reinstall --just-deploy-osds just do deploy ceph osds --debug just print out the commands 常用的命令有： python ceph-auto-deploy.py根据deploy.conf的配置，部署ceph集群 python ceph-auto-deploy.py -p根据deploy.conf的配置，删除旧ceph系统和其中ceph相关数据 python ceph-auto-deploy.py -r根据deploy.conf的配置，执行purge后，重新安装ceph系统相关packages（不会部署ceph集群） 三、依赖执行上述python自动化部署ceph集群，依赖ceph-deploy，整体架构如下： 其中ceph-deploy所在节点也可以是ceph的node节点。 所以有如下两个依赖： admin-node上安装ceph-deploy admin-node到ceph nodes的无密码登陆 四、部署步骤这里以三个虚拟机自动化部署ceph集群为例，说明该自动化脚本的使用： 三个节点分别为： node Hostname Network Devices node1 ceph0 192.10.4.109 vdb, vdc, vdd, vde node2 ceph1 192.10.4.110 vdb, vdc, vdd, vde node3 ceph2 192.10.4.111 vdb, vdc, vdd, vde 每个node节点的系统信息为： 123456root@ceph0:~# lsb_release -aNo LSB modules are available.Distributor ID: UbuntuDescription: Ubuntu 16.04.1 LTSRelease: 16.04Codename: xenial 确定nodes节点角色首先我们需要确定使用哪个node作为admin-node，然后确定ceph集群nodes节点都部署哪些组件，包括Monitor、OSD、RadosGW、MDS等； 根据ceph集群部署需求，我们可以做如下node节点角色规划： node Hostname Role ceph Network Ceph Component OSD/Journal Devices node1 ceph0 admin-node ceph-node 192.10.4.109 Monitor, OSD OSD: vdb, vdc, vdd Journal: vde node2 ceph1 ceph-node 192.10.4.110 Monitor, OSD OSD: vdb, vdc, vdd Journal为OSD盘的一个分区 node3 ceph2 ceph-node 192.10.4.111 Monitor 不配置OSD 注：上述规划与第二章节中的 deploy.conf 配置保持一致 admin-node节点安装ceph-deploy如前文所述，需要在选择的admin-node上安装自动化部署依赖的ceph-deploy，步骤如下： 12345678910111213141516171819root@ceph0:~# apt-get install -y ceph-deployroot@ceph0:~# ceph-deploy --helpusage: ceph-deploy [-h] [-v | -q] [--version] [--username USERNAME] [--overwrite-conf] [--cluster NAME] [--ceph-conf CEPH_CONF] COMMAND ... Easy Ceph deployment -^- / \ |O o| ceph-deploy v1.5.32 ).-.( '/|||\` | '|` | '|` Full documentation can be found at: http://ceph.com/ceph-deploy/docs ... 获取pkg源的ceph版本信息我们当前推荐安装Ceph的Jewel版本（10.2.*），在Ubuntu系统上，我们可以通过下面命令获取其对应的Ceph pkgs版本信息： 123456789root@ceph0:~# apt-get update...root@ceph0:~# apt-cache show cephPackage: cephArchitecture: amd64Version: 10.2.9-0ubuntu0.16.04.1Priority: optionalSection: adminOrigin: Ubuntu 注：若对应Ceph不是Jewel版本，需要更换pkg源 admin-node节点配置无秘访问ceph nodes自动化部署脚本会通过ssh自动在ceph nodes节点执行命令，ceph-deploy也依赖ssh的无秘访问nodes节点，所以这里我们要先配置这些： /etc/hosts上添加ceph nodes节点的hostname与ip的对应关系 12345vim /etc/hosts...192.10.4.109 ceph0192.10.4.110 ceph1192.10.4.111 ceph2 生成admin-node节点的ssh key 1234root@ceph0:~# ssh-keygen...root@ceph0:~# cat ~/.ssh/id_rsa.pubssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCqIu+xLrVo0e0++z3i/zdAbZqqqWeyKshA47oUJzQneqD9HP39AJ8btGS5Pow4I/V/1zGsRJ2iV4EDxMymw2wwJ9LyxLl81WsfhSsuOVo7uvhlu3PiU6xDpr8UK66Sv7lquQ67yx9UnH9Vra2TQFsWPwbiPZm+N+qdXuXHeX4RVpOKlKmtEWN40Q3AZt3sbFe5hfAfP08v8XM70znFIEHKbhlN4XpLmuQxZ+vdxAY7kOA1EUugSZpc5nfc61SnCEzhI0stW+ccDsD/vmZsnCtITitS0YqGYJYvGHziKWfmZAw3ZtQd9CQIlZHnmqyA7vF4eoFSoj6YhkI4ozHzxLqt root@ceph0 在其他节点填入ceph0的public key 12root@ceph0:~# cat ~/.ssh/authorized_keysssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCqIu+xLrVo0e0++z3i/zdAbZqqqWeyKshA47oUJzQneqD9HP39AJ8btGS5Pow4I/V/1zGsRJ2iV4EDxMymw2wwJ9LyxLl81WsfhSsuOVo7uvhlu3PiU6xDpr8UK66Sv7lquQ67yx9UnH9Vra2TQFsWPwbiPZm+N+qdXuXHeX4RVpOKlKmtEWN40Q3AZt3sbFe5hfAfP08v8XM70znFIEHKbhlN4XpLmuQxZ+vdxAY7kOA1EUugSZpc5nfc61SnCEzhI0stW+ccDsD/vmZsnCtITitS0YqGYJYvGHziKWfmZAw3ZtQd9CQIlZHnmqyA7vF4eoFSoj6YhkI4ozHzxLqt root@ceph0 注：因为ceph0既是admin-node，也是ceph node，所以也需要配置本身的ssh无秘访问 admin-node节点执行自动化部署之前的准备工作做完后，就可以通过自动化脚本部署Ceph集群了 获取ceph deploy自动化脚本 GitLab地址：https://github.com/ictfox/ceph-deploy 123root@ceph0:~# cd ceph-deployroot@ceph0:~/ceph-deploy# lsceph-auto-deploy.py deploy.conf 根据第二章节介绍，配置 deploy.conf 文件 1234567891011121314151617181920root@ceph0:~/ceph-deploy# vim deploy.conf#################### host-specific## Put your settings for each node here, these settings will be removed## from the ultimate ceph.conf[host-specific]mon_hosts = ceph0, ceph1, ceph2osd_hosts = ceph0, ceph1rgw_hosts = ceph0 # Example configuration use file system path#osd.ceph0.paths = /var/lib/ceph/osd/ceph-0, /var/lib/ceph/osd/ceph-1#osd.ceph1.paths = /var/lib/ceph/osd/ceph-2, /var/lib/ceph/osd/ceph-3osd.ceph0.devs = vdb:vde, vdc:vde, vdd:vdeosd.ceph1.devs = vdb, vdc, vdd ## for multi network settings## public network 1;cluster network1 = Host1, Host2, Host3## public network 2;cluster network2 = Host4, Host5192.10.4.0/24;192.10.4.0/24 = ceph0, ceph1, ceph2 安装Ceph对应packages 12root@ceph0:~# python ceph-auto-deploy.py -r... 这个会花费些时间，但每个Ceph node上安装ceph相关pkgs是并行的，时间不会太长； 执行命令，自动化部署Ceph集群 12root@ceph0:~# python ceph-auto-deploy.py... 这个会花费较长时间，因为ceph-deploy部署osd时是串行的，为了使得每个Host上的osd id是连续的，方便之后ceph集群使用。 中间也可能出错，错误信息会输出到屏幕上，详细错误信息也会输出到同目录下的文件：ceph-deploy-ceph.log 里，方便出错时候查看定位问题。]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>ceph-deploy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph daemonperf tool分析]]></title>
    <url>%2F2018%2F05%2F17%2Fceph-daemonperf-intro%2F</url>
    <content type="text"><![CDATA[命令格式Ceph有个daemonperf工具，结合Ceph daemon的asok文件，可以检查Ceph各个组件的当前状态。 命令格式：ceph daemonperf 示例如下： 1234567891011121314151617# ceph daemonperf /var/run/ceph/ceph-osd.0.asok 2---objecter--- -----------osd-----------writ read actv|recop rd wr lat ops | 0 0 0 | 0 12k 0 0 1 0 0 0 | 0 532k 0 0 62 0 0 0 | 0 106k 0 0 10 0 0 0 | 0 487k 4.2M 1 71# ceph daemonperf /var/run/ceph/ceph-mds.mds-ceph0.asok 2-----mds------ --mds_server-- ---objecter--- -----mds_cache----- ---mds_log----rlat inos caps|hsr hcs hcr |writ read actv|recd recy stry purg|segs evts subm| 0 137k 22k| 0 0 81 | 4 0 129 | 0 0 108k 4 | 54 40k 43 0 139k 15k| 0 0 7.5k|941 1 132 | 0 0 111k 918 | 46 34k 4.7k 0 145k 16k| 0 0 11k|592 0 129 | 0 0 116k 581 | 54 40k 6.3k 0 123k 7.3k| 0 1 6.9k|231 1 129 | 0 0 119k 214 | 48 33k 3.7k 0 128k 8.2k| 0 0 9.0k|509 0 130 | 0 0 123k 503 | 54 38k 5.0k 1 129k 8.3k| 0 0 924 |405 1 128 | 0 0 123k 402 | 55 39k 867 源码实现文件：ceph.in 1234567891011121314151617181920212223242526272829def main(): ... if parsed_args.admin_socket: sockpath = parsed_args.admin_socket elif len(childargs) &gt; 0 and childargs[0] in ["daemon", "daemonperf"]: daemon_perf = (childargs[0] == "daemonperf") # Treat "daemon &lt;path&gt;" or "daemon &lt;name&gt;" like --admin_daemon &lt;path&gt; # Handle "daemonperf &lt;path&gt;" the same but requires no trailing args require_args = 2 if daemon_perf else 3 ... if sockpath and daemon_perf: interval = 1 count = None if len(childargs) &gt; 0: try: interval = float(childargs[0]) if interval &lt; 0: raise ValueError except ValueError: print('daemonperf: interval should be a positive number', file=sys.stderr) return errno.EINVAL if len(childargs) &gt; 1: if not childargs[1].isdigit(): print('daemonperf: count should be a positive integer', file=sys.stderr) return errno.EINVAL count = int(childargs[1]) DaemonWatcher(sockpath).run(interval, count) return 0 ... 文件：pybind/ceph_daemon.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354class DaemonWatcher(object): """ Given a Ceph daemon's admin socket path, poll its performance counters and output a series of output lines showing the momentary values of counters of interest (those with the 'nick' property in Ceph's schema) """ ... def _load_schema(self): """ Populate our instance-local copy of the daemon's performance counter schema, and work out which stats we will display. """ self._schema = json.loads(admin_socket(self.asok_path, ["perf", "schema"])) # Build list of which stats we will display, based on which # stats have a nickname self._stats = defaultdict(dict) for section_name, section_stats in self._schema.items(): for name, schema_data in section_stats.items(): if schema_data.get('nick'): self._stats[section_name][name] = schema_data['nick'] def run(self, interval, count=None, ostr=sys.stdout): """ Print output at regular intervals until interrupted. :param ostr: Stream to which to send output """ self._load_schema() self._colored = self.supports_color(ostr) self._print_headers(ostr) last_dump = json.loads(admin_socket(self.asok_path, ["perf", "dump"])) rows_since_header = 0 term_height = 25 try: while True: dump = json.loads(admin_socket(self.asok_path, ["perf", "dump"])) if rows_since_header &gt; term_height - 2: self._print_headers(ostr) rows_since_header = 0 self._print_vals(ostr, dump, last_dump) if count is not None: count -= 1 if count &lt;= 0: break rows_since_header += 1 last_dump = dump time.sleep(interval) except KeyboardInterrupt: return 所以ceph daemonperf &lt;asok&gt;命令的输出是根据ceph daemon &lt;asok&gt; perf dump/schema的输出整理的。 实现中存了上一次的perf dump结果，所以这里获取的值是interval里的数据统计。 以ceph mds asok的perf dump/schema输出为例，看看每个项是什么含义： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104ceph daemon &lt;mds-asok&gt; perf schema&#123; "mds": &#123; "reply_latency": &#123; "type": 5, "description": "Reply latency", "nick": "rlat" &#125;, ... "inodes": &#123; "type": 2, "description": "Inodes", "nick": "inos" &#125;, ... "caps": &#123; "type": 2, "description": "Capabilities", "nick": "caps" &#125;, ... &#125;, "mds_cache": &#123; "num_strays": &#123; "type": 2, "description": "Stray dentries", "nick": "stry" &#125;, ... "strays_purged": &#123; "type": 10, "description": "Stray dentries purged", "nick": "purg" &#125;, ... "num_recovering_enqueued": &#123; "type": 2, "description": "Files waiting for recovery", "nick": "recy" &#125;, ... "recovery_completed": &#123; "type": 10, "description": "File recoveries completed", "nick": "recd" &#125; &#125;, "mds_log": &#123; "evadd": &#123; "type": 10, "description": "Events submitted", "nick": "subm" &#125;, ... "ev": &#123; "type": 2, "description": "Events", "nick": "evts" &#125;, ... "seg": &#123; "type": 2, "description": "Segments", "nick": "segs" &#125;, ... &#125;, "mds_server": &#123; "handle_client_request": &#123; "type": 10, "description": "Client requests", "nick": "hcr" &#125;, "handle_slave_request": &#123; "type": 10, "description": "Slave requests", "nick": "hsr" &#125;, "handle_client_session": &#123; "type": 10, "description": "Client session messages", "nick": "hcs" &#125;, ... &#125;, "objecter": &#123; "op_active": &#123; "type": 2, "description": "Operations active", "nick": "actv" &#125;, "op_r": &#123; "type": 10, "description": "Read operations", "nick": "read" &#125;, "op_w": &#123; "type": 10, "description": "Write operations", "nick": "writ" &#125;, ... &#125;,&#125; schema输出里的含义： 123456789101112131415161718192021222324252627文件：common/perf_counters.henum perfcounter_type_d&#123; PERFCOUNTER_NONE = 0, PERFCOUNTER_TIME = 0x1, PERFCOUNTER_U64 = 0x2, PERFCOUNTER_LONGRUNAVG = 0x4, PERFCOUNTER_COUNTER = 0x8,&#125;;class PerfCounters&#123; /** Represents a PerfCounters data element. */ struct perf_counter_data_any_d &#123; ... const char *name; const char *description; const char *nick; enum perfcounter_type_d type; atomic64_t u64; atomic64_t avgcount; atomic64_t avgcount2; ... &#125;; ...&#125;; ceph daemon perf dump中与daemonperf相关的输出项： 123456789101112131415161718192021222324252627282930313233343536373839404142434445ceph daemon &lt;mds-asok&gt; perf dump&#123; "mds": &#123; ... "reply_latency": &#123; "avgcount": 1879241, "sum": 563.417555661 &#125;, ... "inodes": 510943, ... "caps": 68002, ... &#125;, "mds_cache": &#123; "num_strays": 415633, ... "strays_purged": 1192320, ... "num_recovering_enqueued": 0, ... "recovery_completed": 9 &#125;, "mds_log": &#123; "evadd": 2094629, ... "ev": 42664, ... "seg": 57, ... &#125;, "mds_server": &#123; "handle_client_request": 1879915, "handle_slave_request": 0, "handle_client_session": 698, ... &#125;, "objecter": &#123; "op_active": 64, ... "op_r": 832, "op_w": 1219634, ... &#125;,&#125;]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Bcache加速Ceph OSD性能]]></title>
    <url>%2F2018%2F05%2F05%2Fceph-osd-deploy-with-bcache%2F</url>
    <content type="text"><![CDATA[概述在Ceph的环境中，我们通常会使用SSD来作为OSD的Journal，而OSD的数据盘是普通的SATA盘，在实践中，经常会发现SATA盘的性能瓶颈影响了OSD的性能，那能不能继续压榨SSD的性能来提升OSD的性能呢？ 答案是肯定的，可以使用SSD加速SATA盘的策略来加速作为OSD数据盘的SATA盘，通常的策略有： flashcache bcache 有文章对比测试过这两种cache策略的性能，bcache的性能会好很多，这里介绍如何使用bcache来给OSD加速。 测试环境Ceph版本：Jewel 10.2.9 作为Ceph机器的物理机的磁盘配置如下： SSD - 745 G，三块 SATA - 3.7 T，九块 对磁盘规划如下： 每个SSD分出3个10G分区，作为三个SATA盘OSD的journal 每个SSD剩余空间分为1个分区，使用bcache来加速三个SATA盘 磁盘分区前如下： 123456root@ceph0:~/yangguanjun# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT...sdb 8:16 0 745.2G 0 disksdc 8:32 0 745.2G 0 disksdd 8:48 0 745.2G 0 disk 磁盘分区后为： 123456789101112131415161718root@ceph0:~# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT...sdb 8:16 0 745.2G 0 disk├─sdb1 8:17 0 10G 0 part├─sdb2 8:18 0 10G 0 part├─sdb3 8:19 0 10G 0 part└─sdb4 8:20 0 715.2G 0 partsdc 8:32 0 745.2G 0 disk├─sdc1 8:33 0 10G 0 part├─sdc2 8:34 0 10G 0 part├─sdc3 8:35 0 10G 0 part└─sdc4 8:36 0 715.2G 0 partsdd 8:48 0 745.2G 0 disk├─sdd1 8:49 0 10G 0 part├─sdd2 8:50 0 10G 0 part├─sdd1 8:51 0 10G 0 part└─sdd4 8:52 0 715.2G 0 part 设备性能针对当前环境，先要了解下各个硬件的性能，通过fio测试结果如下： 磁盘类型 read write randread randwrite SATA 155MB/s 158MB/s 126 219 SSD 508MB/s 426MB/s 69.2k 45.5k 参考文章配置SATA盘与SSD盘的bcache策略：http://www.yangguanjun.com/2018/03/26/lvm-sata-ssd-bcache/ 配置bcache命令如下： 1# make-bcache -B /dev/sde -C /dev/sdc4 配置后的块设备信息如下： 1234567891011# lsblk...sdc 8:32 0 745.2G 0 disk├─sdc1 8:33 0 10G 0 part├─sdc2 8:34 0 10G 0 part├─sdc3 8:35 0 10G 0 part└─sdc4 8:36 0 715.2G 0 part ├─bcache0 253:0 0 3.7T 0 disk...sde 8:64 0 3.7T 0 disk└─bcache0 253:0 0 3.7T 0 disk 之后测试bcache加速设备性能： bcache的不同缓存策略 randwrite bcache [writethrough] 218 bcache [writeback] 38.8k 上面结果看出 bcache 配置为writeback模式后，加速设备性能很高，会比SSD盘的性能略差些。但之后数据会回刷到SATA上，通过iostat可以看到SATA盘会繁忙好一阵子。 部署OSD默认直接使用ceph-deploy部署bcache设备时会报错，如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465# ceph-deploy osd prepare ceph0:/dev/bcache0:/dev/sdd1[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf[ceph_deploy.cli][INFO ] Invoked (1.5.37): /usr/bin/ceph-deploy osd prepare ceph0:/dev/bcache0:/dev/sdd1[ceph_deploy.cli][INFO ] ceph-deploy options:[ceph_deploy.cli][INFO ] username : None[ceph_deploy.cli][INFO ] disk : [('ceph0', '/dev/bcache0', '/dev/sdd1')][ceph_deploy.cli][INFO ] dmcrypt : False[ceph_deploy.cli][INFO ] verbose : False[ceph_deploy.cli][INFO ] bluestore : None[ceph_deploy.cli][INFO ] overwrite_conf : False[ceph_deploy.cli][INFO ] subcommand : prepare[ceph_deploy.cli][INFO ] dmcrypt_key_dir : /etc/ceph/dmcrypt-keys[ceph_deploy.cli][INFO ] quiet : False[ceph_deploy.cli][INFO ] cd_conf : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7f81bab7b200&gt;[ceph_deploy.cli][INFO ] cluster : ceph[ceph_deploy.cli][INFO ] fs_type : xfs[ceph_deploy.cli][INFO ] func : &lt;function osd at 0x7f81bab6a938&gt;[ceph_deploy.cli][INFO ] ceph_conf : None[ceph_deploy.cli][INFO ] default_release : False[ceph_deploy.cli][INFO ] zap_disk : False[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph0:/dev/bcache0:/dev/sdd1[ceph0][DEBUG ] connected to host: ceph0[ceph0][DEBUG ] detect platform information from remote host[ceph0][DEBUG ] detect machine type[ceph0][DEBUG ] find the location of an executable[ceph_deploy.osd][INFO ] Distro info: CentOS Linux 7.4.1708 Core[ceph_deploy.osd][DEBUG ] Deploying osd to ceph0[ceph0][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf[ceph_deploy.osd][DEBUG ] Preparing host ceph0 disk /dev/bcache0 journal /dev/sdd1 activate False[ceph0][DEBUG ] find the location of an executable[ceph0][INFO ] Running command: /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/bcache0 /dev/sdd1[ceph0][WARNIN] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid[ceph0][WARNIN] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph[ceph0][WARNIN] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph[ceph0][WARNIN] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --log-file $run_dir/$cluster-osd-check.log --cluster ceph --setuser ceph --setgroup ceph[ceph0][WARNIN] get_dm_uuid: get_dm_uuid /dev/bcache0 uuid path is /sys/dev/block/252:8/dm/uuid[ceph0][WARNIN] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size[ceph0][WARNIN] get_dm_uuid: get_dm_uuid /dev/bcache0 uuid path is /sys/dev/block/252:8/dm/uuid[ceph0][WARNIN] get_dm_uuid: get_dm_uuid /dev/bcache0 uuid path is /sys/dev/block/252:8/dm/uuid[ceph0][WARNIN] get_dm_uuid: get_dm_uuid /dev/bcache0 uuid path is /sys/dev/block/252:8/dm/uuid[ceph0][WARNIN] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs[ceph0][WARNIN] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs[ceph0][WARNIN] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs[ceph0][WARNIN] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs[ceph0][WARNIN] get_dm_uuid: get_dm_uuid /dev/sdd1 uuid path is /sys/dev/block/8:51/dm/uuid[ceph0][WARNIN] prepare_device: Journal /dev/sdd1 is a partition[ceph0][WARNIN] get_dm_uuid: get_dm_uuid /dev/sdd1 uuid path is /sys/dev/block/8:51/dm/uuid[ceph0][WARNIN] prepare_device: OSD will not be hot-swappable if journal is not the same device as the osd data[ceph0][WARNIN] command: Running command: /usr/sbin/blkid -o udev -p /dev/sdd1[ceph0][WARNIN] prepare_device: Journal /dev/sdd1 was not prepared with ceph-disk. Symlinking directly.[ceph0][WARNIN] get_dm_uuid: get_dm_uuid /dev/bcache0 uuid path is /sys/dev/block/252:8/dm/uuid[ceph0][WARNIN] set_data_partition: Creating osd partition on /dev/bcache0[ceph0][WARNIN] get_dm_uuid: get_dm_uuid /dev/bcache0 uuid path is /sys/dev/block/252:8/dm/uuid[ceph0][WARNIN] ptype_tobe_for_name: name = data[ceph0][WARNIN] get_dm_uuid: get_dm_uuid /dev/bcache0 uuid path is /sys/dev/block/252:8/dm/uuid[ceph0][WARNIN] create_partition: Creating data partition num 1 size 0 on /dev/bcache0[ceph0][WARNIN] command_check_call: Running command: /usr/sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:cf599e47-c299-46c1-a385-aff4b0d25f1f --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/bcache0[ceph0][WARNIN] Caution: invalid main GPT header, but valid backup; regenerating main header[ceph0][WARNIN] from backup![ceph0][WARNIN][ceph0][WARNIN] Invalid partition data![ceph0][WARNIN] '/usr/sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:cf599e47-c299-46c1-a385-aff4b0d25f1f --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/bcache0' failed with status code 2[ceph0][ERROR ] RuntimeError: command returned non-zero exit status: 1[ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/bcache0 /dev/sdd1[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs 搜索有如下参考，ceph-deploy还不支持bcache设备，默认ceph-deploy会尝试对bcache设备进行分区，而bcache设备是不支持分区后挂载的，所以会导致命令失败。 http://tracker.ceph.com/issues/13278 https://github.com/ceph/ceph/pull/16327 解决办法一修改ceph-disk的代码，把上述链接中的patch加上，可惜没搞成功，很奇怪修改后的代码貌似没跑到。。。因时间紧急就没再研究，但这个办法肯定是可行的！ 解决办法二手动格式化bcache设备，mount上后，通过ceph-deploy指定目录来部署了，步骤如下： 123# mkdir /var/lib/ceph/osd/ceph-0# mkfs.xfs /dev/bcache0# mount /dev/bcache0 /var/lib/ceph/osd/ceph-0 然后再尝试部署，报错如下： 123456789101112131415161718192021222324252627# ceph-deploy --overwrite-conf osd prepare ceph0:/var/lib/ceph/osd/ceph-0:/dev/sdd1...[ceph0][WARNIN] command: Running command: /usr/bin/timeout 300 ceph-osd --cluster ceph --mkfs --mkkey -i 195 --monmap /var/lib/ceph/osd/ceph-0/activate.monmap --osd-data /var/lib/ceph/osd/ceph-0 --osd-journal /var/lib/ceph/osd/ceph-0/journal --osd-uuid 9ff0983b-74e2-4fc1-8ba8-cfb688024284 --keyring /var/lib/ceph/osd/ceph-0/keyring --setuser ceph --setgroup ceph[ceph0][WARNIN] Traceback (most recent call last):[ceph0][WARNIN] File "/usr/sbin/ceph-disk", line 9, in &lt;module&gt;[ceph0][WARNIN] load_entry_point('ceph-disk==1.0.0', 'console_scripts', 'ceph-disk')()[ceph0][WARNIN] File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 5553, in run[ceph0][WARNIN] main(sys.argv[1:])[ceph0][WARNIN] File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 5504, in main[ceph0][WARNIN] args.func(args)[ceph0][WARNIN] File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 3631, in main_activate[ceph0][WARNIN] init=args.mark_init,[ceph0][WARNIN] File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 3451, in activate_dir[ceph0][WARNIN] (osd_id, cluster) = activate(path, activate_key_template, init)[ceph0][WARNIN] File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 3556, in activate[ceph0][WARNIN] keyring=keyring,[ceph0][WARNIN] File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 3010, in mkfs[ceph0][WARNIN] '--setgroup', get_ceph_group(),[ceph0][WARNIN] File "/usr/lib/python2.7/site-packages/ceph_disk/main.py", line 2957, in ceph_osd_mkfs[ceph0][WARNIN] raise Error('%s failed : %s' % (str(arguments), error))[ceph0][WARNIN] ceph_disk.main.Error: Error: ['ceph-osd', '--cluster', 'ceph', '--mkfs', '--mkkey', '-i', u'195', '--monmap', '/var/lib/ceph/osd/ceph-0/activate.monmap', '--osd-data', '/var/lib/ceph/osd/ceph-0', '--osd-journal', '/var/lib/ceph/osd/ceph-0/journal', '--osd-uuid', u'9ff0983b-74e2-4fc1-8ba8-cfb688024284', '--keyring', '/var/lib/ceph/osd/ceph-0/keyring', '--setuser', 'ceph', '--setgroup', 'ceph'] failed : parse error setting 'osd_deep_scrub_interval' to '2592000 // every mouth'[ceph0][WARNIN] 2018-04-26 16:02:27.180328 7f3085e23940 -1 filestore(/var/lib/ceph/osd/ceph-0) mkfs: write_version_stamp() failed: (13) Permission denied[ceph0][WARNIN] 2018-04-26 16:02:27.180346 7f3085e23940 -1 OSD::mkfs: ObjectStore::mkfs failed with error -13[ceph0][WARNIN] 2018-04-26 16:02:27.180441 7f3085e23940 -1 ** ERROR: error creating empty object store in /var/lib/ceph/osd/ceph-0: (13) Permission denied[ceph0][WARNIN][ceph0][ERROR ] RuntimeError: command returned non-zero exit status: 1[ceph_deploy][ERROR ] RuntimeError: Failed to execute command: /usr/sbin/ceph-disk -v activate --mark-init systemd --mount /var/lib/ceph/osd/ceph-0 从输出里看是权限的问题，执行如下两条命令后，部署成功： 12root@ceph0:/var/lib/ceph/osd# chown -R ceph:ceph /dev/sdd1root@ceph0:/var/lib/ceph/osd# chown -R ceph:ceph ceph-0 Ceph OSD开机自启动bcache开机启动添加bcache开机启动 123456789# cat /etc/sysconfig/modules/bcache.modules#! /bin/sh/sbin/modinfo -F filename bcache &gt; /dev/null 2&gt;&amp;1if [ $? -eq 0 ]; then /sbin/modprobe -f bcachefi# chmod 755 /etc/sysconfig/modules/bcache.modules 自动挂载OSD目录添加磁盘自动挂载，保证重启后Ceph OSD能自动运行 123456789101112131415root@ceph0:~# blkid/dev/sdd1: PARTUUID="81732128-2073-4d93-8582-377f4f9a701f".../dev/bcache0: UUID="88ef6ba6-fd13-478c-8b56-82951183f1d3" TYPE="xfs"root@ceph0:~# cat /etc/fstab## /etc/fstab# Created by anaconda on Wed May 24 15:20:03 2017## Accessible filesystems, by reference, are maintained under '/dev/disk'# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info#LABEL=/ / ext4 defaults 1 1UUID=88ef6ba6-fd13-478c-8b56-82951183f1d3 /var/lib/ceph/osd/ceph-0 xfs defaults 0 2]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>bcache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph-deploy 2.0.0 部署 Ceph Luminous 12.2.4]]></title>
    <url>%2F2018%2F04%2F06%2Fceph-deploy-latest-luminous%2F</url>
    <content type="text"><![CDATA[需求Ceph Luminous已经发布到12.2.4版本，经历了前几个版本的磨炼，Luminous版本也越来越稳定，它的Bluestore属性和CephFS提供的多主MDS一直是我们关注的重点。 参考ceph和ceph-deploy官方文档，本文介绍下使用ceph-deploy部署最新版Ceph Luminous 12.2.4，以及部署中遇到的问题。 软件版本ceph-deploy版本12# ceph-deploy --version2.0.0 ceph-deploy 2.0.0的changelog： 123456782.0.016-Jan-2018- Backward incompatible API changes for OSD creation - will use ceph-volume and no longer consume ceph-disk.- Remove python-distribute dependency- Use /etc/os-release as a fallback when linux_distribution() doesn’t work- Drop dmcrypt support (unsupported by ceph-volume for now)- Allow debug modes for ceph-volume 参考：http://docs.ceph.com/ceph-deploy/docs/changelog.html#id1 系统版本123456# lsb_release -aLSB Version: :core-4.1-amd64:core-4.1-noarchDistributor ID: CentOSDescription: CentOS Linux release 7.3.1611 (Core)Release: 7.3.1611Codename: Core Ceph版本yum源配置，选择最新的Luminous版本： 123456789101112131415161718192021222324# cat /etc/yum.repos.d/ceph.repo[ceph]name=Ceph packages for $basearchbaseurl=http://mirrors.163.com/ceph/rpm-luminous/el7/$basearchenabled=1priority=2gpgcheck=1gpgkey=https://download.ceph.com/keys/release.asc[ceph-noarch]name=Ceph noarch packagesbaseurl=http://mirrors.163.com/ceph/rpm-luminous/el7/noarchenabled=1priority=2gpgcheck=1gpgkey=https://download.ceph.com/keys/release.asc[ceph-source]name=Ceph source packagesbaseurl=http://mirrors.163.com/ceph/rpm-luminous/el7/SRPMSenabled=0priority=2gpgcheck=1gpgkey=https://download.ceph.com/keys/release.asc ceph安装 123456# yum install -y ceph ceph-radosgwor# ceph-deploy install [hosts]# ceph -vceph version 12.2.4 (52085d5249a80c5f5121a76d6288429f35e4e77b) luminous (stable) 准备工作使用ceph-deploy开始部署前，有如下几点要提前做好 部署节点安装 ceph-deploy 1# yum install -y ceph-deploy 部署节点与ceph nodes之间ssh无密码访问 12# ssh-keygen# cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys ceph nodes的hostname配置 1# vim /etc/hostname 部署节点/etc/hosts配置 ceph nodes的hostname与ip的对应 1234# vim /etc/hosts100.60.0.20 ceph0100.60.0.21 ceph1100.60.0.22 ceph2 ceph nodes配置ntp server 12# yum install -y ntp ntpdate ntp-doc# systemctl start ntpd ceph nodes安装ssh server 1# yum install -y openssh-server ceph nodes关闭或配置防火墙 12# systemctl stop firewalld# systemctl disable firewalld ​ Ceph Monitor部署开始部署Ceph Cluster，创建三个monitors： 12# ceph-deploy new ceph0 ceph1 ceph2# ceph-deploy mon create ceph0 ceph1 ceph2 创建ceph keys： 1# ceph-deploy gatherkeys ceph0 ceph1 ceph2 一定要开启ceph 认证，不然该命令执行失败 与ceph-deploy mon create命令执行完要有一定时间间隔，等到monitor集群正常了才能执行 分发ceph配置和admin key到ceph集群节点： 1# ceph-deploy admin ceph0 ceph1 ceph2 检查当前ceph集群状态： 123456789101112131415# ceph -s cluster: id: 5b2192ff-299c-4024-9f10-93af008e66d3 health: HEALTH_OK services: mon: 3 daemons, quorum ceph0,ceph2,ceph1 mgr: no daemons active osd: 0 osds: 0 up, 0 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0 bytes usage: 0 kB used, 0 kB / 0 kB avail pgs: 参考：http://docs.ceph.com/ceph-deploy/docs/index.html#creating-a-new-configuration Ceph manager部署部署ceph-mgr组件： 12345678910111213141516# ceph-deploy mgr create ceph0 ceph1 ceph2# ceph -s cluster: id: 5b2192ff-299c-4024-9f10-93af008e66d3 health: HEALTH_OK services: mon: 3 daemons, quorum ceph0,ceph2,ceph1 mgr: ceph0(active), standbys: ceph2, ceph1 osd: 0 osds: 0 up, 0 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0 bytes usage: 0 kB used, 0 kB / 0 kB avail pgs: 参考：http://docs.ceph.com/docs/master/mgr/ Ceph Luminous主推的用于Ceph集群管理的组件，默认在所有部署ceph-mon的节点都启动一个ceph-mgr Ceph OSD部署使用ceph-deploy工具来部署ceph osd： 123456789101112131415161718192021222324252627# ceph-deploy osd create -husage: ceph-deploy osd create [-h] [--data DATA] [--journal JOURNAL] [--zap-disk] [--fs-type FS_TYPE] [--dmcrypt] [--dmcrypt-key-dir KEYDIR] [--filestore] [--bluestore] [--block-db BLOCK_DB] [--block-wal BLOCK_WAL] [--debug] [HOST]positional arguments: HOST Remote host to connectoptional arguments: -h, --help show this help message and exit --data DATA The OSD data logical volume (vg/lv) or absolute path to device --journal JOURNAL Logical Volume (vg/lv) or path to GPT partition --zap-disk DEPRECATED - cannot zap when creating an OSD --fs-type FS_TYPE filesystem to use to format DEVICE (xfs, btrfs) --dmcrypt use dm-crypt on DEVICE --dmcrypt-key-dir KEYDIR directory where dm-crypt keys are stored --filestore filestore objectstore --bluestore bluestore objectstore --block-db BLOCK_DB bluestore block.db path --block-wal BLOCK_WAL bluestore block.wal path --debug Enable debug mode on remote ceph-volume calls 注：从Ceph Luminous 12.2.2开始， ceph-disk 就被弃用了，开始使用新的工具ceph-volume。 参考：http://docs.ceph.com/docs/master/ceph-volume/ 创建一个bluestore的osd，有以下几种设备选择： A block device, a block.wal, and a block.db device A block device and a block.wal device A block device and a block.db device A single block device 参考：http://docs.ceph.com/docs/master/ceph-volume/lvm/prepare/#bluestore block device也有如下三种选项： 整块磁盘 磁盘分区 逻辑卷（a logical volume of LVM） 配置使用整块磁盘时，ceph-volume会自动创建一个logical volume使用 单独块设备创建OSD整块磁盘命令格式： 1# ceph-deploy osd create [host] --data [/path/to/device] 首先销毁磁盘的分区信息： 12345678910111213[root@ceph0 ceph-deploy]# ceph-deploy disk zap ceph0 /dev/sdb...[ceph_deploy][ERROR ] Traceback (most recent call last):[ceph_deploy][ERROR ] File "/usr/lib/python2.7/site-packages/ceph_deploy/util/decorators.py", line 69, in newfunc[ceph_deploy][ERROR ] return f(*a, **kw)[ceph_deploy][ERROR ] File "/usr/lib/python2.7/site-packages/ceph_deploy/cli.py", line 164, in _main[ceph_deploy][ERROR ] return args.func(args)[ceph_deploy][ERROR ] File "/usr/lib/python2.7/site-packages/ceph_deploy/osd.py", line 438, in disk[ceph_deploy][ERROR ] disk_zap(args)[ceph_deploy][ERROR ] File "/usr/lib/python2.7/site-packages/ceph_deploy/osd.py", line 336, in disk_zap[ceph_deploy][ERROR ] if args.debug:[ceph_deploy][ERROR ] AttributeError: 'Namespace' object has no attribute 'debug'[ceph_deploy][ERROR ] 修改osd.py如下： 123[root@ceph0 ceph-deploy]# vim /usr/lib/python2.7/site-packages/ceph_deploy/osd.py #if args.debug: if False: 再尝试disk zap成功： 12345678910111213[root@ceph0 ceph-deploy]# ceph-deploy disk zap ceph0 /dev/sdb...[ceph0][INFO ] Running command: /usr/sbin/ceph-volume lvm zap /dev/sdb[ceph0][DEBUG ] --&gt; Zapping: /dev/sdb[ceph0][DEBUG ] Running command: cryptsetup status /dev/mapper/[ceph0][DEBUG ] stdout: /dev/mapper/ is inactive.[ceph0][DEBUG ] Running command: wipefs --all /dev/sdb[ceph0][DEBUG ] Running command: dd if=/dev/zero of=/dev/sdb bs=1M count=10[ceph0][DEBUG ] stderr: 10+0 records in[ceph0][DEBUG ] 10+0 records out[ceph0][DEBUG ] 10485760 bytes (10 MB) copied[ceph0][DEBUG ] stderr: , 0.0110867 s, 946 MB/s[ceph0][DEBUG ] --&gt; Zapping successful for: /dev/sdb 从输出看，ceph-deploy调用ceph-volume lvm zap，最后是执行dd命令往disk前10M写入全0数据 创建OSD： 12345# ceph-deploy osd create ceph0 --data /dev/sdb...[ceph0][INFO ] Running command: /usr/sbin/ceph-volume --cluster ceph lvm create --bluestore --data /dev/sdb...[ceph_deploy.osd][DEBUG ] Host ceph0 is now ready for osd use. 检查OSD： 1234567891011121314151617181920# df -hFilesystem Size Used Avail Use% Mounted on...tmpfs 16G 48K 16G 1% /var/lib/ceph/osd/ceph-0# ll /var/lib/ceph/osd/ceph-0/total 48-rw-r--r-- 1 ceph ceph 393 Apr 2 18:11 activate.monmaplrwxrwxrwx 1 ceph ceph 93 Apr 2 18:11 block -&gt; /dev/ceph-5b2192ff-299c-4024-9f10-93af008e66d3/osd-block-fe7cd3b1-8513-4be5-b9a8-88fd47dca679-rw-r--r-- 1 ceph ceph 2 Apr 2 18:11 bluefs-rw-r--r-- 1 ceph ceph 37 Apr 2 18:11 ceph_fsid-rw-r--r-- 1 ceph ceph 37 Apr 2 18:11 fsid-rw------- 1 ceph ceph 55 Apr 2 18:11 keyring-rw-r--r-- 1 ceph ceph 8 Apr 2 18:11 kv_backend-rw-r--r-- 1 ceph ceph 21 Apr 2 18:11 magic-rw-r--r-- 1 ceph ceph 4 Apr 2 18:11 mkfs_done-rw-r--r-- 1 ceph ceph 41 Apr 2 18:11 osd_key-rw-r--r-- 1 ceph ceph 6 Apr 2 18:11 ready-rw-r--r-- 1 ceph ceph 10 Apr 2 18:11 type-rw-r--r-- 1 ceph ceph 2 Apr 2 18:11 whoami 查看OSD block对应设备： 123456789101112# lsblk /dev/sdbNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsdb 8:16 0 3.7T 0 disk└─ceph--5b2192ff--299c--4024--9f10--93af008e66d3-osd--block--fe7cd3b1--8513--4be5--b9a8--88fd47dca679 253:3 0 3.7T 0 lvm# pvs /dev/sdb PV VG Fmt Attr PSize PFree /dev/sdb ceph-5b2192ff-299c-4024-9f10-93af008e66d3 lvm2 a-- 3.64t 0# lvs ceph-5b2192ff-299c-4024-9f10-93af008e66d3 LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert osd-block-fe7cd3b1-8513-4be5-b9a8-88fd47dca679 ceph-5b2192ff-299c-4024-9f10-93af008e66d3 -wi-ao---- 3.64t 结论： Ceph OSD的mount路径对应的是tmpfs，Linux基于内存的文件系统，而并没有单独的块设备与之对应 整块磁盘创建一个PV，然后创建VG和一个LV给OSD的block使用 没有单独的块设备与tmpfs对应，那上面的数据存在哪里了？ 答：存在OSD的metadata里了！ 1234567891011121314151617181920212223242526272829303132333435363738394041424344文件：osd/OSD.ccint OSD::write_meta(CephContext *cct, ObjectStore *store, uuid_d&amp; cluster_fsid, uuid_d&amp; osd_fsid, int whoami)&#123; ... snprintf(val, sizeof(val), "%s", CEPH_OSD_ONDISK_MAGIC); r = store-&gt;write_meta("magic", val); if (r &lt; 0) return r; snprintf(val, sizeof(val), "%d", whoami); r = store-&gt;write_meta("whoami", val); if (r &lt; 0) return r; cluster_fsid.print(val); r = store-&gt;write_meta("ceph_fsid", val); if (r &lt; 0) return r; string key = cct-&gt;_conf-&gt;get_val&lt;string&gt;("key"); if (key.size()) &#123; r = store-&gt;write_meta("osd_key", key); if (r &lt; 0) return r; &#125; else &#123; ... r = store-&gt;write_meta("ready", "ready"); ... &#125; 文件：os/bluestore/BlueStore.ccint BlueStore::write_meta(const std::string&amp; key, const std::string&amp; value)&#123; bluestore_bdev_label_t label; string p = path + "/block"; int r = _read_bdev_label(cct, p, &amp;label); if (r &lt; 0) &#123; return ObjectStore::write_meta(key, value); &#125; label.meta[key] = value; r = _write_bdev_label(cct, p, label); assert(r == 0); return ObjectStore::write_meta(key, value);&#125; 磁盘分区命令格式： 1# ceph-deploy osd create [host] --data [/path/to/device-partition] 首先创建磁盘分区： 1234567891011121314151617# fdisk -l /dev/sdc...Disk /dev/sdc: 4000.8 GB, 4000787030016 bytes, 7814037168 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: gptDisk identifier: 84B34A2B-5F0D-4C4F-ADCD-1974B8FD5851# Start End Size Type Name 1 2048 7814037134 3.7T Linux filesyste# lsblk /dev/sdcNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsdc 8:32 0 3.7T 0 disk└─sdc1 8:33 0 3.7T 0 part 请使用gpt分区格式，其他的会报错 创建OSD： 1234567891011# ceph-deploy osd create ceph0 --data /dev/sdc1...[ceph0][INFO ] Running command: /usr/sbin/ceph-volume --cluster ceph lvm create --bluestore --data /dev/sdc1...[ceph0][DEBUG ] --&gt; ceph-volume lvm activate successful for osd ID: 1[ceph0][DEBUG ] --&gt; ceph-volume lvm activate successful for osd ID: None[ceph0][DEBUG ] --&gt; ceph-volume lvm create successful for: /dev/sdc1[ceph0][INFO ] checking OSD status...[ceph0][DEBUG ] find the location of an executable[ceph0][INFO ] Running command: /bin/ceph --cluster=ceph osd stat --format=json[ceph_deploy.osd][DEBUG ] Host ceph0 is now ready for osd use. 检查OSD： 1234567891011121314151617181920212223# ll /var/lib/ceph/osd/ceph-1/total 48-rw-r--r-- 1 ceph ceph 393 Apr 2 18:48 activate.monmaplrwxrwxrwx 1 ceph ceph 93 Apr 2 18:48 block -&gt; /dev/ceph-589101eb-51c3-42b9-adad-915bfccfc4f2/osd-block-8553492d-cb56-4b69-ab9f-d0cfcf0d0970-rw-r--r-- 1 ceph ceph 2 Apr 2 18:48 bluefs-rw-r--r-- 1 ceph ceph 37 Apr 2 18:48 ceph_fsid-rw-r--r-- 1 ceph ceph 37 Apr 2 18:48 fsid-rw------- 1 ceph ceph 55 Apr 2 18:48 keyring-rw-r--r-- 1 ceph ceph 8 Apr 2 18:48 kv_backend-rw-r--r-- 1 ceph ceph 21 Apr 2 18:48 magic-rw-r--r-- 1 ceph ceph 4 Apr 2 18:48 mkfs_done-rw-r--r-- 1 ceph ceph 41 Apr 2 18:48 osd_key-rw-r--r-- 1 ceph ceph 6 Apr 2 18:48 ready-rw-r--r-- 1 ceph ceph 10 Apr 2 18:48 type-rw-r--r-- 1 ceph ceph 2 Apr 2 18:48 whoam# pvs /dev/sdc1 PV VG Fmt Attr PSize PFree /dev/sdc1 ceph-589101eb-51c3-42b9-adad-915bfccfc4f2 lvm2 a-- 3.64t 0# lvs ceph-589101eb-51c3-42b9-adad-915bfccfc4f2 LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert osd-block-8553492d-cb56-4b69-ab9f-d0cfcf0d0970 ceph-589101eb-51c3-42b9-adad-915bfccfc4f2 -wi-ao---- 3.64t 结论： 与使用整块磁盘基本一样，不同的只是用磁盘分区创建一个PV 逻辑卷命令格式： 1# ceph-deploy osd create [host] --data [vg/lv] 首先创建一个逻辑卷： 12345678# pvcreate /dev/sdd Physical volume "/dev/sdd" successfully created.# vgcreate sddvg /dev/sdd Volume group "sddvg" successfully created# lvcreate -n sddlv -l 100%FREE sddvg Logical volume "sddlv" created. 创建OSD：1234567891011# ceph-deploy osd create ceph0 --data sddvg/sddlv...[ceph0][INFO ] Running command: /usr/sbin/ceph-volume --cluster ceph lvm create --bluestore --data sddvg/sddlv...[ceph0][DEBUG ] --&gt; ceph-volume lvm activate successful for osd ID: 2[ceph0][DEBUG ] --&gt; ceph-volume lvm activate successful for osd ID: None[ceph0][DEBUG ] --&gt; ceph-volume lvm create successful for: sddvg/sddlv[ceph0][INFO ] checking OSD status...[ceph0][DEBUG ] find the location of an executable[ceph0][INFO ] Running command: /bin/ceph --cluster=ceph osd stat --format=json[ceph_deploy.osd][DEBUG ] Host ceph0 is now ready for osd use. 若指定--data /dev/sddvg/sddlv，命令会报错，会被认为是block device 检查OSD： 123456789101112131415# ll /var/lib/ceph/osd/ceph-2/total 48-rw-r--r-- 1 ceph ceph 393 Apr 2 18:55 activate.monmaplrwxrwxrwx 1 ceph ceph 16 Apr 2 18:55 block -&gt; /dev/sddvg/sddlv-rw-r--r-- 1 ceph ceph 2 Apr 2 18:55 bluefs-rw-r--r-- 1 ceph ceph 37 Apr 2 18:55 ceph_fsid-rw-r--r-- 1 ceph ceph 37 Apr 2 18:55 fsid-rw------- 1 ceph ceph 55 Apr 2 18:55 keyring-rw-r--r-- 1 ceph ceph 8 Apr 2 18:55 kv_backend-rw-r--r-- 1 ceph ceph 21 Apr 2 18:55 magic-rw-r--r-- 1 ceph ceph 4 Apr 2 18:55 mkfs_done-rw-r--r-- 1 ceph ceph 41 Apr 2 18:55 osd_key-rw-r--r-- 1 ceph ceph 6 Apr 2 18:55 ready-rw-r--r-- 1 ceph ceph 10 Apr 2 18:55 type-rw-r--r-- 1 ceph ceph 2 Apr 2 18:55 whoami 结论： 与前两个一致，区别只是自己收到创建了PV，VG，LV 指定block.wal和block.db设备创建OSD当指定block.wal或block.db时，对应的设备可以为两种： 物理磁盘，但必须是一个磁盘分区 逻辑卷（a logical volume of LVM） 这里只区分指定的block.wal或block.db设备，data设备选择整块磁盘。 bluestore的block.db和block.wal大小 默认值如下，db size = 0，wal size = 100663296，都比较小。 1234567891011# ceph-conf --show-config | grep bluestore_blockbluestore_block_create = truebluestore_block_db_create = falsebluestore_block_db_path =bluestore_block_db_size = 0bluestore_block_path =bluestore_block_preallocate_file = falsebluestore_block_size = 10737418240bluestore_block_wal_create = falsebluestore_block_wal_path =bluestore_block_wal_size = 100663296 参考：https://marc.info/?l=ceph-devel&amp;m=149978799900866&amp;w=2 对block.db和block.wal的大小要求也都比较小，后续测试我们选择block.db和block.wal为10G。 磁盘分区命令格式： 1# ceph-deploy osd create [host] --data [/path/to/device] --block-db [/path/to/device-partition] --block-wal [/path/to/device-partition] 首先创建两个磁盘分区给block.wal和block.db使用： 12345678910# parted -s /dev/sdf printModel: ATA INTEL SSDSC2BB48 (scsi)Disk /dev/sdf: 480GBSector size (logical/physical): 512B/4096BPartition Table: gptDisk Flags:Number Start End Size File system Name Flags 1 1049kB 10.7GB 10.7GB 2 10.7GB 21.5GB 10.7GB 创建OSD： 123456789101112# ceph-deploy disk zap ceph0 /dev/sde# ceph-deploy osd create ceph0 --data /dev/sde --block-db /dev/sdf1 --block-wal /dev/sdf2...[ceph0][INFO ] Running command: /usr/sbin/ceph-volume --cluster ceph lvm create --bluestore --data /dev/sde --block.wal /dev/sdf2 --block.db /dev/sdf1...[ceph0][DEBUG ] --&gt; ceph-volume lvm activate successful for osd ID: 3[ceph0][DEBUG ] --&gt; ceph-volume lvm activate successful for osd ID: None[ceph0][DEBUG ] --&gt; ceph-volume lvm create successful for: /dev/sde[ceph0][INFO ] checking OSD status...[ceph0][DEBUG ] find the location of an executable[ceph0][INFO ] Running command: /bin/ceph --cluster=ceph osd stat --format=json[ceph_deploy.osd][DEBUG ] Host ceph0 is now ready for osd use. 检查OSD： 123456789101112131415161718192021222324# df -hFilesystem Size Used Avail Use% Mounted on...tmpfs 16G 56K 16G 1% /var/lib/ceph/osd/ceph-3# ll /var/lib/ceph/osd/ceph-3total 56-rw-r--r-- 1 ceph ceph 393 Apr 3 09:20 activate.monmaplrwxrwxrwx 1 ceph ceph 93 Apr 3 09:20 block -&gt; /dev/ceph-7777d5e4-9b81-4c17-916d-7a1e48f6268e/osd-block-3ed97688-03b4-4ca6-a497-bbd30e865852lrwxrwxrwx 1 root root 9 Apr 3 09:20 block.db -&gt; /dev/sdf1lrwxrwxrwx 1 root root 9 Apr 3 09:20 block.wal -&gt; /dev/sdf2-rw-r--r-- 1 ceph ceph 2 Apr 3 09:20 bluefs-rw-r--r-- 1 ceph ceph 37 Apr 3 09:20 ceph_fsid-rw-r--r-- 1 ceph ceph 37 Apr 3 09:20 fsid-rw------- 1 ceph ceph 55 Apr 3 09:20 keyring-rw-r--r-- 1 ceph ceph 8 Apr 3 09:20 kv_backend-rw-r--r-- 1 ceph ceph 21 Apr 3 09:20 magic-rw-r--r-- 1 ceph ceph 4 Apr 3 09:20 mkfs_done-rw-r--r-- 1 ceph ceph 41 Apr 3 09:20 osd_key-rw-r--r-- 1 ceph ceph 10 Apr 3 09:20 path_block.db-rw-r--r-- 1 ceph ceph 10 Apr 3 09:20 path_block.wal-rw-r--r-- 1 ceph ceph 6 Apr 3 09:20 ready-rw-r--r-- 1 ceph ceph 10 Apr 3 09:20 type-rw-r--r-- 1 ceph ceph 2 Apr 3 09:20 whoami 结论： 与使用单块盘基本一样，不同的只是指定了block.db -&gt; /dev/sdf1和block.wal -&gt; /dev/sdf2 逻辑卷命令格式： 1# ceph-deploy osd create [host] --data [/path/to/device] --block-db [vg/lv] --block-wal [vg/lv] 首先创建两个逻辑卷给block.wal和block.db使用： 1234# pvcreate /dev/sdm# vgcreate ssdvg /dev/sdm# lvcreate -n db-lv-0 -L 4G ssdvg# lvcreate -n wal-lv-0 -L 8G ssdvg 创建OSD： 123456789101112# ceph-deploy disk zap ceph0 /dev/sdg# ceph-deploy osd create ceph0 --data /dev/sdg --block-db ssdvg/db-lv-0 --block-wal ssdvg/wal-lv-0...[ceph0][INFO ] Running command: /usr/sbin/ceph-volume --cluster ceph lvm create --bluestore --data /dev/sdg --block.wal ssdvg/wal-lv-0 --block.db ssdvg/db-lv-0...[ceph0][DEBUG ] --&gt; ceph-volume lvm activate successful for osd ID: 4[ceph0][DEBUG ] --&gt; ceph-volume lvm activate successful for osd ID: None[ceph0][DEBUG ] --&gt; ceph-volume lvm create successful for: /dev/sdg[ceph0][INFO ] checking OSD status...[ceph0][DEBUG ] find the location of an executable[ceph0][INFO ] Running command: /bin/ceph --cluster=ceph osd stat --format=json[ceph_deploy.osd][DEBUG ] Host ceph0 is now ready for osd use. 若指定--block-db /dev/ssdvg/db-lv-0 --block-wal /dev/ssdvg/wal-lv-0，命令会报错 检查OSD： 123456789101112131415161718192021222324# df -hFilesystem Size Used Avail Use% Mounted on...tmpfs 16G 56K 16G 1% /var/lib/ceph/osd/ceph-4# ll /var/lib/ceph/osd/ceph-4total 56-rw-r--r-- 1 ceph ceph 393 Apr 3 09:30 activate.monmaplrwxrwxrwx 1 ceph ceph 93 Apr 3 09:30 block -&gt; /dev/ceph-017b646a-0332-4677-967b-95033a3a33ab/osd-block-24228535-1fb3-4bcd-bb87-f6d5f49ed24dlrwxrwxrwx 1 root root 18 Apr 3 09:30 block.db -&gt; /dev/ssdvg/db-lv-0lrwxrwxrwx 1 root root 19 Apr 3 09:30 block.wal -&gt; /dev/ssdvg/wal-lv-0-rw-r--r-- 1 ceph ceph 2 Apr 3 09:30 bluefs-rw-r--r-- 1 ceph ceph 37 Apr 3 09:30 ceph_fsid-rw-r--r-- 1 ceph ceph 37 Apr 3 09:30 fsid-rw------- 1 ceph ceph 55 Apr 3 09:30 keyring-rw-r--r-- 1 ceph ceph 8 Apr 3 09:30 kv_backend-rw-r--r-- 1 ceph ceph 21 Apr 3 09:30 magic-rw-r--r-- 1 ceph ceph 4 Apr 3 09:30 mkfs_done-rw-r--r-- 1 ceph ceph 41 Apr 3 09:30 osd_key-rw-r--r-- 1 ceph ceph 19 Apr 3 09:30 path_block.db-rw-r--r-- 1 ceph ceph 20 Apr 3 09:30 path_block.wal-rw-r--r-- 1 ceph ceph 6 Apr 3 09:30 ready-rw-r--r-- 1 ceph ceph 10 Apr 3 09:30 type-rw-r--r-- 1 ceph ceph 2 Apr 3 09:30 whoami 结论： 与使用单块盘基本一样，不同的只是指定了block.db -&gt; /dev/ssdvg/db-lv-0和block.wal -&gt; /dev/ssdvg/wal-lv-0]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>ceph-deploy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSD加速SATA盘之bcache策略]]></title>
    <url>%2F2018%2F03%2F26%2Flvm-sata-ssd-bcache%2F</url>
    <content type="text"><![CDATA[概述在前面的文章中介绍了 SSD加速SATA盘之flashcache策略。一般我们也推荐选择稳定的flashcache策略来做SSD加速SATA盘，但在实践中，发现其在CentOS上编译安装还是很麻烦的，这里就抓紧研究实践了下bcache策略。 另外bcache使用可以用一块SSD来缓存多块SATA盘，对于使用中随时变动磁盘的应用场景来说，操作非常便捷。 并且官网说bcache的性能完全优于flashcache，参考： http://www.accelcloud.com/2012/04/18/linux-flashcache-and-bcache-performance-testing/ Bcache介绍： https://wiki.archlinux.org/index.php/Bcache https://bcache.evilpiepirate.org/ Bcache在Linux kernel 3.10版本加入了mainline，使用它只需要主机的kernel版本大于3.10即可。 bcache-tools 源码：https://evilpiepirate.org/git/bcache-tools.git Ubuntu上安装系统信息12345678# lsb_release -aNo LSB modules are available.Distributor ID: UbuntuDescription: Ubuntu 16.04.2 LTSRelease: 16.04Codename: xenial# uname -r4.4.0-72-generic 编译安装加载系统的bcache module： 123# lsmod | grep bcache# modprobe bcache# lsmod | grep bcache 编译安装bcace-tools： 1234567891011121314# apt-get install -y pkg-config libblkid-dev# git clone https://evilpiepirate.org/git/bcache-tools.git# cd bcache-tools# makecc -O2 -Wall -g `pkg-config --cflags uuid blkid` -c -o bcache.o bcache.cbcache.c:125:9: warning: ‘crc_table’ is static but used in inline function ‘crc64’ which is not static crc = crc_table[i] ^ (crc &lt;&lt; 8); ^cc -O2 -Wall -g `pkg-config --cflags uuid blkid` make-bcache.c bcache.o `pkg-config --libs uuid blkid` -o make-bcache/tmp/ccW6rXtD.o: In function `write_sb':/root/yangguanjun/bcache-tools/make-bcache.c:277: undefined reference to `crc64'collect2: error: ld returned 1 exit status&lt;builtin&gt;: recipe for target 'make-bcache' failedmake: *** [make-bcache] Error 1 网上搜索有这个bug的fix，如下： https://www.spinics.net/lists/linux-bcache/msg02847.html 123456789101112--- a/bcache.c+++ b/bcache.c@@ -115,7 +115,7 @@ static const uint64_t crc_table[256] = &#123; 0x9AFCE626CE85B507ULL&#125;;-inline uint64_t crc64(const void *_data, size_t len)+uint64_t crc64(const void *_data, size_t len)&#123; uint64_t crc = 0xFFFFFFFFFFFFFFFFULL; const unsigned char *data = _data; 按上面patch修改bcache.c后，编译安装正常。 123456789101112131415# makecc -O2 -Wall -g `pkg-config --cflags uuid blkid` -c -o bcache.o bcache.ccc -O2 -Wall -g `pkg-config --cflags uuid blkid` make-bcache.c bcache.o `pkg-config --libs uuid blkid` -o make-bcachecc -O2 -Wall -g `pkg-config --cflags uuid blkid` probe-bcache.c `pkg-config --libs uuid blkid` -o probe-bcachecc -O2 -Wall -g -std=gnu99 bcache-super-show.c bcache.o `pkg-config --libs uuid` -o bcache-super-showcc -O2 -Wall -g -c -o bcache-register.o bcache-register.ccc bcache-register.o -o bcache-register# make installinstall -m0755 make-bcache bcache-super-show /usr/sbin/install -m0755 probe-bcache bcache-register /lib/udev/install -m0644 69-bcache.rules /lib/udev/rules.d/install -m0644 -- *.8 /usr/share/man/man8/install -D -m0755 initramfs/hook /usr/share/initramfs-tools/hooks/bcacheinstall -D -m0755 initcpio/install /usr/lib/initcpio/install/bcacheinstall -D -m0755 dracut/module-setup.sh /lib/dracut/modules.d/90bcache/module-setup.sh CentOS上安装因为bcache在kernel 3.10版本才进入主线，所以我们要保证CentOS的内核版本大于3.10，所以CenOS 6就不要尝试了，直接用新的CentOS 7吧。 系统信息12345678# lsb_release -aLSB Version: :core-4.1-amd64:core-4.1-noarchDistributor ID: CentOSDescription: CentOS Linux release 7.3.1611 (Core)Release: 7.3.1611Codename: Core# uname -r3.10.0-693.17.1.el7.x86_64 编译安装加载系统的bcache模块： 123# lsmod | grep bcache# modprobe bcachemodprobe: FATAL: Module bcache not found. 查看发现在kernel 3.10.0-693 版本里，默认是不编译bcache模块的，参考： https://lakelight.net/2017/12/20/bcache-centos-7.html 这里就需要下载当前内核版本的源码，重新编译内核bcache模块，然后再加载bcache模块。 鉴于之前CentOS上安装flashcache时探索了内核版本的升级，所以这里在已经升级内核版本的机器上尝试，发现kernel 4.4版本默认已经编译好了bcache模块，所以在CentOS上使用cache时，还是建议升级到4.4版本内核。 1234567# uname -r4.4.115-1.el7.elrepo.x86_64# lsmod | grep bcache# modprobe bcache# lsmod | grep bcachebcache 233472 0 编译安装bcace-tools： 123456789101112131415161718# yum install -y git pkgconfig libblkid-devel# git clone https://evilpiepirate.org/git/bcache-tools.git# cd bcache-tools/# makecc -O2 -Wall -g `pkg-config --cflags uuid blkid` -c -o bcache.o bcache.ccc -O2 -Wall -g `pkg-config --cflags uuid blkid` make-bcache.c bcache.o `pkg-config --libs uuid blkid` -o make-bcachecc -O2 -Wall -g `pkg-config --cflags uuid blkid` probe-bcache.c `pkg-config --libs uuid blkid` -o probe-bcachecc -O2 -Wall -g -std=gnu99 bcache-super-show.c bcache.o `pkg-config --libs uuid` -o bcache-super-showcc -O2 -Wall -g -c -o bcache-register.o bcache-register.ccc bcache-register.o -o bcache-register# make installinstall -m0755 make-bcache bcache-super-show /usr/sbin/install -m0755 probe-bcache bcache-register /lib/udev/install -m0644 69-bcache.rules /lib/udev/rules.d/install -m0644 -- *.8 /usr/share/man/man8/install -D -m0755 initramfs/hook /usr/share/initramfs-tools/hooks/bcacheinstall -D -m0755 initcpio/install /usr/lib/initcpio/install/bcacheinstall -D -m0755 dracut/module-setup.sh /lib/dracut/modules.d/90bcache/module-setup.sh Bcache使用下面在CentOS机器上，介绍如何使用bcache。 硬盘信息12345# fdisk -l | grep dev...Disk /dev/vdb: 107.4 GB, 107374182400 bytes, 209715200 sectorsDisk /dev/vdc: 107.4 GB, 107374182400 bytes, 209715200 sectorsDisk /dev/vdd: 53.7 GB, 53687091200 bytes, 104857600 sectors 这里使用三块盘：vdb、vdc、vdd。 其中vdb、vdc是容量型磁盘，vdd是性能型磁盘，实验用vdd通过bcache加速vdb和vdc。 使用步骤与bcache相关的命令有：make-bcache和bcache-super-show 12345678910111213141516# make-bcachePlease supply a deviceUsage: make-bcache [options] device -C, --cache Format a cache device -B, --bdev Format a backing device -b, --bucket bucket size -w, --block block size (hard sector size of SSD, often 2k) -o, --data-offset data offset in sectors --cset-uuid UUID for the cache set --writeback enable writeback --discard enable discards --cache_replacement_policy=(lru|fifo) -h, --help display this help and exit # bcache-super-showUsage: bcache-super-show [-f] &lt;device&gt; 创建backing device12345678910111213141516171819202122# make-bcache -B /dev/vdbUUID: c602abab-bf5a-4b51-b7f6-1492d34239f4Set UUID: 423e1910-f61a-45fa-8cdf-a23aca3b5eb8version: 1block_size: 1data_offset: 16# bcache-super-show /dev/vdbsb.magic oksb.first_sector 8 [match]sb.csum 1376BA45B5F924B [match]sb.version 1 [backing device]dev.label (empty)dev.uuid c602abab-bf5a-4b51-b7f6-1492d34239f4dev.sectors_per_block 1dev.sectors_per_bucket 1024dev.data.first_sector 16dev.data.cache_mode 0 [writethrough]dev.data.cache_state 1 [clean]cset.uuid 4b60c663-7720-4dea-a17a-e9316078e796 创建cache device123456789101112131415161718192021222324252627282930# make-bcache -C /dev/vddUUID: 050998ce-403c-45d7-a89b-492379644c1bSet UUID: 4b60c663-7720-4dea-a17a-e9316078e796version: 0nbuckets: 102400block_size: 1bucket_size: 1024nr_in_set: 1nr_this_dev: 0first_bucket: 1# bcache-super-show /dev/vddsb.magic oksb.first_sector 8 [match]sb.csum 68CDDDC337A2E296 [match]sb.version 3 [cache device]dev.label (empty)dev.uuid 050998ce-403c-45d7-a89b-492379644c1bdev.sectors_per_block 1dev.sectors_per_bucket 1024dev.cache.first_sector 1024dev.cache.cache_sectors 104856576dev.cache.total_sectors 104857600dev.cache.ordered yesdev.cache.discard nodev.cache.pos 0dev.cache.replacement 0 [lru]cset.uuid 4b60c663-7720-4dea-a17a-e9316078e796 绑定backing device到cache device123456789# echo "4b60c663-7720-4dea-a17a-e9316078e796" &gt; /sys/block/bcache0/bcache/attach# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT...vdb 253:16 0 100G 0 disk└─bcache0 251:0 0 100G 0 diskvdc 253:32 0 100G 0 diskvdd 253:48 0 50G 0 disk└─bcache0 251:0 0 100G 0 disk 查看bcache相关信息1、state 12# cat /sys/block/bcache0/bcache/stateclean state的几个状态： no cache：该backing device没有attach任何caching device clean：一切正常，缓存是干净的 dirty：一切正常，已启用回写，缓存是脏的 inconsistent：遇到问题，后台设备与缓存设备不同步 2、缓存数据量 12# cat /sys/block/bcache0/bcache/dirty_data0.0k 3、缓存模式 12345# cat /sys/block/bcache0/bcache/cache_mode[writethrough] writeback writearound none# echo writearound &gt; /sys/block/bcache0/bcache/cache_mode# cat /sys/block/bcache0/bcache/cache_modewritethrough writeback [writearound] none 4、writeback信息 123# cat /sys/block/bcache0/bcache/writeback_writeback_delay writeback_percent writeback_rate_debug writeback_rate_p_term_inverse writeback_runningwriteback_metadata writeback_rate writeback_rate_d_term writeback_rate_update_seconds 解绑backing device的cache device1234567891011# echo "697b764f-b3ef-4675-8761-d9518a12089c" &gt; /sys/block/bcache0/bcache/detach# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT...vdb 253:16 0 100G 0 disk└─bcache0 251:0 0 100G 0 diskvdc 253:32 0 100G 0 diskvdd 253:48 0 50G 0 disk# cat /sys/block/vdb/bcache/stateno cache 解绑后设备可以继续使用，只是没有cache device的加速 添加新backing device1234567891011121314151617# make-bcache -B /dev/vdcUUID: cc790e62-b3eb-4237-8265-dd1b619e15c0Set UUID: edb2b1d0-9eeb-4a8a-b811-3dafd676fac0version: 1block_size: 1data_offset: 16# echo "4b60c663-7720-4dea-a17a-e9316078e796" &gt; /sys/block/bcache1/bcache/attach# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT...vdb 253:16 0 100G 0 disk└─bcache0 251:0 0 100G 0 diskvdc 253:32 0 100G 0 disk└─bcache1 251:1 0 100G 0 diskvdd 253:48 0 50G 0 disk└─bcache1 251:1 0 100G 0 disk 使用bcache device12345678# mkfs.ext4 /dev/bcache1# mount /dev/bcache1 /mnt/# df -hFilesystem Size Used Avail Use% Mounted on.../dev/bcache1 99G 61M 94G 1% /mnt# umount /mnt/ 注销bcache device123456789# echo 1 &gt; /sys/block/vdc/bcache/stop# echo 1 &gt; /sys/block/vdb/bcache/stop# echo 1 &gt; /sys/fs/bcache/10057a1c-15a2-4631-a6d2-f4652d37645d/unregister# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT...vdb 253:16 0 100G 0 diskvdc 253:32 0 100G 0 diskvdd 253:48 0 50G 0 disk echo的数字不重要，可为任何值 ;) 快捷创建bcache device123456789101112131415161718192021222324252627282930# make-bcache -B /dev/vdb /dev/vdc -C /dev/vddUUID: 09f971eb-6063-4f94-bdac-d7d7117c0e0fSet UUID: 697b764f-b3ef-4675-8761-d9518a12089cversion: 0nbuckets: 102400block_size: 1bucket_size: 1024nr_in_set: 1nr_this_dev: 0first_bucket: 1UUID: b45301fa-8932-4194-9518-ab681f37d9c9Set UUID: 697b764f-b3ef-4675-8761-d9518a12089cversion: 1block_size: 1data_offset: 16UUID: 2c0452f7-76de-4319-bd3c-2a73b4fa4b68Set UUID: 697b764f-b3ef-4675-8761-d9518a12089cversion: 1block_size: 1data_offset: 16[root@lvm-centos-tst ~]# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT...vdb 253:16 0 100G 0 disk└─bcache0 251:0 0 100G 0 diskvdc 253:32 0 100G 0 disk└─bcache1 251:1 0 100G 0 diskvdd 253:48 0 50G 0 disk├─bcache0 251:0 0 100G 0 disk└─bcache1 251:1 0 100G 0 disk 遇到的问题make-bcache命令有提示之前做过bcache的device，重做bcache有提示 12345678# make-bcache -B /dev/vdbAlready a bcache device on /dev/vdb, overwrite with --wipe-bcache# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT...vdb 253:16 0 100G 0 disk└─bcache0 251:0 0 100G 0 disk 虽说有提示，但实际bcache device已经创建成果 针对上述情况，可以通过写device前一部分数据的方法解决 12345678910111213141516171819202122可以通过dd命令来清理device的前部分数据：# dd if=/dev/zero of=/dev/vdb bs=1M count=100 oflag=direct# dd if=/dev/zero of=/dev/vdd bs=1M count=100 oflag=direct再创建bcache device，就不会报错了：# make-bcache -B /dev/vdbUUID: c602abab-bf5a-4b51-b7f6-1492d34239f4Set UUID: 423e1910-f61a-45fa-8cdf-a23aca3b5eb8version: 1block_size: 1data_offset: 16# make-bcache -C /dev/vddUUID: 050998ce-403c-45d7-a89b-492379644c1bSet UUID: 4b60c663-7720-4dea-a17a-e9316078e796version: 0nbuckets: 102400block_size: 1bucket_size: 1024nr_in_set: 1nr_this_dev: 0first_bucket: 1 设备没umount就直接注销没有umount，注销bcache device后，设备依旧可以使用，umount后设备消失 123456789101112131415161718192021222324252627282930313233343536# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT...vdb 253:16 0 100G 0 disk└─bcache0 251:0 0 100G 0 disk /mntvdc 253:32 0 100G 0 diskvdd 253:48 0 50G 0 disk└─bcache0 251:0 0 100G 0 disk /mnt# mount | grep bcache/dev/bcache0 on /mnt type ext4 (rw,relatime,seclabel,data=ordered)# echo 1 &gt; /sys/fs/bcache/4b60c663-7720-4dea-a17a-e9316078e796/unregister# echo 0 &gt; /sys/block/vdb/bcache/stop# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT...vdb 253:16 0 100G 0 disk└─bcache0 251:0 0 100G 0 disk /mntvdc 253:32 0 100G 0 diskvdd 253:48 0 50G 0 disk# ls /sys/block/vdb/bcachels: cannot access /sys/block/vdb/bcache: No such file or directory# cd /mnt/# touch tstfile# umount /mnt/# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT...vdb 253:16 0 100G 0 diskvdc 253:32 0 100G 0 diskvdd 253:48 0 50G 0 disk]]></content>
      <categories>
        <category>storage</category>
      </categories>
      <tags>
        <tag>bcache</tag>
        <tag>ssd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSD加速SATA盘之flashcache策略]]></title>
    <url>%2F2018%2F03%2F08%2Fssd-enhance-sata-with-flashcache%2F</url>
    <content type="text"><![CDATA[概述通常SATA盘的性能比较低，对于大多数应用来说性能不够，但纯SSD的盘又比较昂贵，结合这两种盘的使用策略是业内讨论的一个热点，也有很多成熟的方案来使用。 之前有文章讨论过cache策略，我们这里选择通用的flashcache方案来用SSD盘加速SATA盘。 flashcache源码：https://github.com/facebookarchive/flashcache参考：http://www.yangguanjun.com/2018/01/30/lvm-with-cache/ Ubuntu上安装测试使用的是”Ubuntu 16.04.2 LTS”，它的内核比较新，所以这里编译安装flashcache很方便。 系统信息12345678# lsb_release -aNo LSB modules are available.Distributor ID: UbuntuDescription: Ubuntu 16.04.2 LTSRelease: 16.04Codename: xenial# uname -r4.4.0-62-generic 编译安装12345678910111213# apt install -y git make gcc# git clone git@github.com:facebookarchive/flashcache.git# cd flashcache/# make# make install# modprobe flashcache# cat /proc/flashcache/flashcache_versionFlashcache Version : flashcache-3.1.1git commit: 1.0-248-g437afbfe233e# flashcache_flashcache_create flashcache_destroy flashcache_load flashcache_setioctl CentOS上安装测试使用的是”CentOS Linux release 7.3.1611”，它的内核比较旧，在安装flashcache时遇到很多问题。 系统信息123456789# lsb_release -a...Distributor ID: CentOSDescription: CentOS Linux release 7.3.1611 (Core)Release: 7.3.1611Codename: Core# uname -aLinux xs732 3.10.0-514.el7.x86_64 #1 SMP Tue Nov 22 16:42:41 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux 编译安装12345678# yum install -y git make gcc # git clone git@github.com:facebookarchive/flashcache.git# cd flashcache# make.../root/yangguanjun/flashcache/src/flashcache_conf.c:1230:11: error: too many arguments to function ‘wait_on_bit_lock’ flashcache_wait_schedule, TASK_UNINTERRUPTIBLE);... 网上可搜索到该问题：https://github.com/facebookarchive/flashcache/issues/191 升级内核尝试升级CentOS的内核版本，但是编译flashcache还是一样的错误。 123# yum update# uname -aLinux server0 3.10.0-693.17.1.el7.x86_64 #1 SMP Thu Jan 25 20:13:58 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux 无奈，选择升级CentOS内核到最新版本4.15。 参考：http://www.jiagoumi.com/work/1167.html 12345678910111213141516171819202122232425262728293031323334先导入elrepo的key，然后安装elrepo的yum源：# rpm -import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org# rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm列出可用的内核相关包：# yum --disablerepo="*" --enablerepo="elrepo-kernel" list available…kernel-lt.x86_64 4.4.115-1.el7.elrepo elrepo-kernelkernel-lt-devel.x86_64 4.4.115-1.el7.elrepo elrepo-kernel...kernel-ml.x86_64 4.15.1-1.el7.elrepo elrepo-kernelkernel-ml-devel.x86_64 4.15.1-1.el7.elrepo elrepo-kernel...安装最新的主线稳定内核：# yum -y --enablerepo=elrepo-kernel install kernel-ml.x86_64 kernel-ml-devel.x86_64查看系统kernel默认启动顺序：# awk -F\' '$1=="menuentry " &#123;print $2&#125;' /etc/grub2.cfgCentOS Linux (4.15.1-1.el7.elrepo.x86_64) 7 (Core)CentOS Linux (3.10.0-514.el7.x86_64) 7 (Core)CentOS Linux (0-rescue-836c8e52784b401db086b015b56e2fba) 7 (Core)修改系统kernel默认启动内核：# vim /etc/default/grub...GRUB_DEFAULT=saved 修改为 0...重新创建内核配置：# grub2-mkconfig -o /boot/grub2/grub.cfg重启机器：# reboot -nf 1234567891011121314# uname -r4.15.1-1.el7.elrepo.x86_64# makemake -C src KERNEL_TREE=/usr/src/kernels/4.15.1-1.el7.elrepo.x86_64 PWD=/root/flashcache/src all.../root/flashcache/src/flashcache_main.c: In function ‘dm_io_async_bvec_pl’:/root/flashcache/src/flashcache_main.c:119:6: error: ‘struct dm_io_request’ has no member named ‘bi_rw’ iorq.bi_rw = rw; ^/root/flashcache/src/flashcache_main.c: In function ‘dm_io_async_bvec’:/root/flashcache/src/flashcache_main.c:143:6: error: ‘struct dm_io_request’ has no member named ‘bi_rw’ iorq.bi_rw = rw;... 很无语，最新kernel的4.15版本竟然也编译不过flashcache。 查看之前编译安装flashcache成功的Ubuntu平台上的内核版本为：4.4.0-62-generic，最后决定尝试 4.4的kernel版本，步骤与上面的步骤类似，差别如下： 1234567重新升级系统内核，选择kernel-lt*# yum -y --enablerepo=elrepo-kernel install kernel-lt.x86_64 kernel-lt-devel.x86_64之后重启机器# uname -r4.4.115-1.el7.elrepo.x86_64 再尝试编译安装flashcache，一切正常O(∩_∩)O哈哈~ 12345# cd flashcache# make# make install# flashcache_flashcache_create flashcache_destroy flashcache_load flashcache_setioctl flashcache使用硬盘信息12345# fdisk -l | grep dev...Disk /dev/vdb: 107.4 GB, 107374182400 bytes, 209715200 sectorsDisk /dev/vdc: 107.4 GB, 107374182400 bytes, 209715200 sectorsDisk /dev/vdd: 53.7 GB, 53687091200 bytes, 104857600 sectors 这里使用三块盘：vdb、vdc、vdd。 其中vdb、vdc是容量型磁盘，vdd是性能型磁盘，实验用vdd通过bcache加速vdb和vdc。 使用步骤与flashcache相关的命令好几个，如下： 12345678910111213# flashcache_createUsage: flashcache_create [-v] [-p back|thru|around] [-w] [-b block size] [-m md block size] [-s cache size] [-a associativity] cachedev ssd_devname disk_devnameUsage : flashcache_create Cache Mode back|thru|around is required argumentUsage : flashcache_create Default units for -b, -m, -s are sectors, or specify in k/M/G. Default associativity is 512.# flashcache_destroyUsage: flashcache_destroy ssd_devname# flashcache_loadUsage: flashcache_load ssd_devname [cachedev]# flashcache_setioctlUsage: flashcache_setioctl (-c | -a | -r) (-b pid |-w pid) ssd_devname 另外flashcache是通过device mapper来做的设备映射和缓存，所以对flashcache device的操作命令也有dmsetup，使用如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# dmsetupUsage:dmsetup [--version] [-h|--help [-c|-C|--columns]] [-v|--verbose [-v|--verbose ...]] [-f|--force] [--checks] [--manglename &#123;none|hex|auto&#125;] [-r|--readonly] [--noopencount] [--noflush] [--nolockfs] [--inactive] [--udevcookie &lt;cookie&gt;] [--noudevrules] [--noudevsync] [--verifyudev] [-y|--yes] [--readahead &#123;[+]&lt;sectors&gt;|auto|none&#125;] [--retry] [-c|-C|--columns] [-o &lt;fields&gt;] [-O|--sort &lt;sort_fields&gt;] [-S|--select &lt;selection&gt;] [--nameprefixes] [--noheadings] [--separator &lt;separator&gt;] help [-c|-C|--columns] create &lt;dev_name&gt; [-j|--major &lt;major&gt; -m|--minor &lt;minor&gt;] [-U|--uid &lt;uid&gt;] [-G|--gid &lt;gid&gt;] [-M|--mode &lt;octal_mode&gt;] [-u|uuid &lt;uuid&gt;] [--addnodeonresume|--addnodeoncreate] [--readahead &#123;[+]&lt;sectors&gt;|auto|none&#125;] [-n|--notable|--table &#123;&lt;table&gt;|&lt;table_file&gt;&#125;] remove [--deferred] [-f|--force] [--retry] &lt;device&gt; remove_all [-f|--force] suspend [--noflush] [--nolockfs] &lt;device&gt; resume [--noflush] [--nolockfs] &lt;device&gt; [--addnodeonresume|--addnodeoncreate] [--readahead &#123;[+]&lt;sectors&gt;|auto|none&#125;] load &lt;device&gt; [&lt;table&gt;|&lt;table_file&gt;] clear &lt;device&gt; reload &lt;device&gt; [&lt;table&gt;|&lt;table_file&gt;] wipe_table [-f|--force] [--noflush] [--nolockfs] &lt;device&gt; rename &lt;device&gt; [--setuuid] &lt;new_name_or_uuid&gt; message &lt;device&gt; &lt;sector&gt; &lt;message&gt; ls [--target &lt;target_type&gt;] [--exec &lt;command&gt;] [-o &lt;options&gt;] [--tree] info [&lt;device&gt;] deps [-o &lt;options&gt;] [&lt;device&gt;] stats &lt;command&gt; [&lt;options&gt;] [&lt;devices&gt;] status [&lt;device&gt;] [--noflush] [--target &lt;target_type&gt;] table [&lt;device&gt;] [--target &lt;target_type&gt;] [--showkeys] wait &lt;device&gt; [&lt;event_nr&gt;] [--noflush] mknodes [&lt;device&gt;] mangle [&lt;device&gt;] udevcreatecookie udevreleasecookie [&lt;cookie&gt;] udevflags &lt;cookie&gt; udevcomplete &lt;cookie&gt; udevcomplete_all [&lt;age_in_minutes&gt;] udevcookies targets version setgeometry &lt;device&gt; &lt;cyl&gt; &lt;head&gt; &lt;sect&gt; &lt;start&gt; splitname &lt;device&gt; [&lt;subsystem&gt;]&lt;device&gt; may be device name or -u &lt;uuid&gt; or -j &lt;major&gt; -m &lt;minor&gt;&lt;mangling_mode&gt; is one of 'none', 'auto' and 'hex'.&lt;fields&gt; are comma-separated. Use 'help -c' for list.Table_file contents may be supplied on stdin.Options are: devno, devname, blkdevname.Tree specific options are: ascii, utf, vt100; compact, inverted, notrunc; blkdevname, [no]device, active, open, rw and uuid. 磁盘规划因为flashcache要求一个ssd_device对应一个sata_device，所以在使用之前，我们要规划好哪些SSD盘做缓存？哪些SATA盘需要做flashcache？以便给出合理规划。 这里我们有一个SSD盘，两个SATA盘，所以把SSD盘分为两个分区。 12345678910111213# fdisk /dev/vdd...# fdisk -l /dev/vddDisk /dev/vdd: 53.7 GB, 53687091200 bytes, 104857600 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0xa8f11a03 Device Boot Start End Blocks Id System/dev/vdd1 2048 52430847 26214400 83 Linux/dev/vdd2 52430848 104857599 26213376 83 Linux 缓存模式flashcache支持三种缓存模式： Writeback : 对于写入，首先会写入到Cache中，同时将对于block的元数据dirty bit，但是并不会立即写入后备的device Writethrough : 对于写入，写入到Cache中，同时也会将数据写入backing device，知道写完backing device，才算写完 Writearound : 写入的时候，绕过Cache，直接写入backing device，即SSD只当读缓存 三种缓存模式的区别如下图： 创建flashcache device123456789101112131415161718192021222324252627282930313233343536后端设备为一分区：# flashcache_create -p back fcache-dev1 /dev/vdd1 /dev/vdb1cachedev fcache-dev1, ssd_devname /dev/vdd1, disk_devname /dev/vdb1 cache mode WRITE_BACKblock_size 8, md_block_size 8, cache_size 0Flashcache metadata will use 137MB of your 3951MB main memory# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT...vdb 253:16 0 100G 0 disk└─vdb1 253:17 0 100G 0 part └─fcache-dev1 252:0 0 100G 0 dmvdd 253:48 0 50G 0 disk├─vdd1 253:49 0 25G 0 part│ └─fcache-dev1 252:0 0 100G 0 dm└─vdd2 253:50 0 25G 0 part后端设备为整块磁盘：# flashcache_create -p back fcache-dev2 /dev/vdd2 /dev/vdccachedev fcache-dev2, ssd_devname /dev/vdd2, disk_devname /dev/vdc cache mode WRITE_BACKblock_size 8, md_block_size 8, cache_size 0Flashcache metadata will use 137MB of your 3951MB main memory# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT...vdb 253:16 0 100G 0 disk└─vdb1 253:17 0 100G 0 part └─fcache-dev1 252:0 0 100G 0 dmvdc 253:32 0 100G 0 disk└─fcache-dev2 252:1 0 100G 0 dmvdd 253:48 0 50G 0 disk├─vdd1 253:49 0 25G 0 part│ └─fcache-dev1 252:0 0 100G 0 dm└─vdd2 253:50 0 25G 0 part └─fcache-dev2 252:1 0 100G 0 dm 查看flashcache device1234567891011121314151617181920212223# ll /dev/mapper/fcache-dev*lrwxrwxrwx. 1 root root 7 Feb 11 11:14 /dev/mapper/fcache-dev1 -&gt; ../dm-0lrwxrwxrwx. 1 root root 7 Feb 11 11:15 /dev/mapper/fcache-dev2 -&gt; ../dm-1# dmsetup tablefcache-dev2: 0 209715200 flashcache conf: ssd dev (/dev/vdd2), disk dev (/dev/vdc) cache mode(WRITE_BACK) capacity(25498M), associativity(512), data block size(4K) metadata block size(4096b) disk assoc(0K) skip sequential thresh(0K) total blocks(6527488), cached blocks(259), cache percent(0) dirty blocks(0), dirty percent(0) nr_queued(0)Size Hist: 4096:1497fcache-dev1: 0 209713152 flashcache conf: ssd dev (/dev/vdd1), disk dev (/dev/vdb1) cache mode(WRITE_BACK) capacity(25498M), associativity(512), data block size(4K) metadata block size(4096b) disk assoc(0K) skip sequential thresh(0K) total blocks(6527488), cached blocks(259), cache percent(0) dirty blocks(0), dirty percent(0) nr_queued(0)Size Hist: 4096:1497 使用flashcache device创建后的flashcache device可以像普通device一样使用，如下： 12345# mkfs.ext4 /dev/mapper/fcache-dev1# mount /dev/mapper/fcache-dev1 /mnt/# mount | grep mnt/dev/mapper/fcache-dev1 on /mnt type ext4 (rw,relatime,seclabel,data=ordered)# umount /mnt/ 查看flashcache device的flashcache状态在使用一段时间后，我们可以看flashcache的缓存状态如下，可以以此为依据来调整flashcache的策略。 123456789101112131415161718192021# dmsetup status fcache-dev10 209713152 flashcache stats: reads(1063), writes(422824) read hits(786), read hit percent(73) write hits(291) write hit percent(0) dirty write hits(44) dirty write hit percent(0) replacement(0), write replacement(7392) write invalidates(0), read invalidates(2) pending enqueues(2), pending inval(2) metadata dirties(422499), metadata cleans(112914) metadata batch(531237) metadata ssd writes(4432) cleanings(112914) fallow cleanings(0) no room(31) front merge(3042) back merge(109407) force_clean_block(0) disk reads(277), disk writes(112945) ssd reads(113700) ssd writes(427505) uncached reads(2), uncached writes(31), uncached IO requeue(0) disk read errors(0), disk write errors(0) ssd read errors(0) ssd write errors(0) uncached sequential reads(0), uncached sequential writes(0) pid_adds(0), pid_dels(0), pid_drops(0) pid_expiry(0) lru hot blocks(3263744), lru warm blocks(3263744) lru promotions(0), lru demotions(0) 查看flashcache device相关信息1234567# ls /proc/flashcache/vdd1+vdb1/flashcache_errors flashcache_iosize_hist flashcache_pidlists flashcache_stats# cat /proc/flashcache/vdd1+vdb1/flashcache_errorsdisk_read_errors=0 disk_write_errors=0 ssd_read_errors=0 ssd_write_errors=0 memory_alloc_errors=0# cat /proc/flashcache/vdd1+vdb1/flashcache_pidlistsBlacklist:Whitelist: sysctl查看flashcache的信息： 12345678910111213141516171819202122232425262728# sysctl -a | grep flashcachedev.flashcache.vdd1+vdb1.cache_all = 1dev.flashcache.vdd1+vdb1.clean_on_read_miss = 0dev.flashcache.vdd1+vdb1.clean_on_write_miss = 0dev.flashcache.vdd1+vdb1.dirty_thresh_pct = 20dev.flashcache.vdd1+vdb1.do_pid_expiry = 0dev.flashcache.vdd1+vdb1.do_sync = 0dev.flashcache.vdd1+vdb1.fallow_clean_speed = 2dev.flashcache.vdd1+vdb1.fallow_delay = 900dev.flashcache.vdd1+vdb1.fast_remove = 0dev.flashcache.vdd1+vdb1.io_latency_hist = 0dev.flashcache.vdd1+vdb1.lru_hot_pct = 75dev.flashcache.vdd1+vdb1.lru_promote_thresh = 2dev.flashcache.vdd1+vdb1.max_clean_ios_set = 2dev.flashcache.vdd1+vdb1.max_clean_ios_total = 4dev.flashcache.vdd1+vdb1.max_pids = 100dev.flashcache.vdd1+vdb1.new_style_write_merge = 0dev.flashcache.vdd1+vdb1.pid_expiry_secs = 60dev.flashcache.vdd1+vdb1.reclaim_policy = 0dev.flashcache.vdd1+vdb1.skip_seq_thresh_kb = 0dev.flashcache.vdd1+vdb1.stop_sync = 0dev.flashcache.vdd1+vdb1.zero_stats = 0# cd /proc/sys/dev/flashcache/vdd1+vdb1/# lscache_all dirty_thresh_pct fallow_clean_speed io_latency_hist max_clean_ios_set new_style_write_merge skip_seq_thresh_kbclean_on_read_miss do_pid_expiry fallow_delay lru_hot_pct max_clean_ios_total pid_expiry_secs stop_syncclean_on_write_miss do_sync fast_remove lru_promote_thresh max_pids reclaim_policy zero_stats 不明确上述参数是否可以动态调整？ 删除flashcache device12345678910111213# dmsetup info /dev/dm-0Name: fcache-dev1State: ACTIVERead Ahead: 8192Tables present: LIVEOpen count: 0Event number: 0Major, minor: 252, 0Number of targets: 1# dmsetup remove /dev/dm-0# flashcache_destroy /dev/vdd1flashcache_destroy: Destroying Flashcache found on /dev/vdd1. Any data will be lost !! 遇到的问题删除设备出错测试中在remove一个device mapper设备时，报“Device or resource busy”。 12345678910111213141516171819202122232425262728# dmsetup remove /dev/dm-1device-mapper: remove ioctl on fcache-dev2 failed: Device or resource busyCommand failed# dmsetup info /dev/dm-1Name: fcache-dev2State: ACTIVERead Ahead: 256Tables present: LIVEOpen count: 1Event number: 0Major, minor: 252, 1Number of targets: 1# dmsetup tablefcache-dev2: 0 209715200 flashcache conf: ssd dev (/dev/vdd2), disk dev (/dev/vdc) cache mode(WRITE_BACK) capacity(25498M), associativity(512), data block size(4K) metadata block size(4096b) disk assoc(0K) skip sequential thresh(0K) total blocks(6527488), cached blocks(259), cache percent(0) dirty blocks(0), dirty percent(0) nr_queued(0)Size Hist: 1024:2 4096:471740# lsof /dev/vdd2# lsof /dev/vdc# lsof /dev/dm-1 尝试remove --force参数： 12345678910111213141516# dmsetup remove /dev/dm-1 --forcedevice-mapper: remove ioctl on fcache-dev2 failed: Device or resource busyCommand failed# dmsetup tablefcache-dev2: 0 209715200 error# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT...vdb 253:16 0 100G 0 disk└─vdb1 253:17 0 100G 0 partvdc 253:32 0 100G 0 diskvdd 253:48 0 50G 0 disk├─vdd1 253:49 0 25G 0 part└─vdd2 253:50 0 25G 0 partfcache-dev2 252:1 0 100G 0 dm ## 出些在这里了??? 之后尝试很多命令，没法完全把该设备移除，只能重启，重启后一切正常。 创建设备出错1创建flashcache device时报错：“Valid Flashcache already exists on…”。 12345# flashcache_create -p back fcache-dev1 /dev/vdd1 /dev/vdb1cachedev fcache-dev1, ssd_devname /dev/vdd1, disk_devname /dev/vdb1 cache mode WRITE_BACKblock_size 8, md_block_size 8, cache_size 0flashcache_create: Valid Flashcache already exists on /dev/vdd1flashcache_create: Use flashcache_destroy first and then create again /dev/vdd1 这个出错比较明确，按照提示先销毁cache设备中的flashcache数据即可。 1234567# flashcache_destroy /dev/vdd1flashcache_destroy: Destroying Flashcache found on /dev/vdd1. Any data will be lost !!# flashcache_create -p back fcache-dev1 /dev/vdd1 /dev/vdb1cachedev fcache-dev1, ssd_devname /dev/vdd1, disk_devname /dev/vdb1 cache mode WRITE_BACKblock_size 8, md_block_size 8, cache_size 0Flashcache metadata will use 137MB of your 3951MB main memory 创建设备出错2测试中反复操作，创建flashcache device时报错：“Device or resource busy”。 1234567# flashcache_create -p back fcache-dev2 /dev/vdd2 /dev/vdccachedev fcache-dev2, ssd_devname /dev/vdd2, disk_devname /dev/vdc cache mode WRITE_BACKblock_size 8, md_block_size 8, cache_size 0Flashcache metadata will use 137MB of your 3951MB main memorydevice-mapper: reload ioctl on fcache-dev2 failed: Device or resource busyCommand failedecho 0 209715200 flashcache /dev/vdc /dev/vdd2 fcache-dev2 1 2 8 0 512 0 0 8 | dmsetup create fcache-dev2 failed 尝试销毁flashcache设备上的相关数据，之后创建还是报错： 12# flashcache_destroy /dev/vdd2flashcache_destroy: No valid Flashcache found on /dev/vdd2 再重启机器后，创建flashcache就正常了，比较奇怪 ;( 重启机器后设备消失重启机器后，看不到之前创建的flashcache device。 这是因为flashcache重启后不会自动加载设备，需要手动执行命令： 123456789101112131415# modprobe flashcache# flashcache_loadUsage: flashcache_load ssd_devname [cachedev]git commit:# flashcache_load /dev/vdd1# dmsetup infoName: fcache-dev1State: ACTIVERead Ahead: 8192Tables present: LIVEOpen count: 0Event number: 0Major, minor: 252, 0Number of targets: 1 若想flashcache和设备在机器重启后自动加载，可以把相关命令加入“/etc/rc.d/rc.local”。 /etc/rc.d/init.d/里也添加服务也是一个好方法 ;)]]></content>
      <categories>
        <category>storage</category>
      </categories>
      <tags>
        <tag>flashcache</tag>
        <tag>ssd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LVM Usage]]></title>
    <url>%2F2018%2F02%2F28%2Flvm-cmd-usage%2F</url>
    <content type="text"><![CDATA[概述之前文章大致做了LVM简介，推荐在使用本地存储时，使用LVM，那如何使用LVM呢？ 这里从LVM相关组件分析其常用命令，最后给出创建一个LVM volume的步骤。 物理卷相关物理卷 - PhysicalVolume LVM管理的最底层就是一个个物理卷，与之相关的命令基本都以pv开始，大致有如下这些： 12root@ubuntu:~# pvpvchange pvck pvcreate pvdisplay pvmove pvremove pvresize pvs pvscan 创建pvcreate - initialize a disk or partition for use by LVM 从物理盘或者一个分区初始化一个PV，这个PV之后会被LVM用来创建VG； 删除pvremove - remove a physical volume 删除一个PV，之后LVM将不认为该设备为PV； 显示pvs - report information about physical volumes 展示系统PVS的信息，可以指定输出格式等； pvdisplay - display attributes of a physical volume 展示一个/多个PVS的属性信息，可以指定输出格式等； 其他pvscan - scan all disks for physical volumes 扫描整个系统中支持LVM块设备的PVS； pvresize - resize a disk or partition in use by LVM2 调整LVM使用的一个PV的大小，前提是它对应的物理设备大小可以resize； pvchange - change attributes of a physical volume 修改一个PV的属性； 卷组相关卷组 - VolumeGroup 卷组是LVM基于PVS创建的一个块设备集合，类似于有的存储中说的Volume Pool，它综合了底层所有PVS的存储，做了统一的配置，比如：分配块大小等； 与卷组相关的命令都是以vg开始，大致如下： 123root@ubuntu:~# vgvgcfgbackup vgchange vgconvert vgdisplay vgextend vgimportclone vgmknodes vgremove vgs vgsplitvgcfgrestore vgck vgcreate vgexport vgimport vgmerge vgreduce vgrename vgscan 创建vgcreate - create a volume group 基于之前创建的PVS创建一个VG，这里原则上选择同质物理盘创建的PVS； 可以指定每次在PV上申请空间的Extent size（-s, –physicalextentsize PhysicalExtentSize[bBsSkKmMgGtTpPeE]） 删除vgremove - remove a volume group 删除一个VG；若VG上有LV，则会有相关提示； 显示vgs - report information about volume groups 显示系统中有的VGS信息； vgdisplay - display attributes of volume groups 显示VGS的详细属性信息； 其他vgextend - add physical volumes to a volume group 添加PVS到VG中，扩容VG； vgreduce - reduce a volume group 从VG中删除没有使用的PVS，若PV已经分配了PE，则命令失败； vgchange - change attributes of a volume group 修改一个VG的属性； 逻辑卷相关逻辑卷 - LogicalVolume 逻辑卷是最终面向用户使用的块设备，用户可以像使用普通磁盘/分区一样的使用它；同时逻辑卷支持很多普通磁盘没有的特性，比如：mirror，stripe，raid，thin等； 与逻辑卷相关的命令都是以lv开始，大致如下： 123root@ubuntu:~# lvlvchange lvcreate lvextend lvmchange lvmconfig lvmdump lvmpolld lvmsar lvremove lvresize lvscanlvconvert lvdisplay lvm lvmconf lvmdiskscan lvmetad lvmsadc lvreduce lvrename lvs 创建lvcreate - create a logical volume in an existing volume group 从一个创建好的VG里创建一个LV，可以指定LV的很多属性，比如：size，stripe，raid，thin等； 删除lvremove - remove a logical volume 删除一个LV；若该LV系统正在使用，则删除失败； 显示lvs - report information about logical volumes 显示系统LVS的基本信息； lvdisplay - display attributes of a logical volume 显示一个LV的详细属性信息； 其他lvextend - extend the size of a logical volume 扩容一个LV的size； lvresize - resize a logical volume 调整一个LV的size；可以扩容，可以缩容，但缩容需谨慎，可能导致数据丢失； lvchange - change attributes of a logical volume 修改一个LV的属性； lvscan - scan (all disks) for Logical Volumes 扫描系统获取所有LV的 active，snapshot/origin，size等信息； 创建LVM Volume步骤创建PV使用fdisk创建LVM Type的磁盘分区 12345678910111213141516171819202122232425262728293031[root@ceph0 ~]# fdisk /dev/sdjCommand (m for help): nPartition number (1-128, default 1):First sector (34-7814037134, default 2048):Last sector, +sectors or +size&#123;K,M,G,T,P&#125; (2048-7814037134, default 7814037134):Created partition 1 Command (m for help): tSelected partition 1Partition type (type L to list all types): 15Changed type of partition 'Linux filesystem' to 'Linux LVM' Command (m for help): p Disk /dev/sdj: 4000.8 GB, 4000787030016 bytes, 7814037168 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: gpt # Start End Size Type Name 1 2048 7814037134 3.7T Linux LVM Command (m for help): wThe partition table has been altered! Calling ioctl() to re-read partition table.Syncing disks. 12345678910111213141516171819[root@ceph0 ~]# pvs PV VG Fmt Attr PSize PFree /dev/sdi1 lvm2 --- 3.64t 3.64t /dev/sdj1 lvm2 --- 3.64t 3.64t /dev/sdk1 lvm2 --- 3.64t 3.64t /dev/sdl1 lvm2 --- 3.64t 3.64t [root@ceph0 ~]# pvdisplay /dev/sdi1 --- Physical volume --- PV Name /dev/sdi1 VG Name volgroup-sata PV Size 3.64 TiB / not usable 2.82 MiB Allocatable yes PE Size 4.00 MiB Total PE 953861 Free PE 953605 Allocated PE 256 PV UUID rGiKHq-yem2-DUSH-F5AM-4jxE-eZvv-cF1YlG 创建VG1234567891011121314151617181920212223242526272829[root@ceph0 ~]# vgcreate volgroup-sata /dev/sdi1 /dev/sdj1 /dev/sdk1 Volume group "volgroup-sata" successfully created[root@ceph0 ~]#[root@ceph0 ~]# vgs VG #PV #LV #SN Attr VSize VFree volgroup-sata 3 0 0 wz--n- 10.92t 10.92t [root@ceph0 ~]# vgdisplay volgroup-sata --- Volume group --- VG Name volgroup-sata System ID Format lvm2 Metadata Areas 3 Metadata Sequence No 9 VG Access read/write VG Status resizable MAX LV 0 Cur LV 1 Open LV 0 Max PV 0 Cur PV 3 Act PV 3 VG Size 10.92 TiB PE Size 4.00 MiB Total PE 2861583 Alloc PE / Size 256 / 1.00 GiB Free PE / Size 2861327 / 10.92 TiB VG UUID 7rEMc5-pkzR-ckFM-8Yve-cATM-uy9B-buMcR6 创建LV123456789101112131415161718192021222324[root@ceph0 ~]# lvcreate -n lv1 -L 1G volgroup-sata Logical volume "lv1" created.[root@ceph0 ~]# lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert lv1 volgroup-sata -wi-a----- 1.00g [root@ceph0 ~]# lvdisplay volgroup-sata/lv1 --- Logical volume --- LV Path /dev/volgroup-sata/lv1 LV Name lv1 VG Name volgroup-sata LV UUID SWp2Lr-kGLC-8USu-3jqu-Bwpd-yN3D-wMGFGn LV Write Access read/write LV Creation host, time ceph0, 2018-01-26 16:19:17 +0800 LV Status available # open 0 LV Size 1.00 GiB Current LE 256 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 256 Block device 253:3 使用LV可以像使用系统物理device一样，使用LogicalVolume； 12[root@ceph0 ~]# mkfs.ext4 /dev/volgroup-sata/lv1[root@ceph0 ~]# mount /dev/volgroup-sata/lv1 /mnt/]]></content>
      <categories>
        <category>storage</category>
      </categories>
      <tags>
        <tag>lvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LVM简介]]></title>
    <url>%2F2018%2F02%2F06%2Flvm-intro%2F</url>
    <content type="text"><![CDATA[LVM基本组成LVM利用Linux内核的device-mapper来实现存储系统的虚拟化（系统分区独立于底层硬件）。 通过LVM，你可以实现存储空间的抽象化并在上面建立虚拟分区（virtual partitions），可以更简便地扩大和缩小分区，可以增删分区时无需担心某个硬盘上没有足够的连续空间， LVM是用来方便管理的，不会提供额外的安全保证。 LVM的基本组成块（building blocks）如下： 物理卷Physical volume (PV)：可以在上面建立卷组的媒介，可以是硬盘分区，也可以是硬盘本身或者回环文件（loopback file）。物理卷包括一个特殊的header，其余部分被切割为一块块物理区域（physical extents）。 卷组Volume group (VG)：将一组物理卷收集为一个管理单元。 逻辑卷Logical volume (LV)：虚拟分区，由物理区域（physical extents）组成。 物理区域Physical extent (PE)：硬盘可供指派给逻辑卷的最小单位（通常为4MB）。 示例: 12345678910111213两块物理硬盘 硬盘1 (/dev/sda): _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ |分区1 50GB (物理卷) |分区2 80GB (物理卷) | |/dev/sda1 |/dev/sda2 | |_ _ _ _ _ _ _ _ _ _ _ _ _ _ _|_ _ _ _ _ _ _ _ _ _ _ _ _ _ __| 硬盘2 (/dev/sdb): _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ |分区1 120GB (物理卷) | |/dev/sdb1 | | _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _| 123LVM方式 卷组VG1 (/dev/storage1/ = /dev/sda1): 1卷组VG2 (/dev/storage2/ = /dev/sda2 + /dev/sdb1): 1234 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ |逻辑卷1 15GB |逻辑卷2 35GB |逻辑卷3 200GB ||/dev/storage1/rootvol |/dev/storage2/homevol |/dev/storage2/mediavol ||_ _ _ _ _ _ _ _ _ _ _ _ |_ _ _ _ _ _ _ _ _ _ _ _ _ |_ _ _ _ _ _ _ _ _ _ _ _ _ _| 图解如下 优点比起正常的硬盘分区管理，LVM更富于弹性： 使用卷组(VG)，使众多硬盘空间看起来像一个大硬盘。 使用逻辑卷（LV），可以创建跨越众多硬盘空间的分区。 可以创建小的逻辑卷（LV），在空间不足时再动态调整它的大小。 在调整逻辑卷（LV）大小时可以不用考虑逻辑卷在硬盘上的位置，不用担心没有可用的连续空间。It does not depend on the position of the LV within VG, there is no need to ensure surrounding available space. 可以在线（online）对逻辑卷（LV）和卷组（VG）进行创建、删除、调整大小等操作。LVM上的文件系统也需要重新调整大小，某些文件系统也支持这样的在线操作。 无需重新启动服务，就可以将服务中用到的逻辑卷（LV）在线（online）/动态（live）迁移至别的硬盘上。 允许创建快照，可以保存文件系统的备份，同时使服务的下线时间（downtime）降低到最小。 这些优点使得LVM对服务器的管理非常有用，对于桌面系统管理的帮助则没有那么显著，你需要根据实际情况进行取舍。 缺点 在系统设置时需要更复杂的额外步骤。 系统支持当前Linux系统都支持LVM，我们常用的Ubuntu和Centos上都可以方便安装，具体如下： Ubuntu： 12345678910111213root@ubuntu:~# lsb_release -aNo LSB modules are available.Distributor ID: UbuntuDescription: Ubuntu 16.04.2 LTSRelease: 16.04Codename: xenial root@ubuntu:~# apt-get install -y lvm2...root@ubuntu:~# lvm version LVM version: 2.02.133(2) (2015-10-30) Library version: 1.02.110 (2015-10-30) Driver version: 4.34.0 Centos： 12345678910111213root@centos:~# lsb_release -aLSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarchDistributor ID: CentOSDescription: CentOS Linux release 7.3.1611 (Core)Release: 7.3.1611Codename: Core root@centos:~# yum install -y lvm2...root@centos:~# lvm version LVM version: 2.02.171(2)-RHEL7 (2017-05-03) Library version: 1.02.140-RHEL7 (2017-05-03) Driver version: 4.34.0 使用建议使用本地存储时，LVM是对比物理盘有明显的优点，建议除非特别简单使用物理盘的场景，都可以使用LVM来提供存储服务。 使用时候的一些建议如下： 一个物理盘对应一个PV 多个PV构成一个VG 同一VG内最好使用相同性能的物理盘 不同性能的物理盘，创建不同的VG 创建适当大小的LV，容量不够时再扩容]]></content>
      <categories>
        <category>storage</category>
      </categories>
      <tags>
        <tag>lvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LVM之Cache加速]]></title>
    <url>%2F2018%2F01%2F30%2Flvm-with-cache%2F</url>
    <content type="text"><![CDATA[概述我们知道LVM是管理本地磁盘，更好的提供块设备服务的机制； 在现实环境中，我们的机器通常都有几种不同类型的磁盘存在，高性能的磁盘容量低，容量大的磁盘性能差，那如何利用Linux系统的各种cache机制来提升LVM卷的性能呢？ 存储磁盘分类通常我们使用的存储磁盘有三种，分别对应不同的容量、性能、价格，有：SATA盘，SSD盘和PCIE-SSD； 他们三个的大致性能容量和价格如下表所示： 设备 容量 性能 BandWidth IOPS 价格 厂商 应用场景 SATA 容量大，常见为4-8TB 低 约100MB/s 100 低 希捷，西数等 大容量 SSD 容量小，常见为几百GB 高 约500MB/s 读：30, 000+写：5, 000+ 高 Intel，Samsung等 高性能，加速应用 PCIE-SSD 容量较大，常见为1-4T 超高 约2GB/s 读：500, 000+写：100, 000+ 较高 Intel，Samsung，Memblaze，宝存等 超高性能，加速应用 从上面可以看出，可以作为普通磁盘加速的磁盘有：SSD盘和PCIE-SSD，其中SSD盘是最常见和用的最多的，PCIE卡在超高性能需求场所有使用。 注：其实现在SSD/PCIE-SSD的每GB价格相差不大了，若对超高性能有需求，PCIE-SSD是个很好的选择； Cache算法通常我们使用高性能磁盘来加速低性能磁盘时，都会选择一种Linux内核支持的Cache算法，现在Linux内核支持的常用Cache算法有如下几种； flashcacheflashcache 是 facebook 开源的 ssd 存储产品，它基于内核的 devicemapper 机制，允许将 ssd 设备映射为机械存储设备的缓存，堆叠成为一个虚拟设备供用户读写，从而在一定程度上兼顾 ssd 的高速与机械存储设备的高容量，更加经济高效地支撑线上业务。 flashcache 支持三种缓存策略： 回写(Write Back)：修改内容之后，并不立即写入后端设备 写透(Write Through): 修改内容时写入后端设备，同时也更新前端设备中的缓存块 环写(Write Around): 修改内容时，先写入后端设备，同时使前端设备中对应的缓存块失效 参考： https://github.com/facebookarchive/flashcache https://en.wikipedia.org/wiki/Flashcache bcachebcache是linux内核块设备层cache，类似于flashcache使用ssd作为hdd的缓存方案，相比于flashcache，bcache更加灵活，支持ssd作为多块hdd的共享缓存，并且还支持多块ssd（还未完善），能够在运行中动态增加，删除缓存设备和后端设备。 从3.10开始，bcache进入内核主线。bcache支持writeback、writethrough、writearoud三种策略，默认是wriththrough，可以动态修改，缓存替换方式支持lru、fifo和random三种。 参考： https://bcache.evilpiepirate.org/ http://www.sysnote.org/2014/06/20/bcache-analysis/ dm-cache作为linux内核的一部分，dm-cache采用device mapper机制允许用户建立混合卷。dm-cache可以采用一个或多个快速设备为后端慢速存储系统扮演缓存的角色。 dm-cache设计成由3个物理存储设备来混合成一个逻辑卷的形式。操作模式和缓存策略决定了缓存数据的性能。这三个物理设备分别为： 原始设备：提供主要的慢速存储（通常是一个硬盘或者SAN） 缓存设备：提供高速原始设备数据的缓存（通常是一个SSD） 元数据设备：记录硬盘块在缓存中的位置，脏标志以及执行缓存策略所需的内部数据 dm-cache支持写回，写通和旁路三种模式。 参考： https://en.wikipedia.org/wiki/Dm-cache LVM加速策略flashcache基于flashcache的使用策略（SSD-SATA 1对1），我们有如下策略： bcachebcache可以用一块SSD盘做多个SATA盘的缓存，有如下策略： LVM自带Cache策略LVM自带Cache策略可以配置两个LV之间做缓存，有如下策略： LVM Cache策略选择现实中，考虑稳定性和可靠性，使用flashcache作为Cache策略的比较多。 但最近bcache也越来越得到大范围的使用，在某些测试报告中，其性能多数场景优于flashcache。]]></content>
      <categories>
        <category>storage</category>
      </categories>
      <tags>
        <tag>lvm</tag>
        <tag>flashcache</tag>
        <tag>bcache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[valgrind简介和使用]]></title>
    <url>%2F2017%2F12%2F29%2Fvalgrind-intro-usage%2F</url>
    <content type="text"><![CDATA[概述最近有需求要分析定位下开发的文件存储网关的内存泄露问题，对比了几款后选择了valgrind这款工具，功能很是强大，这里我还只使用了一些基本的功能，记录如下。 Valgrind支持很多工具: Memcheck，Addrcheck，Cachegrind，Massif，Helgrind和Callgrind等。 官网：http://valgrind.org/ 安装1234567$ wget ftp://sourceware.org/pub/valgrind/valgrind-3.13.0.tar.bz2$ bzip2 -d valgrind-3.13.0.tar.bz2$ tar -xf valgrind-3.13.0.tar$ cd valgrind-3.13.0/$ ./configure$ make$ sudo make install 使用与分析1234567891011121314$ valgrind --helpusage: valgrind [options] prog-and-args...常用的一些options有：--trace-children=no|yes Valgrind-ise child processes (follow execve)? [no]--log-file=&lt;file&gt; log messages to &lt;file&gt;--leak-check=no|summary|full search for memory leaks at exit? [summary]--show-reachable=yes same as --show-leak-kinds=all --show-reachable=no --show-possibly-lost=yes same as --show-leak-kinds=definite,possible --show-reachable=no --show-possibly-lost=no same as --show-leak-kinds=definite 常用命令格式： 1$ /usr/local/bin/valgrind --log-file=mylog ./file-gateway valgrind的memcheck组件支持的内存泄露检查大概有： 读写非法的内存 访问未初始化的内存 访问已经释放了的内存 申请的内存没有释放 重复释放内存 释放非法内存 source和destination内存的重叠 uninitialised byte(s)检查出程序中未初始化而访问的变量 12345678910111213141516==1350634== Thread 14:==1350634== Syscall param setxattr(value) points to uninitialised byte(s)==1350634== at 0x6D9EB8A: setxattr (in /usr/lib64/libc-2.17.so)==1350634== by 0x487268: VFS::file_setxattr(std::string, std::string, void*, int) (vfs.cc:64)==1350634== by 0x488066: FileGwVFS::file_setxattr(std::string, std::string, void*, int) (vfs.cc:175)==1350634== by 0x459583: FileGateway::set_file_xattr(std::string, sgwd_xattr*) (file_gateway.cc:742)==1350634== by 0x463706: FileGwRedisWorkItem::update_file_xattr_status(gateway_file_status) (redis_work_item.cc:703)==1350634== by 0x4641B1: FileGwRedisWorkItem::file_upload() (redis_work_item.cc:793)==1350634== by 0x45F89E: FileGwRedisWorkItem::process() (redis_work_item.cc:190)==1350634== by 0x486532: WorkThread::process_work_item() (work_thread.cc:49)==1350634== by 0x486850: ShardedWorkThread::entry() (work_thread.cc:112)==1350634== by 0x481B58: Thread::_entry_func(void*) (thread.cc:74)==1350634== by 0x4E3DDC4: start_thread (in /usr/lib64/libpthread-2.17.so)==1350634== by 0x6D9FCEC: clone (in /usr/lib64/libc-2.17.so)==1350634== Address 0x115d95a4 is on thread 14's stack==1350634== in frame #4, created by FileGwRedisWorkItem::update_file_xattr_status(gateway_file_status) (redis_work_item.cc:697) 123456789==1350634== Thread 19:==1350634== Conditional jump or move depends on uninitialised value(s)==1350634== at 0x465155: FileGwRedisWorkItem::callback(void*) (redis_work_item.cc:946)==1350634== by 0x481021: FileGwS3WorkItem::postProcess() (s3_work_item.cc:244)==1350634== by 0x486551: WorkThread::process_work_item() (work_thread.cc:50)==1350634== by 0x48666E: WorkThread::entry() (work_thread.cc:78)==1350634== by 0x481B58: Thread::_entry_func(void*) (thread.cc:74)==1350634== by 0x4E3DDC4: start_thread (in /usr/lib64/libpthread-2.17.so)==1350634== by 0x6D9FCEC: clone (in /usr/lib64/libc-2.17.so) Invalid read/write读取非法地址，通常是继续访问释放后的内存空间 1234567891011121314151617181920212223242526272829303132333435==1346085== Thread 14:==1346085== Invalid read of size 4==1346085== at 0x4E40BB0: pthread_mutex_unlock (in /usr/lib64/libpthread-2.17.so)==1346085== by 0x4559FC: __gthread_mutex_unlock(pthread_mutex_t*) (gthr-default.h:778)==1346085== by 0x45B6C1: std::mutex::unlock() (mutex:152)==1346085== by 0x489CA8: std::lock_guard&lt;std::mutex&gt;::~lock_guard() (mutex:420)==1346085== by 0x4895BB: WorkItemCompletion::finish() (thread_work_item.cc:47)==1346085== by 0x48971E: ThreadWorkItem::postProcess() (thread_work_item.cc:70)==1346085== by 0x45F721: RedisWorkItem::postProcess() (redis_work_item.cc:139)==1346085== by 0x45FFED: FileGwRedisWorkItem::postProcess() (redis_work_item.cc:234)==1346085== by 0x486761: WorkThread::process_work_item() (work_thread.cc:50)==1346085== by 0x486A60: ShardedWorkThread::entry() (work_thread.cc:112)==1346085== by 0x481D68: Thread::_entry_func(void*) (thread.cc:74)==1346085== by 0x4E3DDC4: start_thread (in /usr/lib64/libpthread-2.17.so)==1346085== Address 0xabd3788 is 24 bytes inside a block of size 88 free'd==1346085== at 0x4C2B1CD: operator delete(void*) (vg_replace_malloc.c:576)==1346085== by 0x489253: WorkItemCompletion::~WorkItemCompletion() (thread_work_item.cc:29)==1346085== by 0x4572E4: FileGateway::issue_filegw_redis_workitem(FileOp*, FileInfo*, bool, bool, void (*)(void*), void*) (file_gateway.cc:298)==1346085== by 0x457D31: FileGateway::do_read(FileOp*) (file_gateway.cc:455)==1346085== by 0x455298: GatewayWorkItem::process() (gateway_work_item.cc:74)==1346085== by 0x486742: WorkThread::process_work_item() (work_thread.cc:49)==1346085== by 0x48687E: WorkThread::entry() (work_thread.cc:78)==1346085== by 0x481D68: Thread::_entry_func(void*) (thread.cc:74)==1346085== by 0x4E3DDC4: start_thread (in /usr/lib64/libpthread-2.17.so)==1346085== by 0x6D9FCEC: clone (in /usr/lib64/libc-2.17.so)==1346085== Block was alloc'd at==1346085== at 0x4C2A243: operator new(unsigned long) (vg_replace_malloc.c:334)==1346085== by 0x457171: FileGateway::issue_filegw_redis_workitem(FileOp*, FileInfo*, bool, bool, void (*)(void*), void*) (file_gateway.cc:286)==1346085== by 0x457D31: FileGateway::do_read(FileOp*) (file_gateway.cc:455)==1346085== by 0x455298: GatewayWorkItem::process() (gateway_work_item.cc:74)==1346085== by 0x486742: WorkThread::process_work_item() (work_thread.cc:49)==1346085== by 0x48687E: WorkThread::entry() (work_thread.cc:78)==1346085== by 0x481D68: Thread::_entry_func(void*) (thread.cc:74)==1346085== by 0x4E3DDC4: start_thread (in /usr/lib64/libpthread-2.17.so)==1346085== by 0x6D9FCEC: clone (in /usr/lib64/libc-2.17.so) SUMMARY在valgrind最后会有内存分析的总结 12345678910111213141516==1366931== HEAP SUMMARY:==1366931== in use at exit: 41,110 bytes in 1,019 blocks==1366931== total heap usage: 1,275,222 allocs, 1,274,203 frees, 334,495,820 bytes allocated==1366931====1366931== LEAK SUMMARY:==1366931== definitely lost: 11,632 bytes in 191 blocks==1366931== indirectly lost: 80 bytes in 2 blocks==1366931== possibly lost: 608 bytes in 1 blocks==1366931== still reachable: 28,790 bytes in 825 blocks==1366931== of which reachable via heuristic:==1366931== stdstring : 145 bytes in 4 blocks==1366931== suppressed: 0 bytes in 0 blocks==1366931== Rerun with --leak-check=full to see details of leaked memory==1366931====1366931== For counts of detected and suppressed errors, rerun with: -v==1366931== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0) LEAK SUMMARY：内存泄露信息的总结，详细信息从前面的log中查找 definitely lost: 肯定丢失的内存，这部分必须处理 indirectly lost: 间接丢失的内存，可能是一个structure里指向的内存 possibly lost: 可能丢失的部分，这是由于C/C++语言指针处理的特点造成的，可能不太准确 still reachable: 程序可能是ok的，可以配置--show-reachable=no不显示这部分 若想显示泄露内存的详细信息，使用参数--leak-check=full /usr/local/bin/valgrind --leak-check=full --log-file=mylog ./flle-gateway 1234567891011121314151617181920212223242526272829303132333435363738==1373210== HEAP SUMMARY:==1373210== in use at exit: 30,758 bytes in 843 blocks==1373210== total heap usage: 405,641 allocs, 404,798 frees, 44,538,821 bytes allocated==1373210====1373210== 608 bytes in 1 blocks are possibly lost in loss record 605 of 613==1373210== at 0x4C2B9B5: calloc (vg_replace_malloc.c:711)==1373210== by 0x4011DE4: _dl_allocate_tls (in /usr/lib64/ld-2.17.so)==1373210== by 0x4E3E960: pthread_create@@GLIBC_2.2.5 (in /usr/lib64/libpthread-2.17.so)==1373210== by 0x489969: libsgsvc_init (in /home/yangguanjun3/storage-file-gateway/userspace/build/storage-flle-gw)==1373210== by 0x456B06: FileGateway::init_netlink() (file_gateway.cc:218)==1373210== by 0x456CA4: FileGateway::init() (file_gateway.cc:234)==1373210== by 0x4537A2: main (main.cc:79)==1373210====1373210== 1,360 bytes in 17 blocks are definitely lost in loss record 611 of 613==1373210== at 0x4C29C23: malloc (vg_replace_malloc.c:299)==1373210== by 0x4785C7: Aws::Utils::Stream::PreallocatedStreamBuf* Aws::New&lt;Aws::Utils::Stream::PreallocatedStreamBuf, Aws::Utils::Array&lt;unsigned char&gt;*, unsigned long&gt;(char const*, Aws::Utils::Array&lt;unsigned char&gt;*&amp;&amp;, unsigned long&amp;&amp;) (AWSMemory.h:70)==1373210== by 0x473446: S3BackendStorage::put_object_from_file_slice(std::basic_string&lt;char, std::char_traits&lt;char&gt;, Aws::Allocator&lt;char&gt; &gt;, std::basic_string&lt;char, std::char_traits&lt;char&gt;, Aws::Allocator&lt;char&gt; &gt;, std::string const&amp;, unsigned long, unsigned long) (s3_storage.cc:294)==1373210== by 0x480110: FileGwS3WorkItem::file_upload() (s3_work_item.cc:147)==1373210== by 0x480C0D: FileGwS3WorkItem::process() (s3_work_item.cc:221)==1373210== by 0x486316: WorkThread::process_work_item() (work_thread.cc:49)==1373210== by 0x486452: WorkThread::entry() (work_thread.cc:78)==1373210== by 0x48193C: Thread::_entry_func(void*) (thread.cc:74)==1373210== by 0x4E3DDC4: start_thread (in /usr/lib64/libpthread-2.17.so)==1373210== by 0x6D9FCEC: clone (in /usr/lib64/libc-2.17.so)==1373210====1373210== LEAK SUMMARY:==1373210== definitely lost: 1,360 bytes in 17 blocks==1373210== indirectly lost: 0 bytes in 0 blocks==1373210== possibly lost: 608 bytes in 1 blocks==1373210== still reachable: 28,790 bytes in 825 blocks==1373210== of which reachable via heuristic:==1373210== stdstring : 145 bytes in 4 blocks==1373210== suppressed: 0 bytes in 0 blocks==1373210== Reachable blocks (those to which a pointer was found) are not shown.==1373210== To see them, rerun with: --leak-check=full --show-leak-kinds=all==1373210====1373210== For counts of detected and suppressed errors, rerun with: -v==1373210== ERROR SUMMARY: 2 errors from 2 contexts (suppressed: 0 from 0) 参考http://valgrind.org/docs/manual/manual.htmlhttp://www.oschina.net/translate/valgrind-memcheckhttps://www.ibm.com/developerworks/cn/linux/l-cn-valgrind/]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ceph radosgw where to store data?]]></title>
    <url>%2F2017%2F12%2F20%2Fceph-radosgw-where-to-store-data%2F</url>
    <content type="text"><![CDATA[基于ceph jewel 10.2.5版本 参考文档：http://docs.ceph.com/docs/master/radosgw/layout/ 测试数据创建一个user：ictfox创建这个user下的一个bucket：bruins往bucket里写一个object：hello.txt 所以这里的测试数据有： user info bucket info object info 注：本文描述了查询radosgw数据和元数据的方法，没有深入去解析一些命令输出；若有错误，欢迎指正 ;) RadosGW的pools查看radosgw对应的所有pools，其所有的数据也都存在这些pools里。 123456789101112# rados lspools.rgw.rootdefault.rgw.controldefault.rgw.data.rootdefault.rgw.gcdefault.rgw.logdefault.rgw.users.uiddefault.rgw.users.keysdefault.rgw.buckets.indexdefault.rgw.buckets.datadefault.rgw.users.swiftdefault.rgw.users.email 很多pools的用途通过名字就能明确，若要明确pools的用途，请参考代码: rgw/rgw_rados.cc 1234567891011121314151617181920212223242526static string default_storage_pool_suffix = "rgw.buckets.data";static string default_bucket_index_pool_suffix = "rgw.buckets.index";static string default_storage_extra_pool_suffix = "rgw.buckets.non-ec";static string zone_info_oid_prefix = "zone_info.";static string zone_names_oid_prefix = "zone_names.";static string region_info_oid_prefix = "region_info.";static string zone_group_info_oid_prefix = "zonegroup_info.";static string realm_names_oid_prefix = "realms_names.";static string realm_info_oid_prefix = "realms.";static string default_region_info_oid = "default.region";static string default_zone_group_info_oid = "default.zonegroup";static string period_info_oid_prefix = "periods.";static string period_latest_epoch_info_oid = ".latest_epoch";static string region_map_oid = "region_map";static string zonegroup_map_oid = "zonegroup_map";static string log_lock_name = "rgw_log_lock";static string default_realm_info_oid = "default.realm";const string default_zonegroup_name = "default";const string default_zone_name = "default";static string zonegroup_names_oid_prefix = "zonegroups_names.";static string RGW_DEFAULT_ZONE_ROOT_POOL = "rgw.root";static string RGW_DEFAULT_ZONEGROUP_ROOT_POOL = "rgw.root";static string RGW_DEFAULT_REALM_ROOT_POOL = "rgw.root";static string RGW_DEFAULT_PERIOD_ROOT_POOL = "rgw.root"; 1234567891011121314151617181920212223242526272829303132333435363738394041int RGWZoneParams::fix_pool_names()&#123; list&lt;string&gt; zones; int r = store-&gt;list_zones(zones); if (r &lt; 0) &#123; ldout(cct, 10) &lt;&lt; "WARNING: store-&gt;list_zones() returned r=" &lt;&lt; r &lt;&lt; dendl; &#125; set&lt;string&gt; pool_names; r = get_zones_pool_names_set(cct, store, zones, id, pool_names); if (r &lt; 0) &#123; ldout(cct, 0) &lt;&lt; "Error: get_zones_pool_names" &lt;&lt; r &lt;&lt; dendl; return r; &#125; domain_root = fix_zone_pool_name(pool_names, name, ".rgw.data.root", domain_root.name); if (!metadata_heap.name.empty()) &#123; metadata_heap = fix_zone_pool_name(pool_names, name, ".rgw.meta", metadata_heap.name); &#125; control_pool = fix_zone_pool_name(pool_names, name, ".rgw.control", control_pool.name); gc_pool = fix_zone_pool_name(pool_names, name,".rgw.gc", gc_pool.name); log_pool = fix_zone_pool_name(pool_names, name, ".rgw.log", log_pool.name); intent_log_pool = fix_zone_pool_name(pool_names, name, ".rgw.intent-log", intent_log_pool.name); usage_log_pool = fix_zone_pool_name(pool_names, name, ".rgw.usage", usage_log_pool.name); user_keys_pool = fix_zone_pool_name(pool_names, name, ".rgw.users.keys", user_keys_pool.name); user_email_pool = fix_zone_pool_name(pool_names, name, ".rgw.users.email", user_email_pool.name); user_swift_pool = fix_zone_pool_name(pool_names, name, ".rgw.users.swift", user_swift_pool.name); user_uid_pool = fix_zone_pool_name(pool_names, name, ".rgw.users.uid", user_uid_pool.name); for(auto&amp; iter : placement_pools) &#123; iter.second.index_pool = fix_zone_pool_name(pool_names, name, "." + default_bucket_index_pool_suffix, iter.second.index_pool); iter.second.data_pool = fix_zone_pool_name(pool_names, name, "." + default_storage_pool_suffix, iter.second.data_pool); iter.second.data_extra_pool= fix_zone_pool_name(pool_names, name, "." + default_storage_extra_pool_suffix, iter.second.data_extra_pool); &#125; return 0;&#125; radosgw的元数据信息通过命令radosgw-admin查看原数据信息，包括bucket信息和user信息。 user信息radosgw-admin user info命令 123456789101112131415161718192021222324252627282930313233# radosgw-admin user info --uid=ictfox&#123; "user_id": "ictfox", "display_name": "mike", "email": "", "suspended": 0, "max_buckets": 1000, "auid": 0, "subusers": [], "keys": [ &#123; "user": "ictfox", "access_key": "IYZ800MDM7VF3EDLISC7", "secret_key": "05HfpRrrh1Gs1p8bxBbcn2HcGp0n7UvuaMNPCuHS" &#125; ], "swift_keys": [], "caps": [], "op_mask": "read, write, delete", "default_placement": "", "placement_tags": [], "bucket_quota": &#123; "enabled": false, "max_size_kb": -1, "max_objects": -1 &#125;, "user_quota": &#123; "enabled": false, "max_size_kb": -1, "max_objects": -1 &#125;, "temp_url_keys": []&#125; radosgw-admin metadata命令 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# radosgw-admin metadata list user[ "ictfox"]# radosgw-admin metadata get user:ictfox&#123; "key": "user:ictfox", "ver": &#123; "tag": "_laJXG6YTAtNd-Dzt78okwU5", "ver": 1 &#125;, "mtime": "2017-03-14 08:49:04.735466Z", "data": &#123; "user_id": "ictfox", "display_name": "mike", "email": "", "suspended": 0, "max_buckets": 1000, "auid": 0, "subusers": [], "keys": [ &#123; "user": "ictfox", "access_key": "IYZ800MDM7VF3EDLISC7", "secret_key": "05HfpRrrh1Gs1p8bxBbcn2HcGp0n7UvuaMNPCuHS" &#125; ], "swift_keys": [], "caps": [], "op_mask": "read, write, delete", "default_placement": "", "placement_tags": [], "bucket_quota": &#123; "enabled": false, "max_size_kb": -1, "max_objects": -1 &#125;, "user_quota": &#123; "enabled": false, "max_size_kb": -1, "max_objects": -1 &#125;, "temp_url_keys": [], "attrs": [ &#123; "key": "user.rgw.idtag", "val": "" &#125;, &#123; "key": "user.rgw.manifest", "val": "" &#125; ] &#125;&#125; bucket信息radosgw-admin bucket stats命令 12345678910111213141516171819202122232425# radosgw-admin bucket stats --bucket=bruins&#123; "bucket": "bruins", "pool": "default.rgw.buckets.data", "index_pool": "default.rgw.buckets.index", "id": "070ec129-2e84-47fe-b32a-b0bf9914a51f.14274.1", "marker": "070ec129-2e84-47fe-b32a-b0bf9914a51f.14274.1", "owner": "ictfox", "ver": "0#11", "master_ver": "0#0", "mtime": "2017-03-15 00:46:37.189715", "max_marker": "0#", "usage": &#123; "rgw.main": &#123; "size_kb": 1, "size_kb_actual": 4, "num_objects": 1 &#125; &#125;, "bucket_quota": &#123; "enabled": false, "max_size_kb": -1, "max_objects": -1 &#125;&#125; radosgw-admin metadata命令 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687# radosgw-admin metadata list bucket[ "bruins",]# radosgw-admin metadata get bucket:bruins&#123; "key": "bucket:bruins", "ver": &#123; "tag": "_osgTpZcD-jCFgcwdi3yvHmH", "ver": 1 &#125;, "mtime": "2017-03-15 04:46:37.193584Z", "data": &#123; "bucket": &#123; "name": "bruins", "pool": "default.rgw.buckets.data", "data_extra_pool": "default.rgw.buckets.non-ec", "index_pool": "default.rgw.buckets.index", "marker": "070ec129-2e84-47fe-b32a-b0bf9914a51f.14274.1", "bucket_id": "070ec129-2e84-47fe-b32a-b0bf9914a51f.14274.1", "tenant": "" &#125;, "owner": "ictfox", "creation_time": "0.000000", "linked": "true", "has_bucket_info": "false" &#125;&#125;# radosgw-admin metadata list bucket.instance[ "bruins:070ec129-2e84-47fe-b32a-b0bf9914a51f.14274.1",]# radosgw-admin metadata get bucket.instance:bruins:070ec129-2e84-47fe-b32a-b0bf9914a51f.14274.1&#123; "key": "bucket.instance:bruins:070ec129-2e84-47fe-b32a-b0bf9914a51f.14274.1", "ver": &#123; "tag": "_n2XczjnP1PYx3Hg88iruBbm", "ver": 1 &#125;, "mtime": "2017-03-15 04:46:37.189715Z", "data": &#123; "bucket_info": &#123; "bucket": &#123; "name": "bruins", "pool": "default.rgw.buckets.data", "data_extra_pool": "default.rgw.buckets.non-ec", "index_pool": "default.rgw.buckets.index", "marker": "070ec129-2e84-47fe-b32a-b0bf9914a51f.14274.1", "bucket_id": "070ec129-2e84-47fe-b32a-b0bf9914a51f.14274.1", "tenant": "" &#125;, "creation_time": "0.000000", "owner": "ictfox", "flags": 0, "zonegroup": "d40f14ea-4700-43e7-a92e-fa232f28b590", "placement_rule": "default-placement", "has_instance_obj": "true", "quota": &#123; "enabled": false, "max_size_kb": -1, "max_objects": -1 &#125;, "num_shards": 0, "bi_shard_hash_type": 0, "requester_pays": "false", "has_website": "false", "swift_versioning": "false", "swift_ver_location": "" &#125;, "attrs": [ &#123; "key": "user.rgw.acl", "val": "AgJ7AAAAAwISAAAABgAAAGljdGZveAQAAABtaWtlAwNdAAAAAQEAAAAGAAAAaWN0Zm94DwAAAAEAAAAGAAAAaWN0Zm94BAMyAAAAAgIEAAAAAAAAAAYAAABpY3Rmb3gAAAAAAAAAAAICBAAAAA8AAAAEAAAAbWlrZQAAAAAAAAAA" &#125;, &#123; "key": "user.rgw.idtag", "val": "" &#125;, &#123; "key": "user.rgw.manifest", "val": "" &#125; ] &#125;&#125; rgw的user uid信息保存格式每个user在该pool里都有两个objects 123# rados ls -p default.rgw.users.uidictfox.bucketsictfox 查看两个objects的omap和存储数据： 1234# rados -p default.rgw.users.uid listomapvals ictfox# rados -p default.rgw.users.uid get ictfox file# cat fileictfox .IYZ800MDM7VF3EDLISC7(05HfpRrrh1Gs1p8bxBbcn2HcGp0n7UvuaMNPCuHSmikeictfoxIYZ800MDM7VF3EDLISC7HIYZ800MDM7VF3EDLISC7(05HfpRrrh1Gs1p8bxBbcn2HcGp0n7UvuaMNPCuHS��������������������������������� 12345678910111213141516171819202122# rados -p default.rgw.users.uid listomapkeys ictfox.bucketsbruins# rados -p default.rgw.users.uid listomapvals ictfox.bucketsbruinsvalue (246 bytes) :00000000 07 05 f0 00 00 00 00 00 00 00 0c 00 00 00 00 00 |................|00000010 00 00 2d c7 c8 58 01 00 00 00 00 00 00 00 07 03 |..-..X..........|00000020 c1 00 00 00 06 00 00 00 62 72 75 69 6e 73 18 00 |........bruins..|00000030 00 00 64 65 66 61 75 6c 74 2e 72 67 77 2e 62 75 |..default.rgw.bu|00000040 63 6b 65 74 73 2e 64 61 74 61 2c 00 00 00 30 37 |ckets.data,...07|00000050 30 65 63 31 32 39 2d 32 65 38 34 2d 34 37 66 65 |0ec129-2e84-47fe|00000060 2d 62 33 32 61 2d 62 30 62 66 39 39 31 34 61 35 |-b32a-b0bf9914a5|00000070 31 66 2e 31 34 32 37 34 2e 31 2c 00 00 00 30 37 |1f.14274.1,...07|00000080 30 65 63 31 32 39 2d 32 65 38 34 2d 34 37 66 65 |0ec129-2e84-47fe|00000090 2d 62 33 32 61 2d 62 30 62 66 39 39 31 34 61 35 |-b32a-b0bf9914a5|000000a0 31 66 2e 31 34 32 37 34 2e 31 19 00 00 00 64 65 |1f.14274.1....de|000000b0 66 61 75 6c 74 2e 72 67 77 2e 62 75 63 6b 65 74 |fault.rgw.bucket|000000c0 73 2e 69 6e 64 65 78 1a 00 00 00 64 65 66 61 75 |s.index....defau|000000d0 6c 74 2e 72 67 77 2e 62 75 63 6b 65 74 73 2e 6e |lt.rgw.buckets.n|000000e0 6f 6e 2d 65 63 00 10 00 00 00 00 00 00 01 2d c7 |on-ec.........-.|000000f0 c8 58 1a 7f b4 0b |.X....|000000f6 rgw的user keys信息保存格式每个user在该pool里都有一个以&lt;access_key&gt;的值命名的object 12# rados ls -p default.rgw.users.keysIYZ800MDM7VF3EDLISC7 查看该object的omap和存储数据： 1234# rados -p default.rgw.users.keys listomapvals IYZ800MDM7VF3EDLISC7# rados -p default.rgw.users.keys get IYZ800MDM7VF3EDLISC7 file# cat fileictfox rgw的user bucket原数据信息保存格式格式：.dir.&lt;bucket id&gt; 12# rados -p default.rgw.buckets.index ls.dir.070ec129-2e84-47fe-b32a-b0bf9914a51f.14274.1 查看该object的omap和存储数据： 12345678910111213141516171819202122# rados -p default.rgw.buckets.index get .dir.070ec129-2e84-47fe-b32a-b0bf9914a51f.14274.1 file# cat file## rados -p default.rgw.buckets.index listomapkeys .dir.070ec129-2e84-47fe-b32a-b0bf9914a51f.14274.1hello.txt# rados -p default.rgw.buckets.index listomapvals .dir.070ec129-2e84-47fe-b32a-b0bf9914a51f.14274.1hello.txtvalue (196 bytes) :00000000 08 03 be 00 00 00 09 00 00 00 68 65 6c 6c 6f 2e |..........hello.|00000010 74 78 74 06 00 00 00 00 00 00 00 01 04 03 53 00 |txt...........S.|00000020 00 00 01 0c 00 00 00 00 00 00 00 21 e8 c8 58 42 |...........!..XB|00000030 2f 40 29 20 00 00 00 65 64 30 37 36 32 38 37 35 |/@) ...ed0762875|00000040 33 32 65 38 36 33 36 35 65 38 34 31 65 39 32 62 |32e86365e841e92b|00000050 66 63 35 30 64 38 63 06 00 00 00 69 63 74 66 6f |fc50d8c....ictfo|00000060 78 04 00 00 00 6d 69 6b 65 00 00 00 00 0c 00 00 |x....mike.......|00000070 00 00 00 00 00 00 00 00 00 00 00 00 00 01 01 02 |................|00000080 00 00 00 09 06 0a 2c 00 00 00 30 37 30 65 63 31 |......,...070ec1|00000090 32 39 2d 32 65 38 34 2d 34 37 66 65 2d 62 33 32 |29-2e84-47fe-b32|000000a0 61 2d 62 30 62 66 39 39 31 34 61 35 31 66 2e 31 |a-b0bf9914a51f.1|000000b0 34 32 34 35 2e 35 00 00 00 00 00 00 00 00 00 00 |4245.5..........|000000c0 00 00 00 00 |....|000000c4 rgw的user bucket中对象数据保存格式每个object没有omap信息 1234567# rados -p default.rgw.buckets.data ls070ec129-2e84-47fe-b32a-b0bf9914a51f.14274.1_hello.txt# rados -p default.rgw.buckets.data get 070ec129-2e84-47fe-b32a-b0bf9914a51f.14274.1_hello.txt file# cat fileHello World!# rados -p default.rgw.buckets.data listomapvals 070ec129-2e84-47fe-b32a-b0bf9914a51f.14274.1_hello.txt 查看object的xattr信息 12345678910# rados -p default.rgw.buckets.data listxattr 070ec129-2e84-47fe-b32a-b0bf9914a51f.14274.1_hello.txtuser.rgw.acluser.rgw.cache_controluser.rgw.etaguser.rgw.idtaguser.rgw.manifestuser.rgw.pg_veruser.rgw.source_zone# rados -p default.rgw.buckets.data getxattr 070ec129-2e84-47fe-b32a-b0bf9914a51f.14274.1_hello.txt user.rgw.acl&#123;ictfoxmike]ictfoxictfox2ictfoxmike rgw的user bucket的metadata信息123# rados -p default.rgw.data.root lsbruins.bucket.meta.bruins:070ec129-2e84-47fe-b32a-b0bf9914a51f.14274.1 每个object没有omap信息 12# rados -p default.rgw.data.root listomapvals bruins# rados -p default.rgw.data.root listomapvals .bucket.meta.bruins:070ec129-2e84-47fe-b32a-b0bf9914a51f.14274.1 查看object的存储数据 1234567891011121314151617181920212223242526272829303132333435# rados -p default.rgw.data.root get bruins file# cat file�bruinsdefault.rgw.buckets.data,070ec129-2e84-47fe-b32a-b0bf9914a51f.14274.1,070ec129-2e84-47fe-b32a-b0bf9914a51f.14274.1default.rgw.buckets.indexdefault.rgw.buckets.non-ecictfoxictfox# rados -p default.rgw.data.root get .bucket.meta.bruins:070ec129-2e84-47fe-b32a-b0bf9914a51f.14274.1 file# ceph-dencoder type RGWBucketInfo import file decode dump_json&#123; "bucket": &#123; "name": "bruins", "pool": "default.rgw.buckets.data", "data_extra_pool": "default.rgw.buckets.non-ec", "index_pool": "default.rgw.buckets.index", "marker": "070ec129-2e84-47fe-b32a-b0bf9914a51f.14274.1", "bucket_id": "070ec129-2e84-47fe-b32a-b0bf9914a51f.14274.1", "tenant": "" &#125;, "creation_time": "0.000000", "owner": "ictfox", "flags": 0, "zonegroup": "d40f14ea-4700-43e7-a92e-fa232f28b590", "placement_rule": "default-placement", "has_instance_obj": "true", "quota": &#123; "enabled": false, "max_size_kb": -1, "max_objects": -1 &#125;, "num_shards": 0, "bi_shard_hash_type": 0, "requester_pays": "false", "has_website": "false", "swift_versioning": "false", "swift_ver_location": ""&#125; 查看object的xattr 123456789101112131415161718192021222324252627282930313233343536373839404142# rados -p default.rgw.data.root listxattr bruinsceph.objclass.version# rados -p default.rgw.data.root getxattr bruins ceph.objclass.version$_osgTpZcD-jCFgcwdi3yvHmH# rados -p default.rgw.data.root listxattr .bucket.meta.bruins:070ec129-2e84-47fe-b32a-b0bf9914a51f.14274.1ceph.objclass.versionuser.rgw.acl# rados -p default.rgw.data.root getxattr .bucket.meta.bruins:070ec129-2e84-47fe-b32a-b0bf9914a51f.14274.1 user.rgw.acl &gt; file# ceph-dencoder type RGWAccessControlPolicy import file decode dump_json&#123; "acl": &#123; "acl_user_map": [ &#123; "user": "ictfox", "acl": 15 &#125; ], "acl_group_map": [], "grant_map": [ &#123; "id": "ictfox", "grant": &#123; "type": &#123; "type": 0 &#125;, "id": "ictfox", "email": "", "permission": &#123; "flags": 15 &#125;, "name": "mike", "group": 0 &#125; &#125; ] &#125;, "owner": &#123; "id": "ictfox", "display_name": "mike" &#125;&#125;]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>radosgw</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Cinder的云硬盘动态限速框架设计]]></title>
    <url>%2F2017%2F12%2F05%2Fvolume-dynamic-iotune-arch-design%2F</url>
    <content type="text"><![CDATA[当前状况当前我们仅仅通过cinder创建type，指定type的IO限制来限制不同类型的云硬盘的IO值。 具体方案和调研，参考：云硬盘动态限速调研 目的 充分利用系统的资源，在系统负载和Ceph负载小的情况下，动态调整云硬盘的IO值，给用户提供更好的体验。 设计一个框架，能满足云硬盘动态限速的要求；以后也能方便的加入网络的动态限速； 框架设计需求 统一配置选项 模块化设计，能方便添加别的动态限速需求 统一调度，各物理节点独立运行模式 方案1分散式架构设计，独立实现，各物理节点独立负责监控负载和动态限速； 优点： 独立实现，比较简洁 物理节点单独计算负载，通信量减少 Master只需要维护配置项，可在内存中维护该数据 缺点： 不能与现有监控系统结合 方案2结合Openstack的设计，Master端集中维护系统负载，存入DB，Master-Agent通过Rabbitmq通信； 优点： 结合Openstack的DB，rabbitmq，rpc通信，wsgi等模块 系统负载信息通过DB统一维护 方便与现有监控系统集合 缺点： 结合Openstack的几个通用模块，代码量和工作量比较大 系统负载信息集中保存到DB，当前看是没有必要性 方案选择个人倾向 方案1，不需要把openstack的各个模块引入进来，实现会相对简单，也满足我们项目的需求； 云硬盘限速部分设计概述前面在调研中指出，我们可以在nova instance具体的物理机上，通过virsh的命令调整虚拟机指定云硬盘的io限制。所以具体执行动态限速的命令需要在nova instance的物理机上执行，也就是说每个运行nova instance的物理机需要跑一个限速的agent进程。 我们期望云硬盘动态限速，会受到以下两个因素影响： 物理机的负载 云硬盘对应Ceph Pool的IO负载 所以比较简单的设计为： 每个Agent单独运行，负责本物理机上的nova instances的云硬盘IO动态调整。Agent可以获取本机的实时负载和Ceph对应Pool的IO负载。 考虑的因素 通过配置可以关闭/开启动态限速 支持白名单方式 支持黑名单方式 通过配置指定支持的云硬盘类型 通过配置指定动态限速模式： 物理机负载 + Ceph Pool负载 物理机负载 Ceph Pool负载 配置动态调整限速间隔 配置指定类型云硬盘动态限速的范围: [begin, end] 配置项示例1234567891011121314151617181920212223242526272829303132333435363738[DEFAULT]# Tune dynamic resource limitation to True/Falsedynamic_tune = True/False# interval to check and reset dynamic tune, value in secondsinterval = 60# Tune modules which would do dynamic tune# Keep pace with the below configurationstune_modules = iotune ### io dynamic tune related[iotune]# Which type of volumes supported, default means tune all volume types;# Keep pace with the configuration of cinder type# Example values: default/sata/ssd/sata, ssdvolume_types = default# Model to do tune, value maybe: default/local/ceph# default model: local + ceph# local: only check local load# ceph: only check ceph loadtune_model = default # Set the iotune range for specify volume type to [begin, end]iotune_bw_range = &#123;'sata' : &#123;'read' : [80, 200], 'write' : [50, 100]&#125;, 'ssd' : &#123;'read' : [100, 300], 'write' : [80, 200]&#125;&#125;iotune_iops_range = &#123;'sata' : &#123;'read' : [100, 1000], 'write' : [100, 800]&#125;, 'ssd' : &#123;'read' : [1000, 5000], 'write' : [600, 300]&#125;&#125; # Specify disks tune infoiotune_range = &#123;'d-5pc4hq7y' : &#123;'bw' : &#123;'read' : [80, 200], 'write' : [50, 100]&#125;, 'iops' : &#123;'read' : [100, 1000], 'write' : [100, 800]&#125;&#125;, 'd-ymqagtez' : &#123;'bw' : &#123;'read' : [100, 300], 'write' : [80, 200]&#125;, 'iops' : &#123;'read' : [1000, 5000], 'write' : [600, 300]&#125;&#125; &#125; # Blacklist of nova instances which would not do dynamic iotune#blacklist = nova-ins1, nova-ins2, ...# Whitelist of nova instances which ONLY do dynamic iotune on them#whitelist = nova-ins3, nova-ins4, ... ### specify user tune info[usr-kndz857b]iotune_bw_range = &#123;'sata' : &#123;'read' : [80, 200], 'write' : [50, 100]&#125;, 'ssd' : &#123;'read' : [100, 300], 'write' : [80, 200]&#125;&#125;iotune_iops_range = &#123;'sata' : &#123;'read' : [100, 1000], 'write' : [100, 800]&#125;, 'ssd' : &#123;'read' : [1000, 5000], 'write' : [600, 300]&#125;&#125; 代码中读取上诉配置，在脚本中生成conf的字典信息； 涉及到的命令与解释1)、获取系统cinder volume的信息cinder list --all | grep &quot;in-use&quot; | awk &#39;{print $2,$4,$8,$12}&#39; 12345678(.venv)openstack@Server-01-01:~$ cinder list --all | grep "in-use" | awk '&#123;print $2,$4,$8,$12&#125;'...| ID | Tenant ID | Name | Volume Type |04960b37-7a07-439d-9fd4-dc0664870245 a72514ab47524d00b0b43551466c7d67 d-6a3x2u8j sata06996e93-47ad-4003-acb4-b46aa185d479 f634c4182b634a7ca4a4c2ac72837cb7 d-ibvar57t sata0f45c78c-743b-4d42-b098-b2ee5f95df94 462dc39145324bb1bda7263368d176aa d-f8wkacjy ssd103a0e6d-382c-4caa-b313-63a95eeb351d 616eae54884147d786a3beb39ea6bcbb d-va5fe6bw sata... 2)、找到配置文件中user对应的 Tenant IDopenstack project list | grep &quot;user-id&quot; 12openstack@Server-01-01:~$ openstack user list | grep usr-kndz857b| 63719e972d7d4afe87b7127c9daa120f | usr-kndz857b | 3)、列出本物理机上的virsh instancesvirsh list：list domains 123456789openstack@Server-01-01:~$ virsh list Id Name State---------------------------------------------------- 40 instance-0000028d running 52 instance-0000033d running 122 instance-00000721 running 140 instance-000007a5 running 141 instance-000007a3 running... 4)、列出指定virsh instances的所有blocksvirsh domblklist &lt;domain&gt;：list all domain blocks 12345openstack@Server-01-01:~$ virsh domblklist instance-00000921Target Source------------------------------------------------vda /var/lib/nova/instances/7ef2900b-0653-443b-a2c0-3da7a5d7a10f/diskvdb volumes/volume-7b94ccc5-6b14-4352-b4f3-74a71893f246 5)、查看/修改block device的iotunevirsh blkdeviotune &lt;domain&gt; &lt;device&gt;：Set or query a block device I/O tuning parameters 123456789101112131415openstack@Server-01-01:~$ virsh blkdeviotune instance-00000921 vdbtotal_bytes_sec: 0read_bytes_sec : 104857600write_bytes_sec: 62914560total_iops_sec : 0read_iops_sec : 1500write_iops_sec : 1000total_bytes_sec_max: 0read_bytes_sec_max: 10485760write_bytes_sec_max: 6291456total_iops_sec_max: 0read_iops_sec_max: 150write_iops_sec_max: 100size_iops_sec : 0openstack@Server-01-01:~$ virsh blkdeviotune instance-00000921 vdb --read_iops_sec 5000 --write_iops_sec 3000 Agent框架流程 获取负载信息1)、本机负载信息12$ uptime 14:35:25 up 32 days, 14:54, 5 users, load average: 2.53, 2.82, 2.80 获取最近1,5,15分钟负载分别为：2.53, 2.82, 2.80 2)、Ceph负载信息1234567891011121314151617181920212223242526272829303132333435363738394041424344$ ceph osd treeID WEIGHT TYPE NAME UP/DOWN REWEIGHT PRIMARY-AFFINITY-10 3.41998 root hp-default -7 1.70999 host hp-Server-01-02 33 0.64999 osd.33 up 1.00000 1.00000 32 0.67000 osd.32 up 1.00000 1.00000 31 0.39000 osd.31 up 1.00000 1.00000 -9 1.70999 host hp-Server-02-02 38 0.67000 osd.38 up 1.00000 1.00000 39 0.64999 osd.39 up 1.00000 1.00000 37 0.39000 osd.37 up 1.00000 1.00000 -1 101.91998 root default -2 25.48000 host Server-01-01 6 3.64000 osd.6 up 1.00000 1.00000 5 3.64000 osd.5 up 1.00000 1.00000 4 3.64000 osd.4 up 1.00000 1.00000 3 3.64000 osd.3 up 1.00000 1.00000 2 3.64000 osd.2 up 1.00000 1.00000 1 3.64000 osd.1 up 1.00000 1.00000 0 3.64000 osd.0 up 1.00000 1.00000 -3 25.48000 host Server-01-02 13 3.64000 osd.13 up 1.00000 1.00000 12 3.64000 osd.12 up 1.00000 1.00000 11 3.64000 osd.11 up 1.00000 1.00000 10 3.64000 osd.10 up 1.00000 1.00000 9 3.64000 osd.9 up 1.00000 1.00000 8 3.64000 osd.8 up 1.00000 1.00000 7 3.64000 osd.7 up 1.00000 1.00000 -4 25.48000 host Server-02-01 20 3.64000 osd.20 up 1.00000 1.00000 19 3.64000 osd.19 up 1.00000 1.00000 18 3.64000 osd.18 up 1.00000 1.00000 17 3.64000 osd.17 up 1.00000 1.00000 16 3.64000 osd.16 up 1.00000 1.00000 15 3.64000 osd.15 up 1.00000 1.00000 14 3.64000 osd.14 up 1.00000 1.00000 -5 25.48000 host Server-02-02 27 3.64000 osd.27 up 1.00000 1.00000 26 3.64000 osd.26 up 1.00000 1.00000 25 3.64000 osd.25 up 1.00000 1.00000 24 3.64000 osd.24 up 1.00000 1.00000 23 3.64000 osd.23 up 1.00000 1.00000 22 3.64000 osd.22 up 1.00000 1.00000 21 3.64000 osd.21 up 1.00000 1.00000 从上述输出中获取不同性质磁盘对应的OSD如下信息： 12&#123;"sata" : (0,1,2,3,4,5,6,7,...,26,27)&#125;&#123;"ssd": (31,32,33,37,38,39)&#125; 【注：这个信息是固定的，只需要获取一次即可】 123456789101112131415161718192021222324252627282930313233343536$ ceph osd perfosd fs_commit_latency(ms) fs_apply_latency(ms) 0 0 1 1 0 1 2 0 1 3 0 1 4 0 1 5 0 1 6 0 1 7 2 4 8 0 1 9 0 0 10 0 1 11 0 1 12 0 1 13 0 0 14 0 1 15 0 1 16 0 1 17 0 1 18 0 1 19 0 1 20 0 1 21 0 1 22 0 0 23 0 0 24 0 0 25 0 1 26 0 0 27 0 1 31 1 2 32 0 7 33 0 6 37 0 6 38 0 8 39 0 3 分别获取SATA和SSD磁盘组内的平均负载信息： 1&#123;"sata" : [1, 2], "ssd" : [1, 6]&#125; 【注：这个信息需要周期性获取】 动态调整IO限速公式： 12345678910111213141516171819202122sys_load_throttle = 20cepy_load_throttle1 = 10cepy_load_throttle2 = 20 系统负载，从uptime输出获取，sys_load: [2.53, 2.82, 2.80]ceph负载，从ceph osd perf输出获取，ceph_load: &#123;"sata" : [1, 2], "ssd" : [1, 6]&#125;根据volume_type，获取ceph对应磁盘组的负载信息: cload根据sys_load和cload的值决定增加还是减少bw和iopsif (cload[0] &lt; $cepy_load_throttle1) &amp;&amp; (cload[1] &lt; $cepy_load_throttle1): if (sys_load[0] &lt; $sys_load_throttle) &amp;&amp; (sys_load[1] &lt; $sys_load_throttle): #增大 else #保持不变elif ($cepy_load_throttle2 &gt;= cload[0] &gt;= $cepy_load_throttle1) &amp;&amp; ($cepy_load_throttle2 &gt;= cload[1] &gt;= $cepy_load_throttle1): if (sys_load[0] &gt; $sys_load_throttle) || (sys_load[1] &gt; $sys_load_throttle): #减少 else #保持不变else #减少根据ceph和sys的负载信息，可以计算出SATA/SSD磁盘的iotune是增加，减少，还是保持不变； loop每个instance，针对volume类型是需要增加/减少的volume，设置volume新的iotune值： 1234567891011121314151617181920根据conf配置信息获取volume对应的bw_range和iops_range，然后调用下面的函数执行iotune调整。 set_volume_dynamic_iotune()输入：volume target - 卷的盘符，示例：vdb bw_range - 带宽范围，从配置文件获取，示例：&#123;'read' : [80, 200], 'write' : [50, 100]&#125; #单位：MB iops_range - iops范围，从配置文件获取，示例：&#123;'read' : [100, 300], 'write' : [80, 200]&#125; iotune - iotune的趋势，0: 保持不变；&gt;0: 增加；&lt;0: 减少 1、获取volume现在对应的bw和iops值2、根据输入计算volume新的bw和iops值3、判断新的bw和iops值是不是在bw_range, iops_range范围内？是的话设置对应值为新值，否则保留旧值；4、通过virsh blkdeviotune命令设置read_bytes_sec，write_bytes_sec，read_iops_sec，write_iops_sec 注释：read_bytes_sec - 每秒读bytes write_bytes_sec - 每秒写bytes read_iops_sec - 每秒读iops write_iops_sec - 每秒写iops bw: 增大减少幅度10MB (10485760 bytes)iops: 增大减少幅度100 云硬盘默认限速在动态限速程序退出前，需要调整相关云硬盘到默认限速值，具体的默认值如下： 12345678(.venv)openstack@Server-01-01:~$ cinder qos-list+--------------------------------------+--------------+-----------+----------------------------------------------------------------------------------------------------------------------------+| ID | Name | Consumer | specs |+--------------------------------------+--------------+-----------+----------------------------------------------------------------------------------------------------------------------------+| 06661a25-27d4-4f64-93d4-8b74e3fda218 | sata-qos | front-end | &#123;u'read_bytes_sec': u'104857600', u'write_iops_sec': u'1000', u'write_bytes_sec': u'62914560', u'read_iops_sec': u'1500'&#125; |...| d560db69-e8f3-4a5f-a80c-fff41140f48c | ssd-qos | front-end | &#123;u'read_bytes_sec': u'157286400', u'write_iops_sec': u'3000', u'write_bytes_sec': u'104857600', u'read_iops_sec': u'5000'&#125; |+--------------------------------------+--------------+-----------+----------------------------------------------------------------------------------------------------------------------------+]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>nova</tag>
        <tag>openstack</tag>
        <tag>cinder</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Cinder的云硬盘动态限速调研]]></title>
    <url>%2F2017%2F11%2F28%2Fvolume-dynamic-iotune-invest%2F</url>
    <content type="text"><![CDATA[最近忙存储网关的开发，没有投放精力在Ceph和Openstack上了，这里翻出来一些之前做的项目，分享给大家。 一、QOS概况在OpenStack H版中块存储引入了QoS特性，主要在Cinder和Nova项目中实施。 在OpenStack的QoS特性上，主要依赖于后段存储和前端的Hypervisor来实现，而在Cinder中，提供了一个QoS Spec框架，用户可以创建一个QoS Spec，这个Spec说明了针对目标(后端或者前端)，限制键值对(total_iops_sec=1000)等，每个QoS Spec与Volume Type相联系，用户在创建一个卷时可以将该卷与一个Volume Type联系，这样就间接使得该卷与特定QoS Spec联系。 换句话说，该卷获得了一系列QoS键值对，当该QoS是面向后端时，创建卷命令会将QoS Spec的键值对传给Cinder的后端存储解释。当该QoS面向前端时，在这个卷被附加到一个虚拟机上时才会被实现，比如通过QEMU来实现。 二、当前实现硬盘：Ceph的RBD提供 当前限速实现：通过Openstack Cinder创建type，指定QOS来实现。具体步骤如下： 1）创建cinder volume type12usage: cinder type-create &lt;name&gt;例如：cinder type-create sata 2）创建cinder的一个qos标准12usage: cinder qos-create &lt;name&gt; &lt;key=value&gt; [&lt;key=value&gt; ...]例如： cinder qos-create sata-qos consumer="front-end" read_iops_sec=300 write_iops_sec=300 read_bytes_sec=104857600 write_bytes_sec=62914560 [注释]consumer：通常分为两类 front-end：表示限速在前端hypervisor（例如Qemu）实现 back-end：表示限速在后端存储系统实现 Libvirt/Qemu可配置qos keys： total_bytes_sec: the total allowed bandwidth for the guest per second read_bytes_sec: sequential read limitation write_bytes_sec: sequential write limitation total_iops_sec: the total allowed IOPS for the guest per second read_iops_sec: random read limitation write_iops_sec: random write limitation 后端可配置qos keys： 与具体的后端存储支持相关，当前Ceph不支持块设备的QOS配置，在Ceph社区已经提上日程。 3）管理cinder volume type和qos123456789usage: cinder qos-associate &lt;qos_specs&gt; &lt;volume_type_id&gt;例如：cinder qos-associate 2b3b35b8-e19c-442d-9218-8b72b369f0e4 c224099f-2a8e-4356-ac2a-78fe56ec9523 $ cinder qos-get-association 2b3b35b8-e19c-442d-9218-8b72b369f0e4+------------------+------+--------------------------------------+| Association_Type | Name | ID |+------------------+------+--------------------------------------+| volume_type | sata | c224099f-2a8e-4356-ac2a-78fe56ec9523 |+------------------+------+--------------------------------------+ 4）cinder volume attach到nova instanceattach后可以查看到这些iotune信息 12345678910111213141516171819202122232425$ virsh dumpxml cde0ab13-ff3d-46d0-a7c6-b797f18d0465... &lt;disk type='network' device='disk'&gt; &lt;driver name='qemu' type='raw' cache='none'/&gt; &lt;auth username='volumes'&gt; &lt;secret type='ceph' uuid='c22ced15-b23b-4b31-8329-e14a59273b81'/&gt; &lt;/auth&gt; &lt;source protocol='rbd' name='volumes_2/volume-f9daf542-4ca7-4228-b6b4-d450c7c7dbb4'&gt; &lt;host name='172.16.0.6' port='6789'/&gt; &lt;host name='172.16.0.7' port='6789'/&gt; &lt;host name='172.16.0.17' port='6789'/&gt; &lt;/source&gt; &lt;backingStore/&gt; &lt;target dev='vdb' bus='virtio'/&gt; &lt;iotune&gt; &lt;read_bytes_sec&gt;104857600&lt;/read_bytes_sec&gt; &lt;write_bytes_sec&gt;62914560&lt;/write_bytes_sec&gt; &lt;read_iops_sec&gt;300&lt;/read_iops_sec&gt; &lt;write_iops_sec&gt;300&lt;/write_iops_sec&gt; &lt;/iotune&gt; &lt;serial&gt;f9daf542-4ca7-4228-b6b4-d450c7c7dbb4&lt;/serial&gt; &lt;alias name='virtio-disk1'/&gt; &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/&gt; &lt;/disk&gt;... 三、如何动态限速磁盘？1）根据磁盘容量动态调整难度一般 从上诉分析知道cinder volume的QOS设置在hypervisor Qemu中实现，从cinder中读取这些参数应该在Nova的逻辑中有，我们只需要修改这部分逻辑，添加代码实现随着磁盘容量递增的QOS即可。 2）根据系统负载动态调整难度较大 动态调整虚拟机iotune的方法： 1. nova的change-disk-io-tune命令nova的命令中有一个change-disk-io-tune的子命令，格式如下： 123456789101112131415161718$ nova help change-disk-io-tuneusage: nova change-disk-io-tune [--read_bytes_sec &lt;read_bytes_sec&gt;] [--write_bytes_sec &lt;write_bytes_sec&gt;] [--read_iops_sec &lt;read_iops_sec&gt;] [--write_iops_sec &lt;write_iops_sec&gt;] &lt;server&gt;change the root user password for a serverPositional arguments: &lt;server&gt; Name or ID of serverOptional arguments: --read_bytes_sec &lt;read_bytes_sec&gt; read_bytes_sec bytes/s. --write_bytes_sec &lt;write_bytes_sec&gt; write_bytes_sec bytes/s. --read_iops_sec &lt;read_iops_sec&gt; read_iops_sec. --write_iops_sec &lt;write_iops_sec&gt; write_iops_sec. 通过该命令可以动态修改nova instance中的disk iotune限制，但它不能区分nova instance中的具体硬盘，会把所有盘的iotune都修改为指定值，例如： 123456789101112131415161718192021222324252627$ nova change-disk-io-tune --read_iops_sec 200 --write_iops_sec 200 --read_bytes_sec 102400 --write_bytes_sec 102400 5ceaacab-b021-4f23-ba45-579345b843a9$ virsh dumpxml 5ceaacab-b021-4f23-ba45-579345b843a9 &gt; withdisk.xml$ vim withdisk.xml... &lt;target dev='vda' bus='virtio'/&gt; &lt;iotune&gt; &lt;read_bytes_sec&gt;102400&lt;/read_bytes_sec&gt; &lt;write_bytes_sec&gt;102400&lt;/write_bytes_sec&gt; &lt;read_iops_sec&gt;200&lt;/read_iops_sec&gt; &lt;write_iops_sec&gt;200&lt;/write_iops_sec&gt; &lt;/iotune&gt;... &lt;target dev='vdb' bus='virtio'/&gt; &lt;iotune&gt; &lt;read_bytes_sec&gt;102400&lt;/read_bytes_sec&gt; &lt;write_bytes_sec&gt;102400&lt;/write_bytes_sec&gt; &lt;read_iops_sec&gt;200&lt;/read_iops_sec&gt; &lt;write_iops_sec&gt;200&lt;/write_iops_sec&gt; &lt;/iotune&gt;... &lt;target dev='vdc' bus='virtio'/&gt; &lt;iotune&gt; &lt;read_bytes_sec&gt;102400&lt;/read_bytes_sec&gt; &lt;write_bytes_sec&gt;102400&lt;/write_bytes_sec&gt; &lt;read_iops_sec&gt;200&lt;/read_iops_sec&gt; &lt;write_iops_sec&gt;200&lt;/write_iops_sec&gt; &lt;/iotune&gt; 通过nova change-disk-io-tune 可以实时调整虚拟机上所有blk device的iotune，并立即生效。但它不能指定具体的blk device，不符合我们的使用需求。 2. virsh的blkdeviotune命令virsh命令有disk iotune相关的子命令，如下： 1234567891011121314151617181920212223242526$ virsh blkdeviotune --help NAME blkdeviotune - Set or query a block device I/O tuning parameters. SYNOPSIS blkdeviotune &lt;domain&gt; &lt;device&gt; [--total-bytes-sec &lt;number&gt;] [--read-bytes-sec &lt;number&gt;] [--write-bytes-sec &lt;number&gt;] [--total-iops-sec &lt;number&gt;] [--read-iops-sec &lt;number&gt;] [--write-iops-sec &lt;number&gt;] [--total-bytes-sec-max &lt;number&gt;] [--read-bytes-sec-max &lt;number&gt;] [--write-bytes-sec-max &lt;number&gt;] [--total-iops-sec-max &lt;number&gt;] [--read-iops-sec-max &lt;number&gt;] [--write-iops-sec-max &lt;number&gt;] [--size-iops-sec &lt;number&gt;] [--config] [--live] [--current] DESCRIPTION Set or query disk I/O parameters such as block throttling. OPTIONS [--domain] &lt;string&gt; domain name, id or uuid [--device] &lt;string&gt; block device --total-bytes-sec &lt;number&gt; total throughput limit in bytes per second --read-bytes-sec &lt;number&gt; read throughput limit in bytes per second --write-bytes-sec &lt;number&gt; write throughput limit in bytes per second --total-iops-sec &lt;number&gt; total I/O operations limit per second --read-iops-sec &lt;number&gt; read I/O operations limit per second --write-iops-sec &lt;number&gt; write I/O operations limit per second --total-bytes-sec-max &lt;number&gt; total max in bytes --read-bytes-sec-max &lt;number&gt; read max in bytes --write-bytes-sec-max &lt;number&gt; write max in bytes --total-iops-sec-max &lt;number&gt; total I/O operations max --read-iops-sec-max &lt;number&gt; read I/O operations max --write-iops-sec-max &lt;number&gt; write I/O operations max --size-iops-sec &lt;number&gt; I/O size in bytes --config affect next boot --live affect running domain --current affect current domain 通过该命令可以实时调整对应block device的iotune，但必须在虚拟机对应的物理机上执行该命令。 查看指定设备的iotune，命令如下： 1234567891011121314$ virsh blkdeviotune 5ceaacab-b021-4f23-ba45-579345b843a9 vdctotal_bytes_sec: 0read_bytes_sec : 104857600write_bytes_sec: 62914560total_iops_sec : 0read_iops_sec : 1500write_iops_sec : 1000total_bytes_sec_max: 0read_bytes_sec_max: 10485760write_bytes_sec_max: 6291456total_iops_sec_max: 0read_iops_sec_max: 150write_iops_sec_max: 100size_iops_sec : 0 修改指定设备的iotune值，命令如下： 123456789101112131415$ virsh blkdeviotune 5ceaacab-b021-4f23-ba45-579345b843a9 vdc --read_iops_sec 5000 --write_iops_sec 3000$ virsh blkdeviotune 5ceaacab-b021-4f23-ba45-579345b843a9 vdctotal_bytes_sec: 0read_bytes_sec : 104857600write_bytes_sec: 62914560total_iops_sec : 0read_iops_sec : 5000write_iops_sec : 3000total_bytes_sec_max: 0read_bytes_sec_max: 10485760write_bytes_sec_max: 6291456total_iops_sec_max: 0read_iops_sec_max: 500write_iops_sec_max: 300size_iops_sec : 0 通过virsh可以实时调整虚拟机指定blk device的iotune，并立即生效。但如何结合需求动态调整虚拟机上磁盘的iotune，还需深入考察。]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>nova</tag>
        <tag>openstack</tag>
        <tag>cinder</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 软链接和硬链接]]></title>
    <url>%2F2017%2F11%2F16%2Flinx-symbolic-hard-link%2F</url>
    <content type="text"><![CDATA[概述Linux系统中为了解决文件共享问题，引入了软链接和硬链接的概念，软链接又称符号链接，即soft link或symbolic link，硬链接即为hard link。同时它们还带来了隐藏文件路径、增加权限安全及节省存储等好处。 若一个文件指向另一个文件，该文件的内容仅仅是另一个文件的path，则其为软链接。若一个inode号对应多个文件名，则称这些文件为硬链接。换言之，硬链接就是同一个文件使用了多个别名。软链接可由命令ln创建，硬链接可由命令link或ln创建。 硬链接由于硬链接的文件是有着相同inode号，仅文件名不同，因此硬链接存在以下几点特性： 文件有相同的inode 和 data blocks 只能对文件创建硬链接，不能对目录创建硬链接 只能对已存在的文件创建硬链接 不能跨文件系统创建硬链接 删除一个硬链接文件并不影响其他有相同inode号的文件 12345678910111213141516171819202122232425262728$ lltotal 4-rw-r--r-- 1 root root 928 Nov 11 08:37 tstfile$ ln tstfile hlink$ lltotal 8-rw-r--r-- 2 root root 928 Nov 11 08:37 hlink-rw-r--r-- 2 root root 928 Nov 11 08:37 tstfile$ stat tstfile File: ‘tstfile’ Size: 928 Blocks: 8 IO Block: 4096 regular fileDevice: 803h/2051d Inode: 4851726 Links: 2Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root)Access: 2017-11-11 08:37:07.554081068 +0800Modify: 2017-11-11 08:37:07.554081068 +0800Change: 2017-11-11 08:37:42.216988088 +0800 Birth: -$ stat hlink File: ‘hlink’ Size: 928 Blocks: 8 IO Block: 4096 regular fileDevice: 803h/2051d Inode: 4851726 Links: 2Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root)Access: 2017-11-11 08:37:07.554081068 +0800Modify: 2017-11-11 08:37:07.554081068 +0800Change: 2017-11-11 08:37:42.216988088 +0800 Birth: - 因为硬链接文件具有相同的地位，并没有谁指向谁之说，所以想要查找一个inode号对应的所有硬链接文件并不容易，通常需要遍历目录下所有的文件，系统里可通过find命令查找一个文件相关的所有硬链接文件。 1234567891011121314$ mkdir dir$ ln tstfile dir/hlink$ find ./ -samefile tstfile./dir/hlink./hlink./tstfile$ ls -i tstfile4851726 tstfile$ find ./ -inum 4851726./dir/hlink./hlink./tstfile 可以通过命令strace find ./ -samefile tstfile查看具体执行的系统调用，发现该命令会在输入的目录下查询所有的文件，输出与参数文件inode num一致的文件，所以在目录里文件非常多时，该命令会非常耗时的。 软连接软链接与硬链接不同，若一个文件的数据块中存放的内容是另一文件的路径名时，则该文件就是软链接。软链接就是一个普通文件，只是数据块内容有点特殊。软链接有着自己的inode号以及数据块，因此软链接的创建与使用没有类似硬链接的诸多限制。 软链接的特性如下： 软链接有自己的文件属性及权限等 可对不存在的文件或目录创建软链接 软链接可跨文件系统创建 软链接即可对文件创建，也可对目录创建 创建软链接后，链接计数i_nlink不会增加 删除软链接并不影响被指向的文件 若被指向的原文件被删除，则相关软连接被称为死链接(dangling link)，若被指向路径文件被重新创建，即恢复为正常的软链接 12345678910111213141516171819202122232425262728$ ln -s tstfile slink$ lltotal 4lrwxrwxrwx 1 root root 7 Nov 11 08:39 slink -&gt; tstfile-rw-r--r-- 1 root root 928 Nov 11 08:37 tstfile$ stat tstfile File: ‘tstfile’ Size: 928 Blocks: 8 IO Block: 4096 regular fileDevice: 803h/2051d Inode: 4851726 Links: 1Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root)Access: 2017-11-11 08:37:07.554081068 +0800Modify: 2017-11-11 08:37:07.554081068 +0800Change: 2017-11-11 08:39:04.749766771 +0800 Birth: - $ stat slink File: ‘slink’ -&gt; ‘tstfile’ Size: 7 Blocks: 0 IO Block: 4096 symbolic linkDevice: 803h/2051d Inode: 4851778 Links: 1Access: (0777/lrwxrwxrwx) Uid: ( 0/ root) Gid: ( 0/ root)Access: 2017-11-11 08:39:24.730713312 +0800Modify: 2017-11-11 08:39:22.901718205 +0800Change: 2017-11-11 08:39:22.901718205 +0800 Birth: -$ readlink slinktstfile 测试程序Linux里由命令stat来获取一个文件的状态，文件状态保持在数据结构struct stat里，定义如下： 123456789101112131415struct stat &#123; dev_t st_dev; /* ID of device containing file */ ino_t st_ino; /* inode number */ mode_t st_mode; /* protection */ nlink_t st_nlink; /* number of hard links */ uid_t st_uid; /* user ID of owner */ gid_t st_gid; /* group ID of owner */ dev_t st_rdev; /* device ID (if special file) */ off_t st_size; /* total size, in bytes */ blksize_t st_blksize; /* blocksize for file system I/O */ blkcnt_t st_blocks; /* number of 512B blocks allocated */ time_t st_atime; /* time of last access */ time_t st_mtime; /* time of last modification */ time_t st_ctime; /* time of last status change */&#125;; 其中st_mode变量用来表示文件类型的，特征位的定义如下： 1234567891011121314151617181920212223S_IFMT 0170000 bit mask for the file type bit fieldsS_IFSOCK 0140000 socketS_IFLNK 0120000 symbolic linkS_IFREG 0100000 regular fileS_IFBLK 0060000 block deviceS_IFDIR 0040000 directoryS_IFCHR 0020000 character deviceS_IFIFO 0010000 FIFOS_ISUID 0004000 set-user-ID bitS_ISGID 0002000 set-group-ID bit (see below)S_ISVTX 0001000 sticky bit (see below)S_IRWXU 00700 mask for file owner permissionsS_IRUSR 00400 owner has read permissionS_IWUSR 00200 owner has write permissionS_IXUSR 00100 owner has execute permissionS_IRWXG 00070 mask for group permissionsS_IRGRP 00040 group has read permissionS_IWGRP 00020 group has write permissionS_IXGRP 00010 group has execute permissionS_IRWXO 00007 mask for permissions for others (not in group)S_IROTH 00004 others have read permissionS_IWOTH 00002 others have write permissionS_IXOTH 00001 others have execute permission POSIX标准里定义了如下的宏来检查文件类型： 1234567S_ISREG(m) is it a regular file?S_ISDIR(m) directory?S_ISCHR(m) character device?S_ISBLK(m) block device?S_ISFIFO(m) FIFO (named pipe)?S_ISLNK(m) symbolic link? (Not in POSIX.1-1996.)S_ISSOCK(m) socket? (Not in POSIX.1-1996.) 结合上面的描述，我们可以写如下测试代码： 12345678910111213141516171819202122232425262728293031323334353637$ cat file_stat.cc#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;sys/stat.h&gt;#include &lt;unistd.h&gt;using namespace std;int main(int argc, char* argv[])&#123; struct stat fstat; string file_path = argv[1]; # use lstat instead of stat as lstat handle symbolic link itself # when input path is a symbolic, not the file it refers to. int ret = lstat(file_path.c_str(), &amp;fstat); cout &lt;&lt; "file path: " &lt;&lt; file_path &lt;&lt; endl; cout &lt;&lt; "file size: " &lt;&lt; fstat.st_size &lt;&lt; endl; cout &lt;&lt; "file ino: " &lt;&lt; fstat.st_ino &lt;&lt; endl; cout &lt;&lt; "file mode: " &lt;&lt; fstat.st_mode &lt;&lt; endl; cout &lt;&lt; "file nlink: " &lt;&lt; fstat.st_nlink &lt;&lt; endl; if (S_ISREG(fstat.st_mode)) &#123; cout &lt;&lt; "regular file: " &lt;&lt; file_path &lt;&lt; endl; &#125; else if (S_ISLNK(fstat.st_mode)) &#123; cout &lt;&lt; "link file: " &lt;&lt; file_path &lt;&lt; endl; char buf[256]; ret = readlink(file_path.c_str(), buf, 255); buf[ret] = '\0'; cout &lt;&lt; "readlink return " &lt;&lt; ret &lt;&lt; endl; cout &lt;&lt; "link file real path: " &lt;&lt; buf &lt;&lt; endl; &#125; return 0;&#125;//g++ -std=c++11 -ofile_stat file_stat.cc 命令行输出 1234567891011121314151617$ ./file_stat hlinkfile path: hlinkfile size: 928file ino: 4851726file mode: 33188file nlink: 3regular file: hlink$ ./file_stat slinkfile path: slinkfile size: 7file ino: 4851778file mode: 41471file nlink: 1link file: slinkreadlink return 7link file real path: tstfile 参考https://www.ibm.com/developerworks/cn/linux/l-cn-hardandsymb-links/index.htmlhttp://www.giannistsakiris.com/2011/04/15/counting-and-listing-hard-links-on-linux/]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[aws sdk cpp中S3相关的应用和示例]]></title>
    <url>%2F2017%2F11%2F06%2Faws-sdk-cpp-s3-introduction%2F</url>
    <content type="text"><![CDATA[概述aws提供多种sdk去访问S3，包括java、go、php、js、ruby、net、c++等，本篇文章结合作者最近应用的实践，介绍aws sdk cpp中访问S3的使用，示例中包括对S3的基本put，get等操作。 在使用aws sdk cpp中，发现它还不是那么完善，很多对S3的操作都找不到示例，并且github上这部分的源码更新很快，估计也是在持续开发和完善中，建议读者在自己应用时，下载最新的sdk版本后测试。 安装测试系统：CentOS Linux release 7.2.1511 下载github上的sdk源码 1# git clone https://github.com/aws/aws-sdk-cpp 或者直接下载zip压缩包后解压缩 安装依赖包 12345yum install -y gcc-c++yum install -y cmakeyum install -y zlib-develyum install -y openssl-develyum install -y curl-devel Ubuntu系统上： 123456apt-get install -y g++apt-get install -y cmakeapt-get install -y libssl-devapt-get install -y libcurl4-openssl-devapt-get install -y uuid-devapt-get install -y libboost-all-dev 安装aws cpp sdk 123456789mkdir build_dircd build_dircmake -DCMAKE_BUILD_TYPE=Release ../aws-sdk-cpp# just make and install core and s3make -j `nproc` -C aws-cpp-sdk-coremake -j `nproc` -C aws-cpp-sdk-s3make install -C aws-cpp-sdk-coremake install -C aws-cpp-sdk-s3 示例基本代码要使用aws c++ sdk，必须包含如下基本代码 1234567891011#include &lt;aws/core/Aws.h&gt;int main(int argc, char** argv)&#123; Aws::SDKOptions options; Aws::InitAPI(options); &#123; // make your SDK calls here. &#125; Aws::ShutdownAPI(options); return 0;&#125; 使用aws s3 client来访问对象存储 12345678910Aws::Client::ClientConfiguration cfg;cfg.endpointOverride = "your s3 endpoint";cfg.scheme = Aws::Http::Scheme::HTTP;cfg.connectTimeoutMs = 100000;cfg.requestTimeoutMs = 100000;Aws::Auth::AWSCredentials cred("your access key", "your secret key");S3Client client(cred, cfg, false, false);// Then you can use this client object to access s3 object 示例1 - list bucket1234567891011121314151617181920212223242526272829303132333435363738cat aws-s3-bucket.cpp#include &lt;iostream&gt;#include &lt;aws/s3/S3Client.h&gt;#include &lt;aws/core/Aws.h&gt;#include &lt;aws/core/auth/AWSCredentialsProvider.h&gt;using namespace Aws::S3;using namespace Aws::S3::Model;using namespace std;int main(int argc, char* argv[])&#123; Aws::SDKOptions options; Aws::InitAPI(options); Aws::Client::ClientConfiguration cfg; cfg.endpointOverride = "..."; cfg.scheme = Aws::Http::Scheme::HTTP; Aws::Auth::AWSCredentials cred("...", "..."); S3Client client(cred, cfg, false, false); auto response = client.ListBuckets(); if (response.IsSuccess()) &#123; auto buckets = response.GetResult().GetBuckets(); for (auto iter = buckets.begin(); iter != buckets.end(); ++iter) &#123; cout &lt;&lt; iter-&gt;GetName() &lt;&lt; "\t" &lt;&lt; iter-&gt;GetCreationDate().ToLocalTimeString(Aws::Utils::DateFormat::ISO_8601) &lt;&lt; endl; &#125; &#125; else &#123; cout &lt;&lt; "Error while ListBuckets " &lt;&lt; response.GetError().GetExceptionName() &lt;&lt; " " &lt;&lt; response.GetError().GetMessage() &lt;&lt; endl; &#125; Aws::ShutdownAPI(options); return 0;&#125; 示例2 - get object123456789101112131415161718192021222324252627282930313233/* * Get object range content to local file */int S3Storage::get_object_to_file(const S3Client &amp;client, const Aws::String bucket, const Aws::String object, const string &amp;file_name, const uint64_t offset, uint64_t length)&#123;... GetObjectRequest request; request.WithBucket(bucket).WithKey(object); string range = S3_RANGE_STRING(offset, length); request.SetRange(range.c_str()); auto outcome = client.GetObject(request); if(!outcome.IsSuccess()) &#123; cout &lt;&lt; "GetObject error: " &lt;&lt; outcome.GetError().GetExceptionName() &lt;&lt; " - " &lt;&lt; outcome.GetError().GetMessage() &lt;&lt; endl; return -1; &#125; Aws::OFStream local_file; local_file.open(file_name.c_str(), std::ios::in | std::ios::out | std::ios::binary); assert(local_file.good()); local_file.seekp(offset, std::ios::beg); local_file &lt;&lt; outcome.GetResult().GetBody().rdbuf(); local_file.close();...&#125; 示例2 - put object12345678910111213141516171819202122232425262728/* * Put whole file to s3 object */int S3Storage::put_object_from_file(const S3Client &amp;client, const Aws::String bucket, const Aws::String object, const string &amp;file_name)&#123;... PutObjectRequest request; request.WithKey(object).WithBucket(bucket); auto input_data = Aws::MakeShared&lt;Aws::FStream&gt;("PutObjectInputStream", file_name.c_str(), std::ios_base::in | std::ios_base::binary); //set the stream that will be put to s3 request.SetBody(input_data); auto outcome = client.PutObject(request); if(outcome.IsSuccess()) &#123; cout &lt;&lt; "Put object succeeded!" &lt;&lt; endl; &#125; else &#123; cout &lt;&lt; "PutObject error: " &lt;&lt; outcome.GetError().GetExceptionName() &lt;&lt; " - " &lt;&lt; outcome.GetError().GetMessage() &lt;&lt; endl; &#125;...&#125; 因有需要读取文件的一部分内容，然后上传到S3上的一个object，查询了好多资料没找到示例，最后翻阅aws cpp sdk的源码，参考相关的实现，花费了很多时间才试验出来，贴给有相同需要的朋友。。。 123456789101112131415161718192021222324252627282930313233343536373839404142/* * Put one file slice to s3 object */ int S3Storage::put_object_from_file_slice(const S3Client &amp;client, const Aws::String bucket, const Aws::String object, const string &amp;file_name, const uint64_t offset, const uint64_t length)&#123;... // Upload file with slices to s3 paralleled PutObjectRequest request; request.WithKey(object).WithBucket(bucket); Aws::FStream local_file; local_file.open(file_name.c_str(), std::ios::in | std::ios::binary); assert(local_file.good()); Array&lt;uint8_t&gt; file_contents(length); local_file.seekg(offset, std::ios::beg); cout &lt;&lt; "file read offset " &lt;&lt; local_file.tellg() &lt;&lt; endl; local_file.read((char*)file_contents.GetUnderlyingData(), file_contents.GetLength()); local_file.close(); //set the stream that will be put to s3 auto sbuff = Aws::New&lt;Aws::Utils::Stream::PreallocatedStreamBuf&gt;(CLASS_TAG, &amp;file_contents, file_contents.GetLength()); auto sbody = Aws::MakeShared&lt;Aws::IOStream&gt;("whatever string", sbuff); request.SetBody(sbody); auto outcome = client.PutObject(request); if(outcome.IsSuccess()) &#123; cout &lt;&lt; "Put object succeeded!" &lt;&lt; endl; &#125; else &#123; cout &lt;&lt; "PutObject error: " &lt;&lt; outcome.GetError().GetExceptionName() &lt;&lt; " - " &lt;&lt; outcome.GetError().GetMessage() &lt;&lt; endl; &#125;...&#125; 参考http://docs.aws.amazon.com/zh_cn/sdk-for-cpp/v1/developer-guide/welcome.htmlhttps://github.com/aws/aws-sdk-cpphttp://alientechlab.com/aws-sdk-cpp-part-1/http://www.cnblogs.com/qiuyi21/p/7239129.html]]></content>
      <categories>
        <category>s3</category>
      </categories>
      <tags>
        <tag>s3 c++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用c++ fstream时遇到的一个问题与分析]]></title>
    <url>%2F2017%2F10%2F25%2Fone-issue-about-c%2B%2B-fstream%2F</url>
    <content type="text"><![CDATA[概述在用c++的fstream时遇到一个很奇怪的问题，记录和分析过程如下。 需求两个文件，一个122k（file-bak），一个100k(file)，file是file-bak的前100k的内容，需要从file-bak里读取后面的22k数据，写入file的后面，然后对比file和file-bak，数据应该一致。 c++实现实现1打开ofile后调用：ofile.seekp(102400, ios::beg) 12345678910111213141516171819202122#include &lt;fstream&gt;#include &lt;iostream&gt;using namespace std;int main(int argc, char* argv[])&#123; char buffer[22168]; ifstream ifile("file2-bak", std::ios_base::in | std::ios_base::binary); ifile.seekg(102400); ifile.read(buffer, 22168); ifile.close(); ofstream ofile; ofile.open("file2", std::ios_base::out | std::ios_base::binary); ofile.seekp(102400, ios::beg); // seek to offset 102400 from begin cout &lt;&lt; ofile.tellp() &lt;&lt; endl; ofile.write(buffer, 22168); ofile.close(); return 0;&#125; 编译运行如下： 1234567891011# g++ -std=c++11 -o fstream_tst fstream_tst.cc# ll -h file*-rw-r--r-- 1 root root 100K Oct 18 17:53 file-rw-r--r-- 1 root root 122K Oct 18 15:18 file-bak# ./fstream_tst102400# ll -h file*-rw-r--r-- 1 root root 100K Oct 18 17:56 file-rw-r--r-- 1 root root 122K Oct 18 15:18 file-bak# diff file1 file-bakBinary files file and file-bak differ 结果异常 实现2打开ofile后调用：ofile.seekp(0, ios::end) 12345678910111213141516171819202122#include &lt;fstream&gt;#include &lt;iostream&gt;using namespace std;int main(int argc, char* argv[])&#123; char buffer[22168]; ifstream ifile("file2-bak", std::ios_base::in | std::ios_base::binary); ifile.seekg(102400); ifile.read(buffer, 22168); ifile.close(); ofstream ofile; ofile.open("file2", std::ios_base::out | std::ios_base::binary); ofile.seekp(0, ios::end); // seek to the end of the file cout &lt;&lt; ofile.tellp() &lt;&lt; endl; ofile.write(buffer, 22168); ofile.close(); return 0;&#125; 编译运行如下： 1234567891011# g++ -std=c++11 -o fstream_tst fstream_tst.cc# ll -h file*-rw-r--r-- 1 root root 100K Oct 18 18:01 file-rw-r--r-- 1 root root 122K Oct 18 15:18 file-bak# ./fstream_tst0# ll -h file*-rw-r--r-- 1 root root 22K Oct 18 18:05 file-rw-r--r-- 1 root root 122K Oct 18 15:18 file-bak# diff file1 file-bakBinary files file and file-bak differ 结果异常 很奇怪为啥 ofile.seekp(0, ios::end); 后 ofile.tellp() 的输出为0 实现3打开ofile时指定：std::ios_base::out | std::ios_base::binary | std::ios_base::app 123456789101112131415161718192021#include &lt;fstream&gt;#include &lt;iostream&gt;using namespace std;int main(int argc, char* argv[])&#123; char buffer[22168]; ifstream ifile("file2-bak", std::ios_base::in | std::ios_base::binary); ifile.seekg(102400); ifile.read(buffer, 22168); ifile.close(); ofstream ofile; ofile.open("file2", std::ios_base::out | std::ios_base::binary | std::ios_base::app); cout &lt;&lt; ofile.tellp() &lt;&lt; endl; ofile.write(buffer, 22168); ofile.close(); return 0;&#125; 编译运行如下： 1234567891011# g++ -std=c++11 -o fstream_tst fstream_tst.cc# ll -h file*-rw-r--r-- 1 root root 100K Oct 18 18:12 file-rw-r--r-- 1 root root 122K Oct 18 15:18 file-bak# ./fstream_tst102400# ll -h file*-rw-r--r-- 1 root root 122K Oct 18 18:15 file-rw-r--r-- 1 root root 122K Oct 18 15:18 file-bak# diff file1 file-bak# 结果正常 c实现代码如下： 123456789101112131415161718#include &lt;stdio.h&gt;int main(int argc, char* argv[])&#123; char buffer[22168]; FILE *fpr = fopen("file-bak", "rb"); fseek(fpr, 102400, SEEK_SET); fread(buffer, sizeof(char), 22168, fpr); fclose(fpr); FILE *fpw = fopen("file", "rwb+"); fseek(fpw, 102400, SEEK_SET); fwrite(buffer, sizeof(char), 22168, fpw); fclose(fpw); return 0;&#125; 编译运行如下： 12345678910# gcc -o fstream_tst fstream_tst.c# ll -h file*-rw-r--r-- 1 root root 100K Oct 18 18:23 file-rw-r--r-- 1 root root 122K Oct 18 15:18 file-bak# ./fstream_tst# ll -h file*-rw-r--r-- 1 root root 122K Oct 18 18:25 file-rw-r--r-- 1 root root 122K Oct 18 15:18 file-bak# diff file1 file-bak# 结果正常 分析用c++代码的实现1分析如下： 123456789101112131415# strace ./fstream_tst...open("file-bak", O_RDONLY) = 3lseek(3, 102400, SEEK_SET) = 102400read(3, "4318\n4319\n4320\n4321\n4322\n4323\n43"..., 22168) = 22168close(3) = 0open("file", O_WRONLY|O_CREAT|O_TRUNC, 0666) = 3lseek(3, 102400, SEEK_SET) = 102400lseek(3, 0, SEEK_CUR) = 102400fstat(1, &#123;st_mode=S_IFCHR|0620, st_rdev=makedev(136, 0), ...&#125;) = 0mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f81ce101000write(1, "102400\n", 6102400) = 6writev(3, [&#123;NULL, 0&#125;, &#123;"4318\n4319\n4320\n4321\n4322\n4323\n43"..., 22168&#125;], 2) = 22168close(3) 从上面可以看出编译后的可执行文件在打开要写入的文件时调用的是：open(&quot;file&quot;, O_WRONLY|O_CREAT|O_TRUNC, 0666) = 3 好奇怪为啥译后可执行文件执行open系统调用加了truncate标记！ 网上搜索了好久，没找到相关的解释，于是翻看《C++ Primer》圣书，找到如下说明，也明白了原因： 默认 ofstream流对象关联的文件将以out模式打开，使文件可写；以out模式打开的文件会被情况：丢弃该文件存储的所有数据； 所以从效果上来看，为ofstream对象指定out模式等效于同时指定了out和trunc模式 针对分析到的原因，修改代码如下： 12345678910111213141516171819202122#include &lt;fstream&gt;#include &lt;iostream&gt;using namespace std;int main(int argc, char* argv[])&#123; char buffer[22168]; ifstream ifile("file2-bak", std::ios_base::in | std::ios_base::binary); ifile.seekg(102400); ifile.read(buffer, 22168); ifile.close(); ofstream ofile; ofile.open("file2", std::ios_base::in | std::ios_base::out | std::ios_base::binary); ofile.seekp(102400, ios::beg); // seek to offset 102400 from begin cout &lt;&lt; ofile.tellp() &lt;&lt; endl; ofile.write(buffer, 22168); ofile.close(); return 0;&#125; 编译运行如下： 1234567891011# gcc -o fstream_tst fstream_tst.c# ll -h file*-rw-r--r-- 1 root root 100K Oct 19 12:03 file-rw-r--r-- 1 root root 122K Oct 18 15:18 file-bak# ./fstream_tst102400# ll -h file*-rw-r--r-- 1 root root 122K Oct 19 12:04 file-rw-r--r-- 1 root root 122K Oct 18 15:18 file-bak# diff file1 file-bak# 结果正常 结论经过这么大半天的测试分析查找，稍微明白了C++的fstream的使用，还真是不能直接想当然的拿C的那一套来推测。。。坑啊！ 参考http://www.runoob.com/cplusplus/cpp-files-streams.htmlhttps://stackoverflow.com/questions/34238063/c-seekp0-iosend-not-workinghttps://stackoverflow.com/questions/29593940/why-is-an-fstream-truncated-when-it-is-opened-with-the-flags-iosate-and-iosohttps://social.msdn.microsoft.com/Forums/vstudio/en-US/8f121287-539f-4fe1-96b6-db3e5b9306f4/vc10-using-stdofstream-truncates-file-without-trunc]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>fstream c++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个开源的C++日志框架-G3log的简单示例]]></title>
    <url>%2F2017%2F10%2F19%2Fg3log-introduction%2F</url>
    <content type="text"><![CDATA[概述G3log 是一个开源、支持跨平台的异步 C++ 日志框架，支持自定义日志格式。基于 g2log 构建，提升了性能，支持自定义格式。 G3log 主要特性： 日志和契约式设计框架 异步调用 线程安全 队列式日志 捕获和记录 SIGSEGV 以及其他严重的信号 在 Linux/OSX 上严重的信号会生成堆栈记录 G3log 跨平台，支持 Windows, Linux 和 OSX 链接：https://github.com/KjellKod/g3log 编译安装从github上下载 g3log 源码 准备工作123456# cd g3log# cd 3rdParty/gtest# unzip gtest-1.7.0.zip# cd ../../# mkdir build# cd build 编译12# cmake -DCMAKE_BUILD_TYPE=Release ..# make 当前g3log默认使用的是c++14，若系统用的是c++11，则把目录下所有的c++14都替换为c++11后即可； 安装12345678910111213141516171819202122232425262728293031# sudo make install[ 60%] Built target g3logger[ 73%] Built target g3log-FATAL-contract[ 86%] Built target g3log-FATAL-sigsegv[100%] Built target g3log-FATAL-choiceInstall the project...-- Install configuration: "Release"-- Up-to-date: /usr/local/lib/libg3logger.so.1.3.0-0-- Up-to-date: /usr/local/lib/libg3logger.so-- Up-to-date: /usr/local/include/g3log/active.hpp-- Up-to-date: /usr/local/include/g3log/atomicbool.hpp-- Up-to-date: /usr/local/include/g3log/crashhandler.hpp-- Up-to-date: /usr/local/include/g3log/filesink.hpp-- Up-to-date: /usr/local/include/g3log/future.hpp-- Up-to-date: /usr/local/include/g3log/g3log.hpp-- Up-to-date: /usr/local/include/g3log/logcapture.hpp-- Up-to-date: /usr/local/include/g3log/loglevels.hpp-- Up-to-date: /usr/local/include/g3log/logmessage.hpp-- Up-to-date: /usr/local/include/g3log/logworker.hpp-- Up-to-date: /usr/local/include/g3log/moveoncopy.hpp-- Up-to-date: /usr/local/include/g3log/shared_queue.hpp-- Up-to-date: /usr/local/include/g3log/sink.hpp-- Up-to-date: /usr/local/include/g3log/sinkhandle.hpp-- Up-to-date: /usr/local/include/g3log/sinkwrapper.hpp-- Up-to-date: /usr/local/include/g3log/stacktrace_windows.hpp-- Up-to-date: /usr/local/include/g3log/std2_make_unique.hpp-- Up-to-date: /usr/local/include/g3log/stlpatch_future.hpp-- Up-to-date: /usr/local/include/g3log/time.hpp-- Up-to-date: /usr/local/include/g3log/g2log.hpp-- Up-to-date: /usr/local/include/g3log/generated_definitions.hpp-- Up-to-date: /usr/local/lib/cmake/g3logger/g3loggerConfig.cmake 使用g3log代码示例文件：g3log_tst.cc 12345678910111213141516171819202122232425262728293031323334353637#include &lt;iostream&gt;#include &lt;g3log/g3log.hpp&gt;#include &lt;g3log/logworker.hpp&gt;using namespace std;string path_to_log_file = "/tmp/";string log_file = "g3log_file";std::unique_ptr&lt;g3::LogWorker&gt; worker;void log_init()&#123; worker = g3::LogWorker::createLogWorker(); auto handle = worker-&gt;addDefaultLogger(log_file, path_to_log_file); g3::initializeLogging(worker.get());&#125;void log_shutdown()&#123; g3::internal::shutDownLogging();&#125;int main(int argc, char* argv[])&#123; log_init(); // use LOGF which like printf LOGF(INFO, "Hi log %d", 123); LOGF(WARNING, "Printf-style syntax is also %s", "available”); // use LOG wich like std::cout LOG(INFO) &lt;&lt; "Hi " &lt;&lt; "LOG"; log_shutdown(); return 0;&#125; 编译运行 1234# g++ -std=c++11 -Wl,-rpath,/usr/local/lib -lg3logger -og3log_tst g3log_tst.cc# ./g3log_tstg3log g3FileSink shutdown at: 14:56:51 332071Log file at: [/tmp/g3log_file.g3log.20171011-145651.log] 查看log文件内容： 12345678# cat /tmp/g3log_file.g3log.20171011-145651.log g3log created log at: Wed Oct 11 14:56:51 2017 LOG format: [YYYY/MM/DD hh:mm:ss uuu* LEVEL FILE-&gt;FUNCTION:LINE] message (uuu*: microseconds fractions of the seconds value) 2017/10/11 14:56:51 046351 INFO [g3log_tst.cc-&gt;main:29] Hi log 123 2017/10/11 14:56:51 046376 WARNING [g3log_tst.cc-&gt;main:30] Printf-style syntax is also available 2017/10/11 14:56:51 046413 INFO [g3log_tst.cc-&gt;main:33] Hi LOG g3log g3FileSink shutdown at: 14:56:51 046529 g3log的动态log levelg3log支持dynamic logging level，默认是关闭的，若需要使用，在编译g3log时打开。 1# cmake -DCMAKE_BUILD_TYPE=Release -DUSE_DYNAMIC_LOGGING_LEVELS=ON ../ 在文件 g3log/loglevels.hpp 里有各个log level的定义： 12345678910111213namespace g3 &#123; static const int kDebugValue = 100; static const int kInfoValue = 300; static const int kWarningValue = 500; static const int kFatalValue = 1000; static const int kInternalFatalValue = 2000;&#125; // g3const LEVELS G3LOG_DEBUG&#123;g3::kDebugValue, &#123;"DEBUG"&#125;&#125;, INFO &#123;g3::kInfoValue, &#123;"INFO"&#125;&#125;, WARNING &#123;g3::kWarningValue, &#123;"WARNING"&#125;&#125;, FATAL &#123;g3::kFatalValue, &#123;"FATAL"&#125;&#125;; 设置log level的定义为： 1234567891011121314151617181920212223242526272829namespace log_levels &#123; /// Enable log level &gt;= log_level. /// log levels below will be disabled /// log levels equal or higher will be enabled. void setHighest(LEVELS level); void set(LEVELS level, bool enabled); void disable(LEVELS level); void enable(LEVELS level); /// WARNING: This will also disable FATAL events from being logged void disableAll(); void enableAll(); /// print all levels with their disabled or enabled status std::string to_string(std::map&lt;int, g3::LoggingLevel&gt; levelsToPrint); /// print snapshot of system levels with their /// disabled or enabled status std::string to_string(); /// Snapshot view of the current logging levels' status std::map&lt;int, g3::LoggingLevel&gt; getAll(); enum class status &#123;Absent, Enabled, Disabled&#125;; status getStatus(LEVELS level);&#125; // log_levels 代码示例文件：g3log_tst.cc 1234567891011121314151617181920212223242526272829303132333435363738394041424344#include &lt;iostream&gt;#include &lt;g3log/g3log.hpp&gt;#include &lt;g3log/logworker.hpp&gt;using namespace std;string path_to_log_file = "/tmp/";string log_file = "g3log_file";std::unique_ptr&lt;g3::LogWorker&gt; worker;void log_init()&#123; worker = g3::LogWorker::createLogWorker(); auto handle = worker-&gt;addDefaultLogger(log_file, path_to_log_file); g3::initializeLogging(worker.get());&#125;void log_shutdown()&#123; g3::internal::shutDownLogging();&#125;void set_log_level()&#123; g3::log_levels::setHighest(INFO);&#125;int main(int argc, char* argv[])&#123; log_init(); set_log_level(); // use LOGF which like printf LOGF(INFO, "Hi log %d", 123); LOGF(DEBUG, "Hi debug log %d", 123); LOGF(WARNING, "Printf-style syntax is also %s", "available”); // use LOG wich like std::cout LOG(INFO) &lt;&lt; "Hi " &lt;&lt; "LOG"; log_shutdown(); return 0;&#125; 编译运行 1234# g++ -std=c++11 -Wl,-rpath,/usr/local/lib -lg3logger -og3log_tst g3log_tst.cc# ./g3log_tstg3log g3FileSink shutdown at: 16:25:39 537074Log file at: [/tmp/g3log_file.g3log.20171011-162539.log] 查看log文件输出内容如下，可以看出并没有DEBUG级别的log输出： 12345678# cat /tmp/g3log_file.g3log.20171011-162539.log g3log created log at: Wed Oct 11 16:25:39 2017 LOG format: [YYYY/MM/DD hh:mm:ss uuu* LEVEL FILE-&gt;FUNCTION:LINE] message (uuu*: microseconds fractions of the seconds value) 2017/10/11 16:25:39 041987 INFO [g3log_tst.cc-&gt;main:35] Hi log 123 2017/10/11 16:25:39 042001 WARNING [g3log_tst.cc-&gt;main:37] Printf-style syntax is also available 2017/10/11 16:25:39 042016 INFO [g3log_tst.cc-&gt;main:40] Hi LOG g3log g3FileSink shutdown at: 16:25:39 042180 参考g3log还有很多别的特性，有需要的朋友可以详细研究下代码，很多使用示例也可以在源码里的g3log/example/和test_unit/目录里找到，cmake和代码都可以参考之。]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>g3log c++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[海量数据迁移方案]]></title>
    <url>%2F2017%2F10%2F10%2Fmass-data-transport%2F</url>
    <content type="text"><![CDATA[概述随着云的普及，越来越多的客户需要把数据迁移到云上，针对小量的数据，通过云公司提供的迁移工具走互联网就可以做到，但针对海量的数据，互联网上传输是不现实的，这就需要我们提供一种新的迁移方案来缩短数据迁移时间。 结合过去磁带库的容灾备份方案，针对海量数据迁移到云上的需求，云公司也采用了离线设备+传统运输的解决方案，能大大缩短数据迁移的时间周期，降低成本。 方案步骤 准备离线设备 源端数据拷贝到离线设备 离线设备通过传统方式运输到云公司机房 云公司机房从离线设备上导入数据 / 离线设备直接组建存储集群提供服务 友商提供服务的友商有: AWS，Azure，阿里云 产品功能 定制离线设备 开发客户端 NFS/CIFS数据导入 文件导出到OSS，支持配置文件存放bucket的对应关系 数据加密，压缩 数据秘钥管理 友商实现AWSSnowball 是一种 PB 级数据传输解决方案，它使用安全设备在 AWS 云之间传输大量数据。使用 Snowball 可解决大规模数据传输的难题（包括高昂网络成本、较长传输时间和安全问题）。使用 Snowball 传输数据简单、快速、安全，并且成本可低至高速 Internet 费用的五分之一。 借助 Snowball，您无需编写任何代码或购买任何硬件即可传输您的数据。只需在 AWS 管理控制台中创建一个任务，然后 Snowball 设备将自动交付于您。当它到达后，将该设备挂载到您的本地网络、下载并运行 Snowball 客户端来建立连接，然后使用该客户端选择要传输到该设备的文件目录。客户端随后将对文件进行加密，并将其高速传输至该设备。当传输完成后即可返还该设备，E-Ink 运送标签将自动更新，您可以通过 Amazon Simple Notification Service (SNS)、短信或直接在控制台中跟踪任务状态。 硬件设备便携式大容量设备，这些设备使用防篡改附件、加密和端到端跟踪，旨在确保数据的安全和全程监管链。每台 Snowball 设备可传输数 TB 的数据，根据您需要传输的数据量，您可以使用多台 Snowball 以并行方式或一个接一个地传输大型数据集。 客户端Snowball 客户端是您安装在本地主计算机上的软件，用于高效识别、压缩、加密您指定目录中的数据并将其传输到 Snowball。 产品特色 高速 高度可扩展 防状态和安全性 简单且可兼容 成本低廉 轻松检索数据 AzureAzure 导入/导出服务将大型数据快速、安全地迁移到云中。 使用 Azure 导入/导出服务，可以将硬盘驱动器寄送到 Azure 数据中心，从而安全地将大量数据传输到 Azure Blob 存储（==高度可缩放的非结构化数据对象存储==）。 你还可以使用此服务将数据从 Azure Blob 存储传输到硬盘驱动器，然后再寄送到你的本地站点。 如果需要在本地站点和 Azure 之间传输数 TB 的数据，而由于带宽限制或网络成本过高，通过网络上传或下载数据不可行，在这种情况下，则可以使用此服务。服务要求对硬盘驱动器进行 BitLocker 加密以确保数据的安全性。 服务支持所有公共 Azure 区域中的经典和 Azure Resource Manager 存储帐户（标准层和冷层）。 必须将硬盘驱动器寄送到本文后面指定的某个受支持的位置。 产品特色 高效迁移大量数据集 与网络传输相比，将物理磁盘传入/传出 Azure 可节省时间和金钱。将大量初始工作负荷数据导入 Azure，或者将数据快速发送到客户网站进行简单的内容分发。创建本地数据备份以存储到云中，然后按需将数据恢复到本地位置。 保持数据私有 使用“导入/导出”服务时，数据在整个过程中都很安全。磁盘使用 Microsoft BitLocker 驱动器加密进行加密，并在 Azure 门户或 Azure REST API 上通过 SSL 管理加密密钥。在数据中心内，我们使用快速、专用的网络上传链接将用户的数据从驱动器迁移到云。 确保数据完整性 使用“导入/导出”服务放心迁移数据。内置校验和以及 MD5 哈希可确保数据完整性。通过 Azure 门户上的日志和审核痕迹跟踪并监视数据迁移状态。 硬件设备支持将 2.5 英寸 SSD 或者 2.5/3.5 英寸 SATA II 或 III 内部 HDD 用于导入/导出服务。 单个导入/导出作业最多可以有 10 个 HDD/SSD，并且每个 HDD/SSD 可以为任意大小。大量驱动器可以分布在多个作业上，并且可创建的作业数没有限制。 导入作业概括而言，导入作业包括以下步骤： 确定要导入的数据，以及所需驱动器数目。 确定 Blob 存储中用于存储数据的目标 Blob 位置。 使用 WAImportExport 工具将数据复制到一个或多个硬盘驱动器，并使用 BitLocker 进行加密。 使用 Azure 门户或导入/导出 REST API 在目标存储帐户中创建导入作业。 如果使用 Azure 门户，请上传驱动器日记文件。 请提供回寄地址以及快递商帐户号码，以便我们将驱动器寄回给你。 将硬盘驱动器寄送到在创建作业时获得的寄送地址。 在导入作业详细信息中更新快递跟踪号码，然后提交导入作业。 Azure 数据中心在收到驱动器后会对其进行处理。 该中心会使用你的快递商帐户将驱动器寄送到你在导入作业中提供的回寄地址。 导出作业概括而言，导出作业包括以下步骤： 确定要导出的数据，以及所需驱动器数目。 确定你的数据在 Blob 存储中的源 Blob 或容器路径。 使用 Azure 门户或导入/导出 REST API 在源存储帐户中创建导出作业。 指定你的数据在导出作业中的源 Blob 或容器路径。 请提供回寄地址以及快递商帐户号码，以便我们将驱动器寄回给你。 将硬盘驱动器寄送到在创建作业时获得的寄送地址。 在导出作业详细信息中更新快递跟踪号码，然后提交导出作业。 Azure 数据中心在收到驱动器后会对其进行处理。 驱动器使用 BitLocker 加密；密钥通过 Azure 门户提供。 该中心会使用你的快递商帐户将驱动器寄送到你在导入作业中提供的回寄地址。 导入/导出工具使用此 WAImportExport 工具可以方便地将数据复制到驱动器、使用 BitLocker 加密驱动器上的数据，以及生成驱动器日志文件。 WAImportExport 工具仅兼容 64 位 Windows 操作系统。 阿里云闪电立方 Lightning Cube，支持TB到PB级别数据上云解决方案。通过定制的离线迁移设备，解决本地数据中心大规模数据传输上云的迁移周期长，网络专线费用昂贵，数据复制效率低，迁移过程数据安全等问题。快速、安全、高效、降低90%的数据迁移成本。 2017年6月10号，阿里云发布了产品“闪电立方”。它像是一个可移动的“数据中心”，通过一个安全的存储硬件，可将100TB数据安全地一次性转移，最快24小时即可完成PB级数据迁移。“闪电立方”外形黑色立方，可防尘防水，抗震抗压。容量上可容纳100TB数据，并能实现最高30：1的压缩比，配合高效的去重功能可节省更多的存储空间。内置了20Gbps的光纤网络连接。在数据安全性方面，“闪电立方”采用AES 256端到端加密算法，密钥由用户保管，运送过程中全程断网，同时配备24小时定位监控。 产品特色 高速高效 相比传统的Internet和高速专线速度提升20倍，成本降低60% 数据压缩 支持最高30倍压缩比 数据去重 自动对数据进行去重处理，迁移过程中自动恢复 小文件优化 小文件自动合并，提升读写效率 安全可靠 加密能力 提供设备端到端的加密机制，用户自己管理密码并通过密码授权解密，保障运输过程中数据的安全 权限 用户主动通过RAM授权的方式，允许数据同步到云端用户的存储空间 数据擦除 数据迁移完毕后，通过阿里云官方数据擦除机制，确保数据不会被第三方获取 低成本 单次大容量的离线迁移能力，大幅降低物流成本和网络成本 可扩展 单台设备可支持100TB或480TB的迁移能力，可横向扩展，利用多套设备同时迁移PB+级别数据 硬件设备为数据迁移而生的专业设备，标准机架和电源，可多套同时部署提升迁移效率。支持的数据源包括：本地文件系统，NAS，HDFS，FastDFS等。 客户端没找到文档介绍“闪电立方”的客户端，从它支持的数据源包括：本地FS，NAS，HDFS，FastDFS等，可以推断出肯定有这样的一个客户端软件。 总结在海量数据上云面前，离线物理设备迁移是一种很好的解决方案，几大公有云提供商都提供了专门的迁移设备和迁移服务，比较好的会设计专门的硬件设备，简单点的就使用标准的 2.5/3.5 英寸磁盘，但都会考虑数据安全性，这也是客户比较关注的问题。 总结下产品所需的功能如下： 硬件设备 客户端软件 数据加密 秘钥管理 数据压缩 数据校验 自研需求该产品现在没有开源的解决方案，除了AWS以外，Azure和阿里云都是今年刚推出该服务，我们也只能参考他们的实现来自己研发产品。 硬件设备研发若要设计研发可以跟友商竞争的硬件设备，包括整套专业硬件设备的研发，难点非常大 难度：★★★★★ 客户端软件客户端软件研发也是重点，需要包括的功能有： 支持哪些数据源？ 数据源跟云上产品的映射关系？ 数据加密，压缩，一致性校验 秘钥管理 难度：★★★★☆]]></content>
      <categories>
        <category>storage</category>
      </categories>
      <tags>
        <tag>data transfer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cephfs架构解读与测试分析-part2]]></title>
    <url>%2F2017%2F09%2F18%2Fcephfs-knowledge-share-part2%2F</url>
    <content type="text"><![CDATA[本文是在日知录社区里分享内容的第二部分，主要介绍CephFS的测试方法和结果分析，对视频分享和第一部分感兴趣的请阅读如下链接： cephfs架构解读与测试分析-part1日知录 - CephFS架构解读与测试分析 CephFS测试为了验证CephFS是否满足产品需求，我们基于最新的Ceph Jewel 10.2.7版本做了测试。 CephFS Jewel版本特性 CephFS – Production Ready Single Active MDS，Active-Standby MDSs Single CephFS within a single Ceph Cluster CephFS requires at least kernel 3.10.x Experimental Features Multi Active MDSs Multiple CephFS file systems within a single Ceph Cluster Directory Fragmentation CephFS测试目的 CephFS POSIX基本功能完备？ CephFS性能跑满整个集群？ CephFS长时间是否稳定？ CephFS能否应对MDS异常？ NO： 不是针对MDS的参数调优 不是MDS的压力测试 MDS压力测试时建议配置在单独的机器上 调大 mds_cache_size MDS压力测试请参考https://www.slideshare.net/XiaoxiChen3/cephfs-jewel-mds-performance-benchmark CephFS测试环境针对测试目的，我们选择了三台物理机搭建一个全新的Ceph集群，提供CephFS服务。 物理机的配置 10个4T 7200RPM SATA盘 2个480GB的SATA SSD盘，Intel S3500 2个万兆网卡 SSD盘的性能为： 参数 性能 容量 480GB 顺序读取(最高) 500 MB/s 顺序写入(最高) 410 MB/s 随机读取(100% 跨度) 75000 IOPS 随机写入(100% 跨度) 11000 IOPS SATA盘的性能为： 参数 性能 容量 4TB 顺序读写 120 MB/s IOPS 130 Ceph配置参数测试用的CephFS client为单独的服务器，128G内存，万兆网络连接Ceph集群。 如上图所示，Ceph集群的部署配置为： replica为3 三个Monitor 两个MDS部署为Active/Standy Ceph集群和CephFS client的系统版本信息如下： 模块 版本 Ceph Version Jewel 10.2.7 Ceph Cluster OS CentOS Linux release 7.2.1511 (Core) Ceph Cluster kernel version 3.10.0-327.el7.x86_64 Cephfs Client OS CentOS Linux release 7.2.1511 (Core) Cephfs kernel version 4.11.3-1.el7.elrepo.x86_64 预估Ceph集群性能通过Ceph集群架构和物理磁盘、网络的性能指标，就可以预估整个Ceph集群的性能了。 如我们这个Ceph集群，三台物理机，配置三副本，又受限于单Client端的万兆网络性能，所以整个集群的最大吞吐量为：单台物理机上的磁盘性能 / 万兆网络性能的最小值。 每个物理机上，2个SSD做10个OSD的journal，其整体性能约为：2 * (单个ssd盘的性能) / 10 * (单个sata盘的性能) / 万兆网络性能 的最小值。 CephFS测试工具 功能测试 手动，fstest 性能测试 dd，fio，iozone，filebench 稳定性测试 fio，iozone，自写脚本 异常测试 手动 CephFS测试分析功能测试 手动 我们使用文件系统的常用操作：mkdir/cd/touch/echo/cat/chmod/chown/mv/ln/rm等。 fstest fstest是一套简化版的文件系统posix兼容性测试条件，有3600来个回归测试，测试的系统调用覆盖的也比较全面。 chmod, chown, link, mkdir, mkfifo, open, rename, rmdir, symlink, truncate, unlink 结论功能测试通过 性能测试性能测试比较重要，也是我们测试的重点，按照CephFS的Layout配置，我们选择了三类Layout配置： stripe_unit=1M, stripe_count=4, object_size=4M 目录为: dir-1M-4-4M，条带大小为1M，条带数目为4，object大小为4M 12配置测试目录attr# setfattr -n ceph.dir.layout -v "stripe_unit=1048576 stripe_count=4 object_size=4194304" dir-1M-4-4M stripe_unit=4M, stripe_count=1, object_size=4M 目录为: dir-4M-1-4M，也是系统默认配置 12配置测试目录attr# setfattr -n ceph.dir.layout -v "stripe_unit= 4194304 stripe_count=1 object_size=4194304" dir-4M-1-4M stripe_unit=4M, stripe_count=4, object_size=64M 目录为: dir-4M-4-64M，条带大小为4M，条带数目为4，object大小为64M 12配置测试目录attr# setfattr -n ceph.dir.layout -v "stripe_unit=4194304 stripe_count=4 object_size=67108864" dir-4M-4-64M ==注：后续图表中分别拿上述目录名来代表三种CephFS Layout配置分类== dir-1M-4-4M dir-4M-1-4M dir-4M-4-64M ddlinux系统常用的测试设备和系统性能的工具。 测试命令： Direct IO： oflag/iflag=direct Sync IO：oflag/iflag=sync Normal IO：不指定oflag/iflag 测试文件大小：20G 不能选择太小的测试文件，减少系统缓存的影响 测试结果： 结论： normal io，客户端缓存影响，写性能较高，不做分析。读性能约为1GB/s，受限于万兆网卡。 direct io，客户端写性能只有150MB/s，读性能只有600MB/s，这个是cephfs kernel client端的direct io逻辑导致的。 sync io，随着bsd的增大性能有所提升，写性能能到 550MB/s，读性能有1GB/s Stripe模式变化的角度分析 bs=512k/1M时，各个stripe模式下的IO性能基本相同 bs=4M/16M时，针对direct io，stripe unit=1M的条带性能略低（kernel client的direct io逻辑有关），针对sync io，stripe unit=1M的条带性能较好（并发性较好的原因） 默认的file layout(橙色)，dd的性能就挺好，64Mobjcet 的stripe模式(灰色)没有明显的性能提升 fiofio也是我们性能测试中常用的一个工具，详细介绍Google之。 我们测试中固定配置： 12345-filename=tstfile 指定测试文件的name-size=20G 指定测试文件的size为20G-direct=1 指定测试IO为DIRECT IO-thread 指定使用thread模式-name=fio-tst-name 指定job name 测试bandwidth时： -ioengine=libaio/sync -bs=512k/1M/4M/16M -rw=write/read -iodepth=64 -iodepth_batch=8 -iodepth_batch_complete=8 测试iops时： -ioengine=libaio -bs=4k -runtime=300 -rw=randwrite/randread -iodepth=64 -iodepth_batch=1 -iodepth_batch_complete=1 测试结果： bandwidth的测试结果如下图： bandwidth: direct sync IO write/randwrite 性能最多约为：155 MB/s read/randread 性能最多约为：600 MB/s bandwidth: direct libaio write/randwrite 性能最多约为：810 MB/s read/randread 性能最多约为：1130 MB/s ==这基本就是集群的整体性能== bandwidth: cephfs stripe模式变化时 结论与dd的基本相同 iops的测试结果如下表： io mode type dir-1M-4-4M dir-4M-1-4M dir-4M-4-64M randwrite iops 4791 4172 4130 latency(ms) 13.35 15.33 15.49 randread iops 2436 2418 2261 latency(ms) 26.26 26.46 28.30 注释：上诉测试randread中，因为有cephfs这一层，所以即使direct IO，在OSD上也不一定会read磁盘，因为OSD有缓存数据；所以这里测试采取每次测试前在所有ceph cluster的host上执行sync; echo 3 &gt; /proc/sys/vm/drop_caches;清理缓存； iozoneiozone是目前应用非常广泛的文件系统测试标准工具，它能够产生并测量各种的操作性能，包括read, write, re-read, re-write, read backwards, read strided, fread, fwrite, random read, pread ,mmap, aio_read, aio_write等操作。 测试DIRET IO / SYNC IO - 非throughput模式 不指定threads，测试单个线程的iozone性能 12iozone -a -i 0 -i 1 -i 2 -n 1m -g 10G -y 128k -q 16m -I -Rb iozone-directio-output.xlsiozone -a -i 0 -i 1 -i 2 -n 1m -g 10G -y 128k -q 16m -o -Rb iozone-syncio-output.xls 测试系统吞吐量 - throughput模式 指定threads=16，获取整个系统的throughput 12iozone -a -i 0 -i 1 -i 2 -r 16m -s 2G -I -t 16 -Rb iozone-directio-throughput-output.xlsiozone -a -i 0 -i 1 -i 2 -r 16m -s 2G -o -t 16 -Rb iozone-syncio-throughput-output.xls 测试结果： iozone测试的结果很多，很难每个都画出图表展示出来，这里挑选几组对比数据作为对比。 下图中的类似128K/1M的文字含义为：记录块为128K，测试文件为1M性能输出的单位为：MB/s 非throughput模式 write性能 read性能 写性能：direct IO模式为 150 MB/s，sync IO模式为 350MB/s 读性能：direct IO模式为 560 MB/s，sync IO模式为 7000 MB/s==（iozone的io模式和client端缓存的影响，指标不准确）== Stripe模式变化： 各个Stripe下性能基本一致，对于小文件小IO模式来说，dir-1M-4-4M的性能略好 throughput模式 各种write的性能基本相同，最大约为 750 MB/s，基本是集群写的极限 direct IO模式下，读性能约为 1120 MB/s，client端万兆网络带宽的极限 sync IO模式下，读性能高达 22500 MB/s，iozone的io模式和client端缓存的影响，指标不准确 filebenchfilebench是一款文件系统性能的自动化测试工具，它通过快速模拟真实应用服务器的负载来测试文件系统的性能。 filebench有很多定义好的workload，针对cephfs的测试，我们可以选择其中一部分有代表性的workloads即可。 createfiles.f / openfiles.f / makedirs.f / listdirs.f / removedirs.f randomrw.f / fileserver.f / videoserver.f / webserver.f 结论 filebench测试用例，除了读写操作外，其他的都是元数据操作，基本不受cephfs stripe的影响 各种文件操作的时延都不高，可以满足基本的对filesystem的需求 稳定性测试为了测试cephfs是否能在线上提供服务，我们需要测试下其稳定性，这里采用两种方式测试。 读写数据模式针对读写数据模式，我们选择工具fio，在cephfs client端长时间运行，看会不会报错。 测试逻辑大概如下： 12345# fio循环测试读写while now &lt; time fio write 10G file fio read 10G file delete file 读写元数据模式针对读写元数据模式，我们采用自写脚本，大规模创建目录、文件、写很小数据到文件中，在cephfs client端长时间运行，看会不会报错。 测试逻辑大概如下： 1234567# 百万级别的文件个数while now &lt; time create dirs touch files write little data to each file delete files delete dirs 结论 通过几天的连续测试，cephfs一切正常，这说明cephfs是可以应用到生产环境的。 至于上亿级别的文件测试，也遇到点问题。 问题与解决 日志中报Behind on trimming告警 调整参数mds_log_max_expiring，mds_log_max_segments rm删除上亿文件时报No space left on device错误 调大参数mds_bal_fragment_size_max，mds_max_purge_files，mds_max_purge_ops_per_pg 日志中报_send skipping beacon, heartbeat map not healthy 调大参数mds_beacon_grace，mds_session_timeout，mds_reconnect_timeout 基本思路： 查看Client和MDS端log Google搜索关键字 or 搜索Ceph相关代码 分析原因 调整参数 当然也有的问题不是简单调整参数就搞定的，那就尽量去分析问题，向社区提bug反馈。 异常测试cephfs的功能依赖于MDS和Ceph Cluster，关键的元数据都通过MDS获取，这里测试的异常也主要基于MDS的异常进行分类的。 查看ceph MDS与interl和timeout相关的配置有： 123456OPTION(mds_tick_interval, OPT_FLOAT, 5)OPTION(mds_mon_shutdown_timeout, OPT_DOUBLE, 5)OPTION(mds_op_complaint_time, OPT_FLOAT, 30)OPTION(mds_beacon_grace, OPT_FLOAT, 15)OPTION(mds_session_timeout, OPT_FLOAT, 60) // cap bits and leases time out if client idleOPTION(mds_reconnect_timeout, OPT_FLOAT, 45) // seconds to wait for clients during mds restart, make it (mds_session_timeout - mds_beacon_grace) 在Sage weil的博士论文里提到CephFS允许客户端缓存metadata 30s，所以这里测试对MDS stop/start的时间间隔取为：2s，10s，60s 测试工具：fio 测试分类： 单MDS 主从MDS 测试结果： 单MDS时： 2s/10s 无影响 60s时影响IO 主从MDS时： 主从不同时停无影响 同时停时与单MDS一致 mds停60s时会影响IO，fio测试结果如下图： 另外这里只举例说明了fio，同样异常测试中我们也测试了iozone，因为iozone会读写不同的文件，所以在mds停掉后，新的文件操作就会被hang住。 结论 单MDS的情况下，短暂的MDS crush并不会影响客户端对一个file的读写 单MDS的情况下，MDS crush后，client端对没有缓存过caps的文件操作会hang住 主从MDS的情况下，只要有一个MDS正常，CephFS的服务就不会中断 主从MDS的情况下，两个MDS都crush后，影响与单MDS的一致 所以生产环境中，我们建议配置主从MDS的模式，提高CephFS的可用性。 总结与展望总结 CephFS是production ready的，能满足基本生产环境对文件存储的需求 CephFS kernel client端的Linux kernel版本最好大于4.5-rc1（支持aio） 对性能要求不高时，考虑使用CephFS FUSE client，支持Quotas CephFS的主从MDS是稳定的，优于单MDS配置 生成环境使用CephFS时，独立机器上配置MDS，调大“mds_cache_size” 使用CephFS时，避免单个目录下包含超级多文件（more than millions） CephFS能跑满整个ceph集群的性能 默认stripe模式下(stripe unit=4M, stripe count=1, object size=4M)， CephFS的性能就挺好 小文件的应用场景下，尝试配置小的stripe unit，对比默认stripe的性能 CephFS的Direct IO性能有限，分析后是cephfs kernel client的IO处理逻辑限制的 http://www.yangguanjun.com/2017/06/26/cephfs-dd-direct-io-tst-analysis/ 受到CephFS client端的系统缓存影响，非Direct IO的读写性能都会比较高，这个不具有太大参考意 使用CephFS kernel client，且object size大于16M时，一次性读取大于16M的数据读时IO会hang住 http://www.yangguanjun.com/2017/07/18/cephfs-io-hang-analysis/ 展望Ceph Luminous (v12.2.0) - next long-term stable release series The new BlueStore backend for ceph-osd is now stable and the new default for newly created OSDs Multiple active MDS daemons is now considered stable CephFS directory fragmentation is now stable and enabled by default Directory subtrees can be explicitly pinned to specific MDS daemons]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>cephfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[金融云存储灾备之两地三中心]]></title>
    <url>%2F2017%2F09%2F12%2Fstorage-backup-multisite%2F</url>
    <content type="text"><![CDATA[概述在金融行业 ，特别是银行业，对数据的安全性要求特别高，对业务的连续性也是一样。传统的单数据中心模式和同城双中心的高可用模式也不能满足其需求，于是就诞生了最新的一种超高可用架构 - 两地三中心。 两地三中心，两地是指同城、异地，三中心是指生产中心、同城容灾中心、异地容灾中心。充分结合了同城的距离优势和异地的灾备优势，提供了超高可用的服务架构，模式如下图： 友商提供服务的友商有: 阿里云，腾讯云，百度云 产品功能 前端负载均衡 同城双活 同城数据中心实时同步 异地数据中心异步复制 VPN专线 物理机独享 友商实现金融云方面，友商提供了很多解决方案，一般都采用独立的机房集群，与公有云物理隔离，且满足一行三会金融监管要求。 阿里云金融云解决方案为新金融行业提供量身定制的云计算服务，具备低成本、高弹性、高可用、安全合规的特性。帮助金融客户实现从传统IT向云计算的转型，为客户提供完整的“云.端.数”的能力。 链接：https://www.aliyun.com/solution/finance 金融云专属产品 云服务器ECS - 专属物理集群，高级安全服务，高质量IO保障，高品质客户服务 更高规格SLA保障，可用性为99.95%，可靠性达到99.9999% IO性能保障，采用更可靠宽裕的资源分配机制 对外端口检测，默认不开放互联网访问端口 图形化安全组配置工具，方便用户操作，提升安全级别 云数据库RDS - 专属物理集群，异地灾备服务，高级备份和回滚服务 更高规格SLA保障，可用性99.97%，可靠性达99.999999% 数据更加安全，支持SQL Server 2008R2和MySQL5.5、5.6，使用强同步模式确保数据安全性 主备双中心和异地灾备，同城双中心，异地单向数据复制功能 高级备份和回滚策略，默认备份时间为30天，支持自定义数据备份时间 银行架构部署图 ==数据库== 实现主备数据同步，==数据传输== 实现文件同步 链接： https://www.aliyun.com/solution/finance/bank.html 使用产品 云服务器 负载均衡 云数据库 对象存储 云监控 数据传输 金融云数据库 MySQL 金融版 是一款具有金融级强一致性，高可靠性，能跨机房部署的增强型MySQL数据库集群，是阿里云关系型数据库的升级版本，面向对数据一致性、高可用、高安全性有强烈要求的高端企业用户。链接：https://www.aliyun.com/product/mysqlenterprise 数据传输服务 数据传输(Data Transmission)是阿里云提供的一种支持RDBMS(关系型数据库)、NoSQL、OLAP等多种数据源之间数据交互的数据服务。它提供了数据迁移、实时数据订阅及数据实时同步等多种数据传输能力。通过数据传输可实现不停服数据迁移、数据异地灾备、跨境数据同步、缓存更新策略等多种业务应用场景，助您构建安全、可扩展、高可用的数据架构。链接：https://www.aliyun.com/product/dts 数据传输服务的描述里只写了数据库，没写支持文件同步，比较奇怪 腾讯云金融云解决方案全系列云计算产品，为您实施高可用的业务容灾架构，随心打造。 链接：https://www.qcloud.com/solution/finance 金融云专属产品 专用宿主机 - 支持独享宿主机资源，灵活自主规划，避免资源竞争 金融大数据 - 独享大数据分析平台，提供精准个性化用户分析报告 金融数据库 - 兼容Mysql，数据强一致、异地自动同步、万级QPS高性能、自动扩容、灵活智能恢复 金融安全 - 专享攻击防护能力以及协同防御方案，保障金融业务安全 银行架构部署图 链接：https://www.qcloud.com/solution/solutionSubpage/finance-bank 基于云的同城业务双活，两地三中心 在腾讯云上构建的IT应用系统，可以利用腾讯云多地的合规机房和分布式云数据库TDSQL方便的建立同城业务双活、两地三中心的高可用架构应用。两地三中心灾备部署架构如下图所示。TDSQL云数据库提供金融级的容灾架构支持两地三中心部署结构（异地节点直线距离一般大于100km），数据库的所有组件全部实现跨机房、跨地域部署。在同城的生产中心和灾备中心之间，TDSQL实现数据实时同步；生产中心与异地灾备中心之间实现异步数据复制。 通过上述架构可以以较低的成本实现同城双活，数据异地自动备份，不依赖IOE架构，以低于商业数据库1/2的成本实现99.99999%的超高可靠性。在实际应用中，以腾讯计费为例，TDSQL云数据库承载的账户量超过100亿，每日请求超10亿，99.95%的请求在5ms以内响应。连续两年多的运营零中断、零数据误差。 可以推断是通过 ==分布式云数据库TDSQL== 实现的灾备 云数据库 MariaDB 云数据库 MariaDB (又名 TDSQL) 是基于 MariaDB 内核高度兼容 MySQL 的关系数据库，同时具有商用数据库的高安全性、可用性和开源数据库的低成本，主要应用于企业级服务场景；您只需付出商用数据库的 1/10 费用，即可拥有商用数据库的安全性、数据一致性和可靠性。 产品特性 介绍 数据强一致性 默认配置强同步复制 更高安全 全面的安全防护 更高可用性 提供高于99.99%的可用性，透明的故障转移 更高性能 基于PCI-E SSD、强大IO性能 与 MySQL 兼容 高度兼容 MySQL5.5、5.6 链接：https://www.qcloud.com/product/tdsql 使用产品 金融服务器/专用宿主机 金融数据库 金融大数据 负载均衡 私有网络 对象存储 云安全 云监控 百度云金融云解决方案百度金融云解决方案为银行、证券、保险及互联网金融行业提供安全可靠的IT基础设施、大数据分析、人工智能及百度生态支持等整体方案，为金融机构的效率提升及业务创新提供技术支撑 链接：https://cloud.baidu.com/solution/finance.html 金融云专属产品 专属资源池 - 独享的计算存储资源池 安全体系 VPN/专线 + VPC 大数据分析 - 提供完整的大数据分析服务 人工智能 - 人脸识别/文字识别 容灾体系 - 同城多可用区及异地云机房，构建两地三中心容灾体系 银行架构部署图 可以看出图中画的只是 ==数据库== 的同步 使用产品 应用服务器 数据库 负载均衡 云安全 云监控 大数据分析服务 人工智能服务 总结对比国内几家大的公有云厂商的金融云服务方案，可以得出两地三中心的数据灾备是其核心，其业务有可以拆分为： 同城数据中心数据实时同步 异地数据中心数据异步复制 同城数据实时同步同城数据中心之间一般都要求数据实时同步，RPO为0，RTO为秒级。 上文提到的几种数据同步有： 数据库实时同步 几家的解决方案里，都提到了分布式数据库，实现同城两个数据中心的数据强一致性，这个可以通过部署分布式数据库的实例分别在两个数据中心，然后配置分布式数据库为实时同步即可。 所以这里的关键为：实现跨数据中心的，实时同步的分布式数据库 主备文件同步 阿里云架构图里体现了通过数据传输服务实现文件同步，这个不是实时的。 对象存储同步 腾讯云架构图里体现了对象存储的数据同步，通过分析，这个也不是实时的。 异地数据异步复制异地数据中心之间一般只要求数据异步复制即可，RPO为秒或分钟级，RTO为分钟级。 上文提到的几种数据异步复制有： 数据库异步复制 这个可以通过配置数据库的主从关系和异步复制即可。 文件异步复制 阿里云架构图里的文件异步复制是通过数据传输服务实现的。 对象存储异步复制 腾讯云架构图里体现了对象存储的异步复制，应该为内部实现。 自研需求分布式数据库要求跨数据中心 + 实时同步 自研参考阿里云和腾讯云，基于MySQL/MariaDB上，配置开发跨数据中心配置的数据库难度：★★★★☆ 文件同步要求同城秒级，异地分钟级延迟 建议rsync + inotify实现 分析文件同步的rsync + inotify方案，应用较多，比较成熟，难道不大。难度：★★☆☆☆ 对象存储同步要求同城秒级，异地分钟级延迟 自研Ceph Radosgw 支持跨区域的同步难度：★★★☆☆]]></content>
      <categories>
        <category>storage</category>
      </categories>
      <tags>
        <tag>backup</tag>
        <tag>multisite</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes里存储概念的解疑]]></title>
    <url>%2F2017%2F09%2F04%2Fkubernetes-storage-confuse%2F</url>
    <content type="text"><![CDATA[近日在 Kubernetes 使用 Ceph 的问题中，因为在两地，没有面对面的沟通，导致了理解的差异与不解，后来与本地同事详细沟通后，终于弄明白了之前的隔阂和问题所在，这里做下记录。 问题Kubernetes存储简介Kubernetes 里存储的使用，可分为两类： PersistentVolume和PersistentVolumeClaims A PersistentVolume(PV) is a piece of storage in the cluster that has been provisioned by an administrator. A PersistentVolumeClaims(PVC) is a request for storage by a user. StorageClasses A StorageClass provides a way for administrators to describe the “classes” of storage they offer. PVs支持两种方式的provision: statically or dynamically. Static A cluster administrator creates a number of PVs. Dynamic Base StorageClasses, when none of the static PVs the administrator created matches a user’s PersistentVolumeClaim, the cluster may try to dynamically provision a volume specially for the PVC. 从是否支持多客户端同时读写，也可分为三类： ReadWriteOnce ReadOnlyMany ReadWriteMany Kubernetes存储需求Kubernetes 对存储的需求，大致可以总结为两类： 独立使用的PV 支持多客户端同时读写的PV 支持多客户端同时读写 + Dynamic provisioning的PV Ceph支持Kubernetes使用 Ceph 支持 Kubernetes，是否满足需求？ 独立使用的PV Ceph RBD支持即可 支持多客户端同时读写的PV CephFS / RBD + NFS 所以从我的角度理解：Ceph是可以满足Kubernetes对存储的需求的。 但 Kubernetes 的同事却坚持 Ceph 不能满足需求支持多客户端同时读写的PV，要使用 Glusterfs 。Why？ 分析这个事情在后来我与本地熟悉 Kubernetes 的同事详细沟通后，才终于弄明白之前的隔阂和问题所在。 下面从两个人所熟悉的知识领域来分析下问题。 volume概念存储角度首先说下在存储领域的volume含义，在存储领域也工作了快十年了，以为之前做过NAS和SAN存储，提起volume，我的印象还是一个卷，一个块设备。 熟悉存储的都知道，块设备是没法做到支持多客户端同时读写的，必须通过分布式文件系统来实现。所以提到volume，我就明确的表示它不会支持多客户端同时读写。 在网上搜索了下volume的定义，有下面几个不同的说法： storage volume A storage volume is an identifiable unit of data storage. It can be a removable hard disk, but it does not have to be a unit that can be physically removed from a computer or storage system. 参考：http://searchstorage.techtarget.com/definition/volume LUN与volume的描述 这个描述，与我理解的volume不一致 :( LUN是对存储设备而言的，volume是对主机而言的。LUN相对于主机来讲就是一个“物理硬盘”，与C盘D盘所在IDC或SCSI硬盘的性属是相同的。在该“物理硬盘”上创建一个或多个分区，再创建文件系统，才可以得到一个VOLUME。此时VOLUME相对于主机是一个逻辑设备。http://blog.csdn.net/liukuan73/article/details/45506441 ==我不赞同上述文章里的说法！后面介绍的才是正确的== 可以参考这里的介绍，更加可靠些： A volume normally refers to a “disk” created via a “volume manager” such as Veritas or a volume created by an operating system such as Windows NT.A LUN refers to a “logical unit number” presented to a host as ==a SCSI ID==. (i.e., LUN number 1 specifies SCSI ID 1 on that port. Therefore, the term volume can be considered software based and LUN considered hardware based.http://searchstorage.techtarget.com/answer/The-difference-between-volume-and-a-LUN 在另一篇文章里也图形化描述了LUN和volume的区别： 参考：https://jmetz.com/2016/11/whats-the-difference-between-a-lun-and-a-volume/ Glusterfs volume Glusterfs里对volume的说明 A volume is a logical collection of bricks where each brick is an export directory on a server in the trusted storage pool. 其他 在LVM和openstack里，volume的概念则与SAN中的一致，都是块设备。 ==所以，以后提到volume还是要看具体的应用场景！！！== Kubernetes角度在 Kubernetes 的官网上有这么一段话： At its core, a volume is just a directory, possibly with some data in it, which is accessible to the containers in a pod. How that directory comes to be, the medium that backs it, and the contents of it are determined by the particular volume type used. 参考： https://kubernetes.io/docs/concepts/storage/volumes/#background 所以从上面可知，这里的volume是指后端存储提供的一个目录。 若volume指一个后端存储提供的目录，那是否支持多客户端同时读写，就看后端提供该目录的实现了。 provisioning概念存储角度存储里有storage provisioning概念，storage provisioning指的是分配空间的处理过程，通常这种空间是服务器磁盘空间，通过存储自动配置可以优化存储局域网（SAN）的性能，分为fat-provisioning和thin-provisioning两种。 fat-provisioning 预先分配，底层存储会预留出对应请求的存储空间。 thin-provisioning 实时分配（也称为精简分配），底层存储会根据实时需求动态分配存储空间，而不预先预留出存储空间。 有这么一段介绍： Thin provisioning (TP) is a method of optimizing the efficiency with which the available space is utilized in storage area networks (SAN). 所以存储角度说的provisioning，是SAN系统相关的，不是文件系统相关的。 Kubernetes角度在 Kubernetes 里，提出Dynamic Provisioning的概念，是在Storageclass里提到的，介绍如下： Dynamic When none of the static PVs the administrator created matches a user’s PersistentVolumeClaim, the cluster may try to dynamically provision a volume specially for the PVC. This provisioning is based on StorageClasses: the PVC must request a class and the administrator must have created and configured that class in order for dynamic provisioning to occur. Claims that request the class “” effectively disable dynamic provisioning for themselves. 大致就是用户不用配置很多个PV了，只需要配置好Storageclass，在PVC里指定Storageclass即可，这样 Kubernetes 会自动的从Storageclass里配置好的存储里动态分配出一个指定size的PV，把PVC绑定到PV上。 参考： http://blog.kubernetes.io/2016/10/dynamic-provisioning-and-storage-in-kubernetes.html 所以 Kubernetes 角度说的provisioning，其实是PV的动态分配，跟存储里的provisioning不一样。 同事当时说成fs provisioning，弄的我很困惑，存储里没有fs provisioning这一说 明确需求经过上面的分析，我明确了同事对 Kubernetes 存储的需求是要支持：Storageclass的Dynamic Provisioning + ReadWriteMany。 查看Kubernetes官方文档，各个后端存储的支持情况为： PV Volume Plugin ReadWriteOnce ReadOnlyMany ReadWriteMany AWSElasticBlockStore ✓ - - AzureFile ✓ ✓ ✓ AzureDisk ✓ - - ==CephFS== ✓ ✓ ✓ Cinder ✓ - - FC ✓ ✓ - FlexVolume ✓ ✓ - Flocker ✓ - - GCEPersistentDisk ✓ ✓ - ==Glusterfs== ✓ ✓ ✓ HostPath ✓ - - iSCSI ✓ ✓ - PhotonPersistentDisk ✓ - - Quobyte ✓ ✓ ✓ NFS ✓ ✓ ✓ ==RBD== ✓ ✓ - VsphereVolume ✓ - - PortworxVolume ✓ - ✓ ScaleIO ✓ ✓ - StorageOS ✓ - - StorageClasses Volume Plugin Internal Provisioner AWSElasticBlockStore ✓ AzureFile ✓ AzureDisk ✓ ==CephFS== - Cinder ✓ FC - FlexVolume - Flocker ✓ GCEPersistentDisk ✓ ==Glusterfs== ✓ iSCSI - PhotonPersistentDisk ✓ Quobyte ✓ NFS - ==RBD== ✓ VsphereVolume ✓ PortworxVolume ✓ ScaleIO ✓ 对比上面可以得知，Ceph提供的RBD和CephFS都不能满足 Kubernetes 的Storageclass的Dynamic Provisioning + ReadWriteMany的需求，而Glusterfs则可以。 CephFS的Quota现在只有Ceph FUSE客户端支持，kernel client还不支持，这也许就是Storageclass还没支持CephFS的原因吧]]></content>
      <categories>
        <category>storage</category>
      </categories>
      <tags>
        <tag>rbd</tag>
        <tag>cephfs</tag>
        <tag>volume</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cephfs架构解读与测试分析-part1]]></title>
    <url>%2F2017%2F08%2F30%2Fcephfs-knowledge-share-part1%2F</url>
    <content type="text"><![CDATA[前几天有幸在日知录社区里分享了自己对CephFS的理解与CephFS的测试分析，然后把内容整理如下，因为内容比较多，保持与日知录社区的文章一致，这里先贴出第一部分。 若对我的分享比较感兴趣，可以访问链接：日知录 - CephFS架构解读与测试分析 CephFS架构解读cephFS简介CephFS是Ceph提供的兼容Posix协议的文件系统 ，对比RBD和RGW，它是最晚满足Production ready的一个功能。CephFS的底层还是使用rados存储数据，用MDS管理filesystem的元数据。 在Jewel版本里，CephFS的基本功能已经ready，但是很多feature还是experimental的，并不建议在生成环境打开这些feature。 CephFS特性 可扩展性 CephFS的client端是直接读写OSDs的，所以OSDs的扩展性也体现在CephFS中 共享文件系统 CephFS是一个共享的文件系统，支持多个clients同时读写文件系统的一个file 高可用性 CephFS支持配置元数据服务器集群，也可以配置为Active-Standby的主从服务器，提高可用性 文件/目录Layout配置 CephFS支持配置任一文件/目录的Layout，文件/目录若不单独配置，默认继承父目录的Layout属性 POSIX ACLs支持 CephFS支持POSIX的ACLs，在CephFS支持的两个client中，kernel client默认支持，FUSE client需要修改配置来支持 Quotas支持 CephFS在后端没有实现Quota的功能，它的Quota是在CephFS FUSE client端实现的，而kernel client里还没有添加 CephFS架构如下图所示： 最底层还是基础的OSDs和Monitors，添加了MDSs，上层是支持客户端的CephFS kernel object，CephFS FUSE，CephFS Library等。 CephFS组件间通信 如上图所示，CephFS各个组件间通信如下： Client &lt;–&gt; MDS 元数据操作和capalities Client &lt;–&gt; OSD 数据IO Client &lt;–&gt; Monitor 认证，集群map信息等 MDS &lt;–&gt; Monitor 心跳，集群map信息等 MDS &lt;–&gt; OSD 元数据IO Monitor &lt;–&gt; OSD 心跳，集群map信息等 CephFS MDS组件Ceph MDS设计的比较强大，作为一个能存储PB级数据的文件系统，它充分考虑了对元数据服务器的要求，设计了MDS集群。另外也引入了MDS的动态子树迁移，MDS的热度负载均衡。 但也正是这么超前的设计，使得MDS集群很难做到稳定，所以目前Jewel版本里默认还是单MDS实例，用户可配置主从MDS实例，提高可用性。 但在未来，MDS集群的这些属性都将稳定下来，为我们提供超强的元数据管理性能。 上图描述了CephFS的Dynamic subtree partition功能，它支持目录分片在多个MDS之间服务，并支持基于MDS负载均衡的动态迁移。 Client跟MDS通信后，会缓存对应的“目录-MDS”映射关系，这样Client任何时候都知道从哪个MDS上获取对应的元数据信息。 MDS自身的元数据有： per-MDS journal 每个MDS都有一个对应的journal文件，支持大到几百兆字节的size，保证元数据的一致性和顺序提交的性能，它也是直接存储到OSD cluster里的 CephFS MetaData MDS管理的CephFS的元数据也以文件格式存储到OSD cluster上，有些元数据信息会存到object的OMAP里 CephFS使用方式与通常的网络文件系统一样，要访问cephfs，需要有对应的client端。cephfs现在支持两种client端： CephFS kernel client since 2.6.34 CephFS FUSE Client端访问CepFS的步骤如下： client端与MDS节点通讯，获取metadata信息（metadata也存在osd上） client直接写数据到OSD Client端访问CephFS示例 Client发送open file请求给MDS MDS返回file inode，file size，capability和stripe信息 Client直接Read/Write数据到OSDs MDS管理file的capability Client发送close file请求给MDS，释放file的capability，更新file详细信息 这里cephfs并没有像其他分布式文件系统设计的那样，有分布式文件锁来保障数据一致性它是通过文件的capability来保证的 CephFS相关命令12345678910111213创建MDS Daemon# ceph-deploy mds create &lt;…&gt;创建CephFS Data Pool# ceph osd pool create &lt;…&gt;创建CephFS Metadata Pool# ceph osd pool create &lt;…&gt;创建CephFS# ceph fs new &lt;…&gt;查看CephFS# ceph fs ls name: tstfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]删除CephFS# ceph fs rm &lt;fs-name&gt; --yes-i-really-mean-it 查看MDS状态 12# ceph mds stat e8: tstfs-1/1/1 up tstfs2-0/0/1 up &#123;[tstfs:0]=mds-daemon-1=up:active&#125; e8 e表示epoch 8是epoch号 tstfs-1/1/1 up tstfs是cephfs名字 三个1分别是 mds_map.in/mds_map.up/mds_map.max_mds up是cephfs状态 {[tstfs:0]=mds-daemon-1=up:active} [tstfs:0]指tstfs的rank 0 mds-daemon-1是服务tstfs的mds daemon name up:active是cephfs的状态为 up &amp; active mount使用CephFS CephFS kernel client 12# mount -t ceph &lt;monitor ip&gt;:6789 /mntdir# umount /mntdir CephFS FUSE 12345安装ceph-fuse pkg# yum install -y ceph-fuse# ceph-fuse -m &lt;monitor ip&gt;:6789 /mntdir # fusermount -u /mntdircentos7里没有fusermount命令，可以用umount替代 FUSE的IO Path较长，会先从用户态调用到内核态，再返回到用户态使用CephFS FUSE模块访问Ceph集群，如下图所示： 对比 性能：Kernel client &gt; ceph-fuse Quota支持：只有ceph-fuse(client-side quotas) Quota不是在CephFS后端实现的，而是在client-side实现的。若某些应用中要求使用Quota，这时就必须考虑使用CephFS FUSE了 CephFS LayoutCephfs支持配置目录、文件的layout和stripe，这些元数据信息保存在目录和文件的xattr中。 目录的layout xattrs为：ceph.dir.layout 文件的layout xattrs为：ceph.file.layout CephFS支持的layout配置项有： pool 数据存储到指定pool namespace 数据存储到指定namespace，比pool更细的粒度(rbd/rgw/cephfs都还不支持) stripe_unit 条带大小，单位Byte stripe_count 条带个数 默认文件/目录继承父目录的layout和striping 示例： 配置一个目录的Layout 1# setfattr -n ceph.dir.layout -v "stripe_unit=524288 stripe_count=2 object_size=4194304 pool=cephfs_data2" /mnt/mike512K/ 目录中一个9MB的文件的layout分布图为： CephFS认证有时候可能应用有这样的需求，不同的用户访问不同的CephFS目录，这时候就需要开启CephFS的认证。 不过首先需要开启的是Ceph集群的认证，然后就可以创建CephFS认证的client端了，指定client对mon，mds，osd的访问权限。 开启Ceph集群认证12345配置ceph.conf# vim /etc/ceph/ceph.confauth_cluster_required = cephxauth_service_required = cephxauth_client_required = cephx 创建CephFS认证的client1234# ceph auth get-or-create client.*client_name* \ mon 'allow r' \ mds 'allow r, allow rw path=/*specified_directory*' \ osd 'allow rw pool=&lt;data pool&gt;’ 示例与解释： 1# ceph auth get-or-create client.tst1 mon ‘allow r’ mds ‘allow r, allow rw path=/tst1’ osd ‘allow rw pool=cephfs_data' mon ‘allow r’ 允许client从monitor读取数据；必须配置 mds ‘allow r, allow rw path=/tst1’ 允许client从MDS读取数据，允许client对目录/tst1读写； 其中‘ allow r’必须配置，不然client不能从mds读取数据，mount会报permission error； osd ‘allow rw pool=cephfs_data’ 允许client从osd pool=cephfs_data 上读写数据； 若不配置，client只能从mds上获取FS的元数据信息，没法查看各个文件的数据 对osd的权限也是必须配置的，不然client只能从mds上获取fs的元数据信息，没法查看文件的数据 检查ceph auth1234567# ceph auth get client.tst1 exported keyring for client.tst1 [client.tst] key = AQCd+UBZxpi4EBAAUNyBDGdZbPgfd4oUb+u41A== caps mds = allow r, allow rw path=/tst1" caps mon = "allow r" caps osd = "allow rw pool=cephfs_data" mount测试1# mount -t ceph &lt;ip&gt;:6789:/tst1 /mnt -o name=tst1,secret=AQCd+UBZxpi4EBAAUNyBDGdZbPgfd4oUb+u41A== 认证机制不完善CephFS的认证机制还不完善，上述client.tst1可以mount整个CephFS目录，能看到并读取整个CephFS的文件 1# mount -t ceph &lt;ip&gt;:6789:/ /mnt -o name=tst1,secret=AQCd+UBZxpi4EBAAUNyBDGdZbPgfd4oUb+u41A== 另外没找到能支持readonly访问某一目录的方法。 只验证了cephfs kernel client，没试过ceph-fuse的认证 CephFS的 FSCK &amp; RepairFS的fsck和repair对一个文件系统是非常重要的，如果这个没搞定，一个文件系统是不会有人敢用的。 Ceph在Jewel版本里提供了Ready的CephFS的scrub和repair工具，它哪能处理大部分的元数据损坏。但在使用中请记住两点： 修复命令==慎重执行==，需要专业人士操作 若命令支持，修复前请先备份元数据 cephfs journal工具cephfs-journal-tool，用来修复损坏的MDS journal，它支持的命令有下面几类，详细的介绍看下help就明白了。 1234cephfs-journal-tool： - inspect/import/export/reset - header get/set - event get/apply/recover_dentries/splice cephfs online check/scrub12345ceph tell mds.&lt;id&gt; damage ls ceph tell mds.&lt;id&gt; damage rm &lt;int&gt;# scrub an inode and output resultsceph mds mds.&lt;id&gt; scrub_path &lt;path&gt; &#123;force|recursive|repair [force|recursive|repair...]&#125; cephfs offline repair12345cephfs-data-scan init [--force-init] cephfs-data-scan scan_extents [--force-pool] &lt;data pool name&gt; cephfs-data-scan scan_inodes [--force-pool] [--force-corrupt] &lt;data pool name&gt;cephfs-data-scan scan_frags [--force-corrupt]cephfs-data-scan tmap_upgrade &lt;metadata_pool&gt;]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>cephfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开源存储系统罗列]]></title>
    <url>%2F2017%2F08%2F28%2Fstorage-system-summary%2F</url>
    <content type="text"><![CDATA[整理之前的文章时候，发现了这个，仅仅是一些存储系统的简单罗列，但对想熟悉开源存储系统的人来说也有些帮助。 理解有限，若有不对的地方，欢迎指正！ 分布式文件系统 名称 描述 存储功能 语言 官网 / WIKI 源码 Ceph 基于RADOS提供高扩展性、高可靠性、高性能的分布式文件系统 对象存储，块存储，文件存储 C++ http://ceph.com/ https://github.com/ceph/ceph Swift Openstack官方的对象存储系统 对象存储 Python https://wiki.openstack.org/wiki/Swift https://github.com/openstack/swift Cinder Openstack官方的块存储系统 块存储 Python https://wiki.openstack.org/wiki/Cinder https://github.com/openstack/cinder GlusterFS 分布式文件系统，弹性扩容 无metadata服务器 文件存储 C https://www.gluster.org/ https://github.com/gluster/glusterfs Lustre 一个大规模的、安全可靠的，具备高可用性的集群文件系统 文件存储 C http://lustre.org/ git clone git://git.hpdd.intel.com/fs/lustre-release.git Sheepdog 分布式块存储系统 块存储 C http://www.sheepdog-project.org/ https://github.com/sheepdog/sheepdog TFS 淘宝针对海量非结构化数据存储设计的分布式系统 文件存储 C++ http://tfs.taobao.org/ https://github.com/alibaba/tfs HDFS 基于GFS原理实现的java版的GFS 文件存储 Java https://wiki.apache.org/hadoop/HDFS https://github.com/apache/hadoop MooseFS 一个高容错性的分布式文件系统 文件存储 C http://moosefs.org/ https://github.com/moosefs/moosefs 参考：http://blog.csdn.net/metaxen/article/details/7108958 数据库 名称 描述 语言 官网 / WIKI 源码 LevelDB Google开发的，一个速度非常块的KV存储库 C++ http://leveldb.org/ https://github.com/google/leveldb RocksDB Facebook 的可嵌入式的支持持久化的key-value 存储系统 C++ http://rocksdb.org https://github.com/facebook/rocksdb Redis 开源的，支持网络、基于内存亦可持久化的日志型键－值仓储 C http://redis.io/ https://github.com/antirez/redis MongoDB 一个高性能，开源，无模式的文档型数据库 在许多场景下可用于替代传统的关系型数据库或键/值存储方式 C++ https://www.mongodb.com/ https://github.com/mongodb/mongo Cassandra 开源分布式NoSQL数据库系统 最大的特点就是完全去中心化 Go http://cassandra.apache.org/ https://github.com/apache/cassandra HBase 运行在hadoop上的数据库 一个分布式的，扩展性高的，存储大数据的数据库 Jave https://hbase.apache.org/ https://github.com/apache/hbase 其他 名称 描述 语言 官网 / WIKI 源码 Alluxio 一个开源内存级虚拟大数据存储系统 它是架构在底层分布式文件系统和上层分布式计算框架之间的一个中间件 主要职责是以文件形式在内存或其它存储设施中提供数据的存取服务 Java http://www.alluxio.org/ https://github.com/Alluxio/alluxio TiDB 基于Google Spanner，F1文章实现的开源版的NewSQL 类似于国外Google工程师出来创建的Startup公司 CockroachDB Go http://www.pingcap.com/ https://github.com/pingcap/tidb]]></content>
      <categories>
        <category>storage</category>
      </categories>
      <tags>
        <tag>storage system</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[公有云存储网关架构分析]]></title>
    <url>%2F2017%2F08%2F25%2Fcloud-storage-gateway%2F</url>
    <content type="text"><![CDATA[概述定义一种混合云解决方案，连接私有云/内部IT设施和公有云的Gateway，旨在帮助企业或个人实现本地存储与公有云存储的无缝衔接。提供本地数据远端存储到OSS上，即可以保持本地盘的性能，又可以拥有云上的大容量。 友商提供服务的友商有： AWS，阿里云，腾讯云，百度云 产品架构存储网关的大致架构如下： 产品功能 本地/云端部署 数据加密 数据压缩 缓存算法，优化本地访问性能 支持NFS/CIFS文件协议 支持ISCSI协议 带宽管理功能 日志功能 提供包含存储网关服务的虚拟机镜像 友商实现AWS介绍AWS Storage Gateway 是一种混合存储服务，您的内部应用程序可以借助它来无缝地使用 AWS 云中的存储。您可以使用该服务进行备份、存档、灾难恢复、云突增、存储分层和迁移。您的应用程序可以使用 NFS、iSCSI 等标准存储协议通过网关设备连接到该服务。网关会连接到 Amazon S3、Amazon Glacier、Amazon EBS 等 AWS 存储服务，这些服务为 AWS 中的文件、卷和虚拟磁带提供存储。该服务包含高度优化的数据传输机制，能够进行带宽管理、自动实现网络弹性、高效传输数据，并为活动数据的低延迟内部访问提供本地缓存。 链接：https://amazonaws-china.com/cn/storagegateway/ 使用场景 作为文件存储连接 File Gateway 配置通过 NFS 连接为本地服务器和应用程序提供网络文件共享。文件数据将缓存在 File Gateway 上以实现本地性能，并转换为对象存储在 Amazon S3 中。您可以通过本机 AWS 工具 (比如生命周期策略、版本控制和跨区域复制) 保护和归档对象。 作为块设备连接 Volume Gateway 配置将使用 iSCSI 作为本地磁盘连接到本地服务器和应用程序。这些卷中的数据可以传输到 Amazon S3 云存储中，并通过 Volume Gateway 进行访问。将数据存储在本地以获得最高性能 (同时将快照备份到云中)，或者通过将常用数据存储在本地并将不常用的数据存储在云中 (同时创建快照和克隆以实现保护)，平衡延迟和规模。 作为虚拟磁带库连接 Tape Gateway 已配置为用本地磁盘和云存储取代备份磁带和磁带自动化设备。现有的备份和恢复软件可将原生备份任务写入存储在 Tape Gateway 上的虚拟磁带中。虚拟磁带可迁移到 Amazon S3 中，最终存档到 Amazon Glacier 中，以便实现最低成本。数据通过您的备份应用程序进行访问，而且备份目录中仍然可以看到所有备份任务和磁带。 将数据传入传出AWS云 Storage Gateway 会自动缓存本地数据，并将其高效移入 (和移出) 云存储服务。这可以降低在您的办公地点和 AWS 云之间转移数据所需的时间和成本。分段管理、增量传输、带宽限制和带宽计划等优化功能是所有接口的标准功能。 特性 支持多个客户端类型：file、volume、虚拟磁带库等 虚拟设备既可以运行于本地，也能运行在 AWS 中 内置压缩、加密和带宽管理功能 支持数据存档到AWS Glacier中 使用步骤 从 AWS 管理控制台下载 AWS Storage Gateway 虚拟机 (VM) 映像 在所选的虚拟机监控程序下安装虚拟机：VMware ESXi 或 Microsoft Hyper-V 配置网关，从以下三种配置中进行选择：网关缓存卷、网关存储卷或网关虚拟磁带库 (VTL) 从直连式存储 (DAS)、网络附加存储 (NAS) 或网络区域存储 (SAN) 向已安装的本地网关分配本地存储。在网关缓存配置中，本地存储用于存储您经常访问的数据。在网关存储配置中，它将用于存储您的原始数据。在网关-VTL 配置中，此本地存储用于持久缓存上传到 AWS 的数据以及缓存最近读取的虚拟磁带数据 使用 AWS 管理控制台激活本地网关，方法是将网关的 IP 地址与 AWS 账户关联起来，并为网关选择 AWS 地区用于存储上传数据 对于网关缓存和网关存储配置，将会创建卷，并将这些卷作为 iSCSI 设备连接到您的本地应用程序服务器。对于网关-VTL，将虚拟磁带驱动器和虚拟介质更换器安装到备份服务器上，并让备份软件能够发现虚拟磁带库和虚拟磁带 阿里云介绍云存储网关是一款可在线下和云上部署的软网关，以阿里云上的OSS作为后端存储，可在企业的内部IT环境和基于云的存储基础设施之间提供无缝、安全的集成。支持行业标准的文件存储协议，并通过在本地缓存经常访问的数据来提供低延迟性能， 您可以安全地将数据存储在阿里云OSS中。 链接：https://www.aliyun.com/product/hcs 产品架构 特性 支持本地数据中心/云端ECS部署 基于OSS对象存储，提供NFS/CIFS文件协议 缓存算法支持缓存模式和写透模式 异步多线程上传，自动MD5校验 提供存储网关端的操作日志和云端OSS的访问日志 使用步骤本地数据中心部署步骤 1, 本地安装网关VM 不同的Hypervisor支持的安装方式和使用的安装文件都不相同，安装文件如下表所示： Hypervisor 支持的安装方式 安装文件格式 KVM 使用VM Manager的方式通过qcow2安装存储网关 qcow2 VMware 使用VM创建向导通过vmdk安装存储网关 vmdk VMware 使用OVF导入方式安装存储网关 ovf Xen 使用Xen的安装命令进行安装 raw 2, 登录网关VM并配置网络3, 配置远端OSS4, 激活存储网关4, 配置NFS/CIFS服务5, 开启缓存服务，设置缓存参数 云上部署步骤 用云存储网关镜像启动一ECS实例 ECS实例的最低要求是: 2核2GB内存 40GB系统盘 一个公网IP 安全组设置需要允许控制台连接 任意Linux操作系统 配置安全组 配置OSS 激活存储网关 腾讯云介绍存储网关（Cloud Storage Gateway）是一种混合云存储方案，旨在帮助企业或个人实现本地存储与公有云存储的无缝衔接。您无需关心多协议本地存储设备与云存储的兼容性，只需要在本地安装云存储网关即可实现混合云部署，并拥有媲美本地性能的海量云端存储。 链接：https://www.qcloud.com/product/csg 产品架构 缓存卷网关（iscsi） 把腾讯云上volume通过存储网关映射到本地使用呢，提升本地访问腾讯云上volume的性能。 存储卷网关（iscsi） 把本地volume的备份通过存储网关备份到腾讯云上。 文件网关（NFS） 把本地NFS存储通过存储网关扩展到腾讯云上。 特性 支持在本地安装存储网关 支持volume的映射扩展 仅支持本地NFS文件协议 本地缓存机制实现冷热数据分离 数据在本地加密压缩后传到云端 使用步骤 控制台创建存储网关，选择地区 选择存储网关类型：卷网关/文件网关 选择网关运行的平台：本地VMware VM上/腾讯云CVM上 VMware上部署：下载镜像文件到本地，部署到VMware虚拟机 CVM上部署：选择CVM地域和机型，选择存储网关镜像部署 连接到存储网关 激活存储网关 配置本地缓存磁盘 百度云介绍存储网关是一种混合云存储方案，可以无缝衔接本地和云端存储。存储网关基于对象存储BOS提供NFS和CIFS协议的文件存储服务，您从此无需担心本地应用与云存储间的协议兼容性。此外，存储网关也可以部署在云端，实现云主机间的文件共享。 链接：https://cloud.baidu.com/product/bsg.html 产品架构 混合云下存储扩展 本地IDC的存储空间往往是有限，通过使用存储网关，您可以像读写本地磁盘一样无缝使用云端的海量存储资源。存储网关可以部署在本地或者云端的虚拟机、物理机上。 云主机间文件共享 多台云主机间通常有文件共享的需求，例如应用服务器将日志存放在共享文件存储上以方便后续的日志集中处理和分析，负载均衡的后端云主机也可以将应用数据放在共享存储上从而实现云主机自身的无状态，方便搭建高可用业务架构。 特性 支持部署在本地或云端的虚拟机、物理机上 部署在云端，提供云主机间文件共享功能 内置本地高速缓存 提供NFS和CIFS协议的文件存储服务，数据持久化存储在BOS 使用步骤 创建存储网关：可以通过API/bgs.py脚本创建 当前存储网关还只能创建在默认VPC内，后续会支持指定VPC进行创建也既是只能在云主机上部署存储网关，还不支持本地部署 创建共享目标 一个存储网关上可以创建多个共享目标。一个共享目标对应一个BOS bucket 设置访问权限 使用存储网关：NFS/Samba挂载 总结作为本地存储到公有云的一种扩展，存储网关是混合云中很重要的一部分，在各大公有云厂商中都有提供。在提供存储网关的基本功能外，各个公有云厂商的存储网关功能会有些差异，详细参考下一节的功能对比。 功能对比对比友商的存储网关功能，列表如下： 对比项 AWS 阿里云 腾讯云 百度云 NFS √ √ √ √ CIFS x √ x √ ISCSI √ x √ x 虚拟磁带库 √ x √ x 缓存算法 缓存 缓存/写透 缓存 缓存 本地部署 √ √ √ √ 云端部署 x √ √ √ 后端存储 OSS/Glacier OSS OSS OSS 数据加密 √ x(OSS传输加密) √ x(OSS传输加密) 数据压缩 √ x √ x 操作日志 x √ x x 带宽管理 √ x √ x 自研 √ x(数梦工场) √ √ 存储网关基本功能参考存储网关的定义和上一节中的各个厂商存储网关的功能对比，我们可以总结出存储网关的基本功能如下，可作为我们实现存储网关的参考。 制作存储网关VM镜像 支持部署在本地 支持NFS文件协议 缓存算法 后端使用OSS存储 难点 NFS文件协议 支持完善的NFS文件协议，是个比较繁琐和困难的功能 缓存算法 如何实现效率高、缓存效果好的缓存算法是上述功能的难点 但实现第一版基础的缓存算法，还是有很多现成的算法可以参考的 考虑不完善，需要继续细化 存储网关附加功能在存储网关基本功能之上，可以考虑添加的附加功能有： 部署在云端虚拟机上 支持CIFS文件协议 支持数据加密、数据压缩 支持ISCSI 带宽管理 数据预取算法 难点 CIFS文件协议 支持完善的CIFS文件协议，是个比较繁琐和困难的功能 ISCSI卷映射 ISCSI卷的映射管理，快照功能都需要很多开发量 数据预取算法 实现高效、准确的数据预取算法对存储网关也是个难点，做好了也会是一个亮点]]></content>
      <categories>
        <category>storage</category>
      </categories>
      <tags>
        <tag>gateway</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据加密算法调研]]></title>
    <url>%2F2017%2F08%2F15%2Fdata-encryption-algorithm%2F</url>
    <content type="text"><![CDATA[数据加密数据加密，是一门历史悠久的技术，指通过加密算法和加密密钥将明文转变为密文，而解密则是通过解密算法和解密密钥将密文恢复为明文。它的核心是密码学。 数据加密目前仍是计算机系统对信息进行保护的一种最可靠的办法。它利用密码技术对信息进行加密，实现信息隐蔽，从而起到保护信息的安全的作用。 ==算法==：属性函数==秘钥==：可变参数（加密秘钥、解密秘钥） 加密系统中，算法是相对稳定的，而秘钥是可以改变的。 数据加密分类 对称加密 对称加密采用了对称密码编码技术，它的特点是文件加密和解密使用相同的密钥，即加密密钥也可以用作解密密钥 非对称加密 非对称加密需要两个密钥：公开密钥（publickey）和私有密 （privatekey）。公开密钥与私有密钥是一对，如果用公开密钥对数据进行加密，只有用对应的私有密钥才能解密；如果用私有密钥对数据进行加密，那么只有用对应的公开密钥才能解密 混合加密 对称加密 + 非对称加密 数据加密算法对称加密算法常用算法 DES(Data Encryption Standard) 一种对称加密标准，是目前使用最广泛的密钥系统，特别是在保护金融数据的安全中。曾是美国联邦政府的加密标准，但现已被AES所替代 3DES(Triple DES) 基于DES，对一块数据用三个不同的密钥进行三次加密，强度更高 AES(Advanced Encryption Standard) 又称Rijndael加密法，是美国联邦政府采用的一种区块加密标准，有AES-128, AES-192或AES-256算法 PBE(Password Base Encryption) 一种基于口令的加密算法，特点在于口令由用户自己掌握，采用随机数（我们这里叫做盐）杂凑多重加密等方法保证数据的安全性 对称加密算法的优点 算法公开 计算量小 加密速度快，加密效率高 对称加密算法的缺点 加解密双方需要使用相同的秘钥 秘钥管理很不方便，如果用户很多，那么秘钥的管理成几何性增长 任何一方秘钥泄露，数据都不安全 非对称加密算法常用算法 RSA RSA是目前最有影响力和最常用的公钥加密算法，它能够抵抗到目前为止已知的绝大多数密码攻击，已被ISO推荐为公钥数据加密标准 Elgamal 基于离散对数问题的加密算法，只能单向加解密，能用于数据加密和数字签名 D-H 基于计算离散对数的难度的秘钥交换算法，它使得通信的双方能在非安全的信道中安全的交换密钥，用于加密后续的通信消息，不用于消息的加解密 ECC Elliptic Curve Cryptography，基于椭圆曲线的一种加密算法 与RSA算法对比，它的安全性更高，处理速度更快，但由于它的数学理论非常深奥和复杂，在工程应用中比较难于实现 非对称加密算法的优点 一对公私钥，安全性更高 秘钥管理很方便 非对称加密算法的缺点 计算量大 加密速度慢，比对称加密算法慢1000倍左右 不适合大数据量加密 使用使用分类在实践中，结合对称加密和非对称加密算法的优缺点，可分为如下两种情况： 小数据量 直接使用非对称加密算法 大数据量 使用对称加密算法对数据加密，使用非对称加密算法加密对称加密算法中的秘钥 算法推荐 对称加密算法，推荐使用AES 非对称加密算法，推荐使用RSA和DH 秘钥长度根据需求决定 参考http://ch-tseng.blogspot.jp/http://liaoph.com/encrytion-and-openssl/http://deshui.wang/%E6%8A%80%E6%9C%AF/2016/01/10/security-3http://blog.csdn.net/zhengchao1991/article/details/53483357http://blog.csdn.net/zuozhiyoulaisam/article/details/54929661http://blog.csdn.net/zuozhiyoulaisam/article/details/54947402]]></content>
      <categories>
        <category>other</category>
      </categories>
      <tags>
        <tag>Encryption</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ceph Knowledges tree]]></title>
    <url>%2F2017%2F08%2F08%2Fceph-knowledges-tree%2F</url>
    <content type="text"><![CDATA[Ceph知识树很久之前看到过的一个ceph知识树，感觉不错，这里分享一下，很多都是存储相关的，也是自己的一个努力方向！ 另外因为ceph跟openstack结合紧密，融入了openstack的一些知识点，也值得所有做ceph的去了解熟悉下。 参考http://blog.csdn.net/scaleqiao/article/details/50243149]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql performance test on localdisk and rbd]]></title>
    <url>%2F2017%2F07%2F27%2Fcompare-mysql-on-localdisk-rbddisk%2F</url>
    <content type="text"><![CDATA[概述在我们的应用中，发现数据库上跑rbd的性能比较差，为此我对比测试了mysql的benchmark在本地 sata，ssd和rbd上的性能，并进行了分析比较。 测试工具sysbenchsysbench是一个模块化的、跨平台、多线程基准测试工具，主要用于评估测试各种不同系统参数下的数据库负载情况。关于这个项目的详细介绍请看：https://github.com/akopytov/sysbench sysbench的数据库OLTP测试支持MySQL、PostgreSQL、Oracle，目前主要用于Linux操作系统，开源社区已经将sysbench移植到了Windows，并支持SQL Server的基准测试。 所以这里选用sysbench测试mysql性能，mysql table engine选择innodb。 sysbench安装从 https://github.com/akopytov/sysbench 上下载sysbench源码，然后编译安装即可。 12345# yum install -y gcc gcc-c++ automake make libtool mysql-devel# ./autogen.sh# ./configure# make# make install 对比测试测试命令 12345678910111213141516171819202122232425262728293031323334353637383940#!/bin/bashhost=$1port=3306user=$2passwd=$3if [ $# -eq 5 ]; then port=$5fidb_name=sbtestdb_size=5000000thread_count=32duration=600#print a log and then exitfunction EXIT() &#123; [ $# -ne 0 ] &amp;&amp; [ "$1" != "" ] &amp;&amp; printf "$1\n" exit 1&#125;#create databasemysql -u$user -p$passwd -h $host -P $port -e "create database if not exists $db_name;"[ $? -eq 0 ] || EXIT "Create database FAILED!"# prepare datatime sysbench --oltp-table-size=$db_size --test=/root/mike/sysbench-master/tests/include/oltp_legacy/oltp.lua --mysql-table-engine=innodb --mysql-host=$host --mysql-port=$port --mysql-user=$user --mysql-password=$passwd --mysql-db=$db_name prepare[ $? -eq 0 ] || EXIT "Prepare data FAILED!"# start benchmarkif [ "$4" == "ro" ]; then time sysbench --max-time=$duration --max-requests=0 --num-threads=$thread_count --oltp-table-size=$db_size --test=/root/mike/sysbench-master/tests/include/oltp_legacy/oltp.lua --mysql-table-engine=innodb --mysql-host=$host --mysql-port=$port --mysql-user=$user --mysql-password=$passwd --mysql-db=$db_name --oltp-read-only=on runelse time sysbench --max-time=$duration --max-requests=0 --num-threads=$thread_count --oltp-table-size=$db_size --test=/root/mike/sysbench-master/tests/include/oltp_legacy/oltp.lua --mysql-table-engine=innodb --mysql-host=$host --mysql-port=$port --mysql-user=$user --mysql-password=$passwd --mysql-db=$db_name runfi[ $? -eq 0 ] || EXIT "Start benchmark FAILED!"# cleanup environmentsysbench --num-threads=$thread_count --oltp-table-size=$db_size --test=/root/mike/sysbench-master/tests/include/oltp_legacy/oltp.lua --mysql-table-engine=innodb --mysql-host=$host --mysql-port=$port --mysql-user=$user --mysql-password=$passwd --mysql-db=$db_name cleanup[ $? -eq 0 ] || EXIT "Cleanup FAILED!" 本地磁盘sata盘和ssd盘上的性能类似，结果如下： 123456789101112131415161718192021222324252627282930SQL statistics: queries performed: read: 40007562 write: 11430732 other: 5715366 total: 57153660 transactions: 2857683 (4762.67 per sec.) queries: 57153660 (95253.36 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.)General statistics: total time: 600.0172s total number of events: 2857683Latency (ms): min: 1.37 avg: 6.72 max: 100.65 95th percentile: 17.01 sum: 19193736.29Threads fairness: events (avg/stddev): 89302.5938/256.06 execution time (avg/stddev): 599.8043/0.00real 10m0.030suser 11m12.564ssys 9m3.083s rbd设备准备rbd设备命令： 12# rbd create -p tstpool foxvol --size 30720 --image-format 1# rbd map tstpool/foxvol 针对应用这里选择的image format为1，后来测试了image format为2的volume，性能一样 在设备/dev/rbd0上测试结果如下： 123456789101112131415161718192021222324252627282930SQL statistics: queries performed: read: 14509516 write: 4145576 other: 2072788 total: 20727880 transactions: 1036394 (1727.28 per sec.) queries: 20727880 (34545.51 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.)General statistics: total time: 600.0166s total number of events: 1036394Latency (ms): min: 2.02 avg: 18.52 max: 460.01 95th percentile: 63.32 sum: 19197876.44Threads fairness: events (avg/stddev): 32387.3125/109.50 execution time (avg/stddev): 599.9336/0.01real 10m0.028suser 3m52.788ssys 3m17.306s 分析raid卡查看本地盘的配置，本地盘的raid卡配置的写是writeback模式，缓存有2G大小。测试机器上总共有两种raid卡： raid卡1 1234567891011# hpssacli ctrl all show config detail...Cache Board Present: TrueCache Status: OKCache Ratio: 10% Read / 90% WriteDrive Write Cache: DisabledTotal Cache Size: 2.0 GBTotal Cache Memory Available: 1.8 GBNo-Battery Write Cache: DisabledSSD Caching RAID5 WriteBack Enabled: TrueSSD Caching Version: 2 raid卡2 123456789101112131415# /opt/MegaRAID/MegaCli/MegaCli64 -cfgdsply -aALL ==============================================================================Adapter: 0Product Name: PERC H730P MiniMemory: 2048MBBBU: PresentSerial No: 57E00PQ==============================================================================...Default Cache Policy: WriteBack, ReadAhead, Direct, No Write Cache if Bad BBUCurrent Cache Policy: WriteBack, ReadAhead, Direct, No Write Cache if Bad BBUDefault Access Policy: Read/WriteCurrent Access Policy: Read/WriteDisk Cache Policy : Disk's Default 性能测试写IOPSdd命令测试写IOPS性能 测试本地sata盘，发现raid卡能提升本地盘的IOPS到 20000 多； 123456789101112131415# dd if=/dev/zero of=tstfile bs=4k count=1024000 oflag=direct &amp;# iostat -kx 2 sdk...avg-cpu: %user %nice %system %iowait %steal %idle 0.16 0.00 0.71 2.37 0.00 96.77 Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %utilsdk 0.00 0.00 0.00 26587.00 0.00 106348.00 8.00 0.85 0.03 0.00 0.03 0.03 85.35 avg-cpu: %user %nice %system %iowait %steal %idle 0.14 0.00 0.72 2.37 0.00 96.77 Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %utilsdk 0.00 0.00 0.00 26706.50 0.00 106826.00 8.00 0.84 0.03 0.00 0.03 0.03 84.35... 测试rbd设备，IOPS只有1000多点； 123456789101112131415# dd if=/dev/zero of=/dev/rbd1 bs=4k count=1024000 oflag=direct &amp;#iostat -kx 2 rbd1...avg-cpu: %user %nice %system %iowait %steal %idle 0.19 0.00 0.50 2.94 0.00 96.37 Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %utilrbd1 0.00 0.00 0.00 1135.50 0.00 4542.00 8.00 0.97 0.86 0.00 0.86 0.86 97.45 avg-cpu: %user %nice %system %iowait %steal %idle 0.86 0.00 0.63 2.93 0.00 95.58 Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %utilrbd1 0.00 0.00 0.00 1193.50 0.00 4774.00 8.00 0.98 0.82 0.00 0.82 0.82 97.85... 分析：rbd设备的所有IO是要走网络和ceph IO栈的，IO的平均时延都是ms级别的，所以每秒的IOPS也就1000多点。。。 查看网络时延 1234567[root@tstserver yangguanjun]# ping 10.10.1.5PING 10.10.1.5 (10.10.1.5) 56(84) bytes of data.64 bytes from 10.10.1.5: icmp_seq=1 ttl=63 time=0.191 ms64 bytes from 10.10.1.5: icmp_seq=2 ttl=63 time=0.157 ms64 bytes from 10.10.1.5: icmp_seq=3 ttl=63 time=0.151 ms64 bytes from 10.10.1.5: icmp_seq=4 ttl=63 time=0.147 ms64 bytes from 10.10.1.5: icmp_seq=5 ttl=63 time=0.164 ms 网络的时延也是ms级别的，就算没有ceph io的时延，这个iops也只能是5k多，所以针对mysql这样的应用，rbd的性能会比较低；]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>rbd</tag>
        <tag>mysql</tag>
        <tag>sysbench</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cephfs测试中IO hang问题分析]]></title>
    <url>%2F2017%2F07%2F18%2Fcephfs-io-hang-analysis%2F</url>
    <content type="text"><![CDATA[问题为了测试cephfs的可用性，我们对cephfs进行了几组stripe的测试，在跑自动化测试时，发现fio总是会跑一部分后hang住，而此时ceph -s输出显示read值太大，但实际环境中查看并没这么大的流量。 ceph输出123456789101112# ceph -w cluster 16f59233-16da-4864-8c5a-1fec71d119ad health HEALTH_OK monmap e1: 3 mons at &#123;cephcluster-server1=10.10.1.6:6789/0,cephcluster-server2=10.10.1.7:6789/0,cephcluster-server3=10.10.1.8:6789/0&#125; election epoch 12, quorum 0,1,2 cephcluster-server1,cephcluster-server2,cephcluster-server3 fsmap e17: 1/1/1 up &#123;0=mds-daemon-38=up:active&#125;, 1 up:standby osdmap e180: 30 osds: 30 up, 30 in flags sortbitwise,require_jewel_osds pgmap v55504: 1088 pgs, 3 pools, 81924 MB data, 10910 objects 241 GB used, 108 TB / 109 TB avail 1088 active+clean client io 68228 MB/s rd, 1066 op/s rd, 0 op/s wr 分析：cephfs client io显示的信息是正确的。从后面的分析得知cephfs client一直loop着尝试read object，单次read就有64MB，object数据缓存在osd的内存里，所以ceph统计的read速率很大。 查看网络用iftop看网络的流量只有10MB/s 12345# iftop -B -i eth4 25.5MB 50.9MB 76.4MB 102MB 127MB└──────────────────────────┴─────────────────────────┴─────────────────────────┴─────────────────────────┴──────────────────────────cephcluster-server2.****.com =&gt; 10.10.1.18 4.46MB 4.51MB 4.52MB &lt;= 220KB 221KB 222KB 对比几个stripe发现对64MB的无条带化的object读时，会触发这个bug： 123# getfattr -n ceph.file.layout dir4/tstfile# file: dir4/tstfileceph.file.layout="stripe_unit=67108864 stripe_count=1 object_size=67108864 pool=cephfs_data” bs=64M的fio hang住 1234567# fio -filename=/home/yangguanjun/cephfs/dir4/tstfile -size=20G -thread -group_reporting -direct=1 -ioengine=libaio -bs=64M -rw=read -iodepth=64 -iodepth_batch_submit=8 -iodepth_batch_complete=8 -name=write_64k_64qwrite_64k_64q: (g=0): rw=read, bs=64M-64M/64M-64M/64M-64M, ioengine=libaio, iodepth=64fio-2.2.8Starting 1 thread^Cbs: 1 (f=1): [R(1)] [inf% done] [0KB/0KB/0KB /s] [0/0/0 iops] [eta 1158050441d:06h:59m:55s]fio: terminating on signal 2... bs=32M的fio hang住 12345# fio -filename=/home/yangguanjun/cephfs/dir4/tstfile -size=20G -thread -group_reporting -direct=1 -ioengine=libaio -bs=32M -rw=read -iodepth=64 -iodepth_batch_submit=8 -iodepth_batch_complete=8 -name=write_64k_64qwrite_64k_64q: (g=0): rw=read, bs=32M-32M/32M-32M/32M-32M, ioengine=libaio, iodepth=64fio-2.2.8Starting 1 thread^Cbs: 1 (f=1): [R(1)] [inf% done] [0KB/0KB/0KB /s] [0/0/0 iops] [eta 1158050441d:07h:00m:09s] bs=16M的dd hang住 123# dd if=tstfile of=/dev/null bs=16M count=200root 12763 0.0 0.0 124344 664 pts/0 D+ 14:29 0:00 dd if=tstfile of=/dev/null bs=16M count=200 1234567891011121314151617# ceph -w cluster 16f59233-16da-4864-8c5a-1fec71d119ad health HEALTH_OK monmap e1: 3 mons at &#123;cephcluster-server1=10.10.1.6:6789/0,cephcluster-server2=10.10.1.7:6789/0,cephcluster-server3=10.10.1.8:6789/0&#125; election epoch 12, quorum 0,1,2 cephcluster-server1,cephcluster-server2,cephcluster-server3 fsmap e23: 1/1/1 up &#123;0=mds-daemon-37=up:active&#125; osdmap e183: 30 osds: 30 up, 30 in flags sortbitwise,require_jewel_osds pgmap v63806: 1088 pgs, 3 pools, 93200 MB data, 13729 objects 274 GB used, 108 TB / 109 TB avail 1088 active+clean client io 1330 MB/s rd, 41 op/s rd, 0 op/s wr2017-07-05 14:30:11.483974 mon.0 [INF] pgmap v63806: 1088 pgs: 1088 active+clean; 93200 MB data, 274 GB used, 108 TB / 109 TB avail; 1330 MB/s rd, 41 op/s2017-07-05 14:30:16.484856 mon.0 [INF] pgmap v63807: 1088 pgs: 1088 active+clean; 93200 MB data, 274 GB used, 108 TB / 109 TB avail; 1212 MB/s rd, 37 op/s2017-07-05 14:30:21.485295 mon.0 [INF] pgmap v63808: 1088 pgs: 1088 active+clean; 93200 MB data, 274 GB used, 108 TB / 109 TB avail; 1206 MB/s rd, 37 op/s2017-07-05 14:30:26.485953 mon.0 [INF] pgmap v63809: 1088 pgs: 1088 active+clean; 93200 MB data, 274 GB used, 108 TB / 109 TB avail; 1212 MB/s rd, 37 op/s ceph cluster节点信息在ceph cluster的一个节点上看到测试节点有读数据请求 123456789101112131415161718# iftop -n -i eth410.10.1.7 =&gt; 10.10.1.18 12.3Mb 11.6Mb 11.6Mb &lt;= 285Kb 284Kb 284Kb打开source port后10.10.1.7:6810 =&gt; 10.10.1.18 12.5Mb 11.5Mb 11.5Mb &lt;= 278Kb 255Kb 255Kb# netstat -nap | grep 6810tcp 0 0 10.10.1.7:6810 0.0.0.0:* LISTEN 79360/ceph-osdtcp 0 0 10.10.1.7:6810 0.0.0.0:* LISTEN 79360/ceph-osdtcp 0 0 10.10.1.7:6810 10.10.1.7:57266 ESTABLISHED 79360/ceph-osd# ps aux | grep -w 79360ceph 79360 85.9 0.5 4338588 709312 ? Ssl Jul04 1556:54 /usr/bin/ceph-osd -f --cluster ceph --id 15 --setuser ceph --setgroup ceph# df/dev/sdg1 3905070088 9079428 3895990660 1% /var/lib/ceph/osd/ceph-15 查看sdg没有io 1234567891011121314151617181920# iostat -kx 2 sdgLinux 3.10.0-514.10.2.el7.x86_64 (cephcluster-server2.****.com) 07/05/2017 _x86_64_ (32 CPU)avg-cpu: %user %nice %system %iowait %steal %idle 12.47 0.00 1.64 0.03 0.00 85.86Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %utilsdg 0.01 0.07 2.48 2.01 95.78 140.98 105.44 0.08 17.79 9.69 27.79 3.44 1.54avg-cpu: %user %nice %system %iowait %steal %idle 0.80 0.00 2.96 0.02 0.00 96.22Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %utilsdg 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00avg-cpu: %user %nice %system %iowait %steal %idle 0.86 0.00 2.99 0.00 0.00 96.15Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %utilsdg 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 查看文件location测试文件的location信息，正好第一块数据就在osd 15上 12345678910111213# cephfs tstfile show_locationWARNING: This tool is deprecated. Use the layout.* xattrs to query and modify layouts.location.file_offset: 0location.object_offset:0location.object_no: 0location.object_size: 67108864location.object_name: 10000000805.00000000location.block_offset: 0location.block_size: 67108864location.osd: 15# ceph osd map cephfs_data 10000000805.00000000osdmap e188 pool 'cephfs_data' (1) object '10000000805.00000000' -&gt; pg 1.12d00fd5 (1.1d5) -&gt; up ([15,0,26], p15) acting ([15,0,26], p15) 停止ceph osd 15 1# systemctl stop ceph-osd@15.service 过一会查看osd 15所在节点没有与测试节点的数据流量了这时在ceph osd 0节点，出现了与测试节点的数据流量，查看发现正好是osd 0与测试节点的数据通信查看此时测试文件第一个object的map信息如下： 12# ceph osd map cephfs_data 10000000805.00000000osdmap e185 pool 'cephfs_data' (1) object '10000000805.00000000' -&gt; pg 1.12d00fd5 (1.1d5) -&gt; up ([0,26], p0) acting ([0,26], p0) 问题明确现在这个问题很好重现了 创建一个 stripe_unit=67108864 stripe_count=1 object_size=67108864 的文件 dd if=/dev/zero of=foxfile bs=64M count=1 写成功 dd if=foxfile of=/dev/null bs=64M count=1 读就会hang住 查看测试节点与对应的object所在的osd有数据流量 通过strace命令查看dd读数据时确实hang在read函数： 1234567891011# strace dd if=foxfile of=/dev/null bs=64M count=1...open("foxfile", O_RDONLY) = 3dup2(3, 0) = 0close(3) = 0lseek(0, 0, SEEK_CUR) = 0open("/dev/null", O_WRONLY|O_CREAT|O_TRUNC, 0666) = 3dup2(3, 1) = 1close(3) = 0mmap(NULL, 67121152, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f01712d8000read(0, 收集client log参考文章 cephfs kernel client debug 收集osd log1# ceph daemon /var/run/ceph/ceph-osd.&lt;osdid&gt;.asok config set debug_osd "20/20" 分析log在cephfs client的log中能循环发现如下： 12345678910111213141516171819Jul 6 11:30:29 cephfs-client kernel: [ 2010.231008] ceph_sock_data_ready on ffff88202c987030 state = 5, queueing workJul 6 11:30:29 cephfs-client kernel: [ 2010.231012] get_osd ffff88202c987000 3 -&gt; 4Jul 6 11:30:29 cephfs-client kernel: [ 2010.231015] queue_con_delay ffff88202c987030 0Jul 6 11:30:29 cephfs-client kernel: [ 2010.231034] try_read start on ffff88202c987030 state 5Jul 6 11:30:29 cephfs-client kernel: [ 2010.231035] try_read tag 1 in_base_pos 0Jul 6 11:30:29 cephfs-client kernel: [ 2010.231036] try_read got tag 8Jul 6 11:30:29 cephfs-client kernel: [ 2010.231036] prepare_read_ack ffff88202c987030Jul 6 11:30:29 cephfs-client kernel: [ 2010.231041] ceph_sock_data_ready on ffff88202c987030 state = 5, queueing workJul 6 11:30:29 cephfs-client kernel: [ 2010.231041] get_osd ffff88202c987000 4 -&gt; 5Jul 6 11:30:29 cephfs-client kernel: [ 2010.231053] queue_con_delay ffff88202c987030 0Jul 6 11:30:29 cephfs-client kernel: [ 2010.231062] got ack for seq 1 type 42 at ffff881008668100Jul 6 11:30:29 cephfs-client kernel: [ 2010.231066] ceph_msg_put ffff881008668100 (was 2)Jul 6 11:30:29 cephfs-client kernel: [ 2010.231081] prepare_read_tag ffff88202c987030Jul 6 11:30:29 cephfs-client kernel: [ 2010.231083] try_read start on ffff88202c987030 state 5Jul 6 11:30:29 cephfs-client kernel: [ 2010.231086] try_read tag 1 in_base_pos 0Jul 6 11:30:29 cephfs-client kernel: [ 2010.231088] try_read got tag 7Jul 6 11:30:29 cephfs-client kernel: [ 2010.231088] prepare_read_message ffff88202c987030Jul 6 11:30:29 cephfs-client kernel: [ 2010.231089] read_partial_message con ffff88202c987030 msg (null)Jul 6 11:30:29 cephfs-client kernel: [ 2010.231090] try_read done on ffff88202c987030 ret -5 证明cephfs client一直在尝试读取osd数据，但收到数据后却认为msg为null 在ceph osd的log中能循环发现如下： 123456789101112131415162017-07-06 11:30:29.185056 7f1788eb2700 1 osd.20 188 ms_handle_reset con 0x7f1748c39d80 session 0x7f170f4f31c02017-07-06 11:30:29.185683 7f17719fc700 10 osd.20 188 new session 0x7f174d435380 con=0x7f1748c39f00 addr=10.10.1.18:0/1443241332017-07-06 11:30:29.185839 7f17719fc700 20 osd.20 188 should_share_map client.24357 10.10.1.18:0/144324133 1882017-07-06 11:30:29.185854 7f17719fc700 15 osd.20 188 enqueue_op 0x7f174d42f100 prio 127 cost 0 latency 0.000047 osd_op(client.24357.1:3 1.1bdf9243 1000000095b.00000000 [read 0~67108864 [1@-1]] snapc 0=[] RETRY=6 ondisk+retry+read+known_if_redirected e188) v42017-07-06 11:30:29.185891 7f1777bff700 10 osd.20 188 dequeue_op 0x7f174d42f100 prio 127 cost 0 latency 0.000085 osd_op(client.24357.1:3 1.1bdf9243 1000000095b.00000000 [read 0~67108864 [1@-1]] snapc 0=[] RETRY=6 ondisk+retry+read+known_if_redirected e188) v4 pg pg[1.43( v 188'6597 (180'3500,188'6597] local-les=157 n=10 ec=136 les/c/f 157/157/0 151/156/136) [20,10,7] r=0 lpr=156 crt=188'6595 lcod 188'6596 mlcod 188'6596 active+clean]2017-07-06 11:30:29.185937 7f1777bff700 10 osd.20 pg_epoch: 188 pg[1.43( v 188'6597 (180'3500,188'6597] local-les=157 n=10 ec=136 les/c/f 157/157/0 151/156/136) [20,10,7] r=0 lpr=156 crt=188'6595 lcod 188'6596 mlcod 188'6596 active+clean] handle_message: 0x7f174d42f1002017-07-06 11:30:29.185951 7f1777bff700 20 osd.20 pg_epoch: 188 pg[1.43( v 188'6597 (180'3500,188'6597] local-les=157 n=10 ec=136 les/c/f 157/157/0 151/156/136) [20,10,7] r=0 lpr=156 crt=188'6595 lcod 188'6596 mlcod 188'6596 active+clean] op_has_sufficient_caps pool=1 (cephfs_data ) owner=0 need_read_cap=1 need_write_cap=0 need_class_read_cap=0 need_class_write_cap=0 -&gt; yes2017-07-06 11:30:29.185970 7f1777bff700 10 osd.20 pg_epoch: 188 pg[1.43( v 188'6597 (180'3500,188'6597] local-les=157 n=10 ec=136 les/c/f 157/157/0 151/156/136) [20,10,7] r=0 lpr=156 crt=188'6595 lcod 188'6596 mlcod 188'6596 active+clean] do_op osd_op(client.24357.1:3 1.1bdf9243 1000000095b.00000000 [read 0~67108864 [1@-1]] snapc 0=[] RETRY=6 ondisk+retry+read+known_if_redirected e188) v4 may_read -&gt; read-ordered flags ondisk+retry+read+known_if_redirected2017-07-06 11:30:29.185990 7f1777bff700 10 osd.20 pg_epoch: 188 pg[1.43( v 188'6597 (180'3500,188'6597] local-les=157 n=10 ec=136 les/c/f 157/157/0 151/156/136) [20,10,7] r=0 lpr=156 crt=188'6595 lcod 188'6596 mlcod 188'6596 active+clean] get_object_context: found obc in cache: 0x7f171e4231802017-07-06 11:30:29.185997 7f1777bff700 10 osd.20 pg_epoch: 188 pg[1.43( v 188'6597 (180'3500,188'6597] local-les=157 n=10 ec=136 les/c/f 157/157/0 151/156/136) [20,10,7] r=0 lpr=156 crt=188'6595 lcod 188'6596 mlcod 188'6596 active+clean] get_object_context: 0x7f171e423180 1:c249fbd8:::1000000095b.00000000:head rwstate(none n=0 w=0) oi: 1:c249fbd8:::1000000095b.00000000:head(188'4499 mds.0.20:66259 dirty|data_digest|omap_digest s 67108864 uv 4499 dd cdba94a2 od ffffffff) ssc: 0x7f172dcfce40 snapset: 1=[]:[]+head2017-07-06 11:30:29.186010 7f1777bff700 10 osd.20 pg_epoch: 188 pg[1.43( v 188'6597 (180'3500,188'6597] local-les=157 n=10 ec=136 les/c/f 157/157/0 151/156/136) [20,10,7] r=0 lpr=156 crt=188'6595 lcod 188'6596 mlcod 188'6596 active+clean] find_object_context 1:c249fbd8:::1000000095b.00000000:head @head oi=1:c249fbd8:::1000000095b.00000000:head(188'4499 mds.0.20:66259 dirty|data_digest|omap_digest s 67108864 uv 4499 dd cdba94a2 od ffffffff)2017-07-06 11:30:29.186026 7f1777bff700 10 osd.20 pg_epoch: 188 pg[1.43( v 188'6597 (180'3500,188'6597] local-les=157 n=10 ec=136 les/c/f 157/157/0 151/156/136) [20,10,7] r=0 lpr=156 crt=188'6595 lcod 188'6596 mlcod 188'6596 active+clean] execute_ctx 0x7f1776beed002017-07-06 11:30:29.186035 7f1777bff700 10 osd.20 pg_epoch: 188 pg[1.43( v 188'6597 (180'3500,188'6597] local-les=157 n=10 ec=136 les/c/f 157/157/0 151/156/136) [20,10,7] r=0 lpr=156 crt=188'6595 lcod 188'6596 mlcod 188'6596 active+clean] do_op 1:c249fbd8:::1000000095b.00000000:head [read 0~67108864 [1@-1]] ov 188'44992017-07-06 11:30:29.186041 7f1777bff700 10 osd.20 pg_epoch: 188 pg[1.43( v 188'6597 (180'3500,188'6597] local-les=157 n=10 ec=136 les/c/f 157/157/0 151/156/136) [20,10,7] r=0 lpr=156 crt=188'6595 lcod 188'6596 mlcod 188'6596 active+clean] taking ondisk_read_lock2017-07-06 11:30:29.186047 7f1777bff700 10 osd.20 pg_epoch: 188 pg[1.43( v 188'6597 (180'3500,188'6597] local-les=157 n=10 ec=136 les/c/f 157/157/0 151/156/136) [20,10,7] r=0 lpr=156 crt=188'6595 lcod 188'6596 mlcod 188'6596 active+clean] do_osd_op 1:c249fbd8:::1000000095b.00000000:head [read 0~67108864 [1@-1]]2017-07-06 11:30:29.186056 7f1777bff700 10 osd.20 pg_epoch: 188 pg[1.43( v 188'6597 (180'3500,188'6597] local-les=157 n=10 ec=136 les/c/f 157/157/0 151/156/136) [20,10,7] r=0 lpr=156 crt=188'6595 lcod 188'6596 mlcod 188'6596 active+clean] do_osd_op read 0~67108864 [1@-1] 分析看出cephfs client发送read 64MB的request，ceph osd读64M数据return，但是cephfs client认为msg为null，然后重试一直重复上面的请求，所以cephfs client的IO就hang住了 提交bug提交该bug到ceph社区：http://tracker.ceph.com/issues/20528 cephfs client端的配置限制了read message的最大size为16M。 in net/ceph/messenger.c 123456789101112static int read_partial_message(struct ceph_connection *con)&#123;... front_len = le32_to_cpu(con-&gt;in_hdr.front_len); if (front_len &gt; CEPH_MSG_MAX_FRONT_LEN) return -EIO; middle_len = le32_to_cpu(con-&gt;in_hdr.middle_len); if (middle_len &gt; CEPH_MSG_MAX_MIDDLE_LEN) return -EIO; data_len = le32_to_cpu(con-&gt;in_hdr.data_len); if (data_len &gt; CEPH_MSG_MAX_DATA_LEN) return -EIO; 所以我们使用cephfs中，不能配置文件的stripe_unit大于16M。]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>cephfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cephfs的测试简报]]></title>
    <url>%2F2017%2F07%2F16%2Fcephfs-test-method-lite%2F</url>
    <content type="text"><![CDATA[cephfs简介cephfs是ceph提供的兼容POSIX协议的文件系统，对比rbd和rgw功能，这个是ceph里最晚满足production ready的一个功能，它底层还是使用rados存储数据 cephfs的架构 使用cephfs的两种方式 cephfs kernel module cephfs-fuse 从上面的架构可以看出，cephfs-fuse的IO path比较长，性能会比cephfs kernel module的方式差一些； client端访问cephfs的流程 client端与mds节点通讯，获取metadata信息（metadata也存在osd上） client直接写数据到osd cephfs测试环境三台物理机搭建了ceph集群，配置了Active-Standby的MDS，作为测试cephfs的cluster，详细配置如下： 模块 版本 Ceph Version Jewel 10.2.7 Ceph Cluster OS CentOS Linux release 7.2.1511 (Core) Ceph Cluster kernel version 3.10.0-327.el7.x86_64 Cephfs Client OS CentOS Linux release 7.2.1511 (Core) Cephfs kernel version 4.11.3-1.el7.elrepo.x86_64 三台物理机上，每个上面有10个4T 7200RPM SATA盘，两个480GB的SATA SSD盘做journal设备，每个SSD盘分出5个20GB的分区做5个OSD的journal。三个物理机上部署了三个monitor，其中两台部署了两个MDS。 Ceph默认配置replica=3，所以三台物理机组成的Ceph Cluster的整体写性能约等于一台物理机上的硬件的性能。 每个物理机上，2个SSD做10个OSD的journal，其整体性能约为：2 * (单个ssd盘的性能) 或 10 * (单个sata盘的性能) 使用的SSD盘型号为：Intel S3500系列，其性能指标为： 参数 性能 容量 480GB 顺序读取(最高) 500 MB/s 顺序写入(最高) 410 MB/s 随机读取(100% 跨度) 75000 IOPS 随机写入(100% 跨度) 11000 IOPS 创建mds使用ceph-deploy部署ceph mds很方便，只需要简单的一条命令就搞定，不过它依赖之前ceph-deploy时候生成的一些配置和keyring文件；在之前部署ceph集群的节点目录，执行ceph-deploy mds create： 12345# ceph-deploy --overwrite-conf mds create cephfs-host2:mds-daemon-37# ceph-deploy --overwrite-conf mds create cephfs-host3:mds-daemon-38// 去节点检查下daemon[root@cephfs-host2 yangguanjun]# ps ax | grep ceph-mds 283564 ? Ssl 0:38 /usr/bin/ceph-mds -f --cluster ceph --id mds-daemon-37 --setuser ceph --setgroup ceph 创建测试cephfs在上述的测试集群里搭建cephfs，因为都是内部使用，测试集群没有打开ceph的认证。 创建cephfs的步骤如下 12345678# ceph osd pool create cephfs_data 512 512 // 创建data poolpool 'cephfs_data' created# ceph osd pool create cephfs_metadata 512 512 // 创建metadata poolpool 'cephfs_metadata' created# ceph fs new tstfs cephfs_metadata cephfs_data // 创建cephfsnew fs with metadata pool 1 and data pool 2# ceph fs lsname: tstfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ] 查看cephfs对应的active MDS 12# ceph mds state229: 1/1/1 up &#123;0=mds-daemon-37=up:active&#125;, 1 up:standby cephfs测试为了评估cephfs的稳定性和性能，结合常用的测试场景和测试工具，我们对cephfs的测试进行了分类，然后在选定的另一台测试节点上mount并测试cephfs。 在测试节点mount上cephfs 1[root@cephfs-client yangguanjun]# mount -t ceph 10.1.1.6:6789:/ /home/yangguanjun/cephfs/ cephfs功能测试cephfs是兼容POSIX协议的文件系统，这里通过手动和自动测试工具的方式来验证cephfs的可用性。 手动模式通过手动模式测试常见的文件系统操作 1234567891011121314151617181920212223242526272829303132创建测试dir和file# cd /home/yangguanjun/cephfs/# mkdir tstdir# cd tstdir# touch tstfile测试file的读写# echo "hello cephfs" &gt; tstfile# cat tstfilehello cephfs测试file的chmod# ll tstfile-rw-r--r-- 1 root root 13 Jul 7 10:33 tstfile# chmod 755 tstfile# ll tstfile-rwxr-xr-x 1 root root 13 Jul 7 10:33 tstfile测试file的chown# chown yangguanjun:yangguanjun tstfile# ll tstfile-rwxr-xr-x 1 yangguanjun yangguanjun 13 Jul 7 10:33 tstfile测试file的symlink# ln -s tstfile lnfile# ll lnfilelrwxrwxrwx 1 root root 7 Jul 7 10:31 lnfile -&gt; tstfile删除测试dir和file# rm -f tstfile# cd ../# rm -rf tstdir 自动测试工具这里使用fstest测试工具对cephfs进行测试。 fstest是一套简化版的文件系统POSIX兼容性测试套件，它可以工作在FreeBSD, Solaris, Linux上用于测试UFS, ZFS, ext3, XFS and the NTFS-3G等文件系统。fstest目前有3601个回归测试用例，测试的系统调用覆盖chmod, chown, link, mkdir, mkfifo, open, rename, rmdir, symlink, truncate, unlink。 获取fstest 12网站：http://www.tuxera.com/community/posix-test-suite/# wget http://tuxera.com/sw/qa/pjd-fstest-20090130-RC.tgz 安装fstest 12345# yum install -y libacl-devel# tar -zxf pjd-fstest-20090130-RC.tgz# cd pjd-fstest-20090130-RC# makegcc -Wall -DHAS_ACL -lacl fstest.c -o fstest 运行fstest 1234567891011121314151617181920212223242526272829303132333435363738394041424344# cd /home/yangguanjun/cephfs/# prove -r /home/yangguanjun/pjd-fstest-20090130-RC//home/yangguanjun/pjd-fstest-20090130-RC/tests/chflags/00.t ... ok.../home/yangguanjun/pjd-fstest-20090130-RC/tests/chflags/13.t ... ok/home/yangguanjun/pjd-fstest-20090130-RC/tests/chmod/00.t ..... ok.../home/yangguanjun/pjd-fstest-20090130-RC/tests/chmod/11.t ..... ok/home/yangguanjun/pjd-fstest-20090130-RC/tests/chown/00.t ..... ok.../home/yangguanjun/pjd-fstest-20090130-RC/tests/chown/10.t ..... ok/home/yangguanjun/pjd-fstest-20090130-RC/tests/link/00.t ...... ok.../home/yangguanjun/pjd-fstest-20090130-RC/tests/link/17.t ...... ok/home/yangguanjun/pjd-fstest-20090130-RC/tests/mkdir/00.t ..... ok.../home/yangguanjun/pjd-fstest-20090130-RC/tests/mkdir/12.t ..... ok/home/yangguanjun/pjd-fstest-20090130-RC/tests/mkfifo/00.t .... ok.../home/yangguanjun/pjd-fstest-20090130-RC/tests/mkfifo/12.t .... ok/home/yangguanjun/pjd-fstest-20090130-RC/tests/open/00.t ...... ok.../home/yangguanjun/pjd-fstest-20090130-RC/tests/open/23.t ...... ok/home/yangguanjun/pjd-fstest-20090130-RC/tests/rename/00.t .... ok.../home/yangguanjun/pjd-fstest-20090130-RC/tests/rename/20.t .... ok/home/yangguanjun/pjd-fstest-20090130-RC/tests/rmdir/00.t ..... ok.../home/yangguanjun/pjd-fstest-20090130-RC/tests/rmdir/15.t ..... ok/home/yangguanjun/pjd-fstest-20090130-RC/tests/symlink/00.t ... ok.../home/yangguanjun/pjd-fstest-20090130-RC/tests/symlink/12.t ... ok/home/yangguanjun/pjd-fstest-20090130-RC/tests/truncate/00.t .. ok.../home/yangguanjun/pjd-fstest-20090130-RC/tests/truncate/14.t .. ok/home/yangguanjun/pjd-fstest-20090130-RC/tests/unlink/00.t .... ok.../home/yangguanjun/pjd-fstest-20090130-RC/tests/unlink/13.t .... ok/home/yangguanjun/pjd-fstest-20090130-RC/tests/xacl/00.t ...... ok.../home/yangguanjun/pjd-fstest-20090130-RC/tests/xacl/06.t ...... okAll tests successful.Files=191, Tests=1964, 92 wallclock secs ( 0.63 usr 0.11 sys + 4.58 cusr 10.40 csys = 15.72 CPU)Result: PASS 结论cephfs的功能性验证通过。 cephfs性能测试文件系统的性能测试工具有很多，这里我们选择常用的dd, fio, iozone和filebench。 cephfs的所有数据和元数据都是直接存在RADOS上的，并且cephfs支持stripe配置，这里我们做了以下stripe配置，作为测试cephfs性能的几种应用场景： stripe_unit=1M, stripe_count=4, object_size=4M 条带大小为1M，条带数目为4，object大小为4M 12配置测试目录attrsetfattr -n ceph.dir.layout -v "stripe_unit=1048576 stripe_count=4 object_size=4194304" dir-1M-4-4M stripe_unit=4M, stripe_count=1, object_size=4M ceph的默认配置：无条带化 12配置测试目录attrsetfattr -n ceph.dir.layout -v "stripe_unit= 4194304 stripe_count=1 object_size=4194304" dir-4M-1-4M stripe_unit=4M, stripe_count=4, object_size=64M 条带大小为4M，条带数目为4，object大小为64M 12配置测试目录attrsetfattr -n ceph.dir.layout -v "stripe_unit=4194304 stripe_count=4 object_size=67108864" dir-4M-4-64M 参考 cephfs的stripe配置 dddd是linux系统常用的测试设备和系统性能的工具。 测试分类与测试命令 Direct IO 12写：dd if=/dev/zero of=tstfile bs=&lt;bs-size&gt; count=&lt;2G/bs-size&gt; oflag=direct读：dd if=tstfile of=/dev/null bs=&lt;bs-size&gt; count=&lt;2G/bs-size&gt; iflag=direct Sync IO 12写：dd if=/dev/zero of=tstfile bs=&lt;bs-size&gt; count=&lt;2G/bs-size&gt; oflag=sync读：dd if=tstfile of=/dev/null bs=&lt;bs-size&gt; count=&lt;2G/bs-size&gt; iflag=sync Normal IO 12写：dd if=/dev/zero of=tstfile bs=&lt;bs-size&gt; count=&lt;2G/bs-size&gt;读：dd if=tstfile of=/dev/null bs=&lt;bs-size&gt; count=&lt;2G/bs-size&gt; fiofio是一个I/O标准测试和硬件压力验证工具，它支持13种不同类型的I/O引擎（sync, mmap, libaio, posixaio, SG v3, splice, null, network, syslet, guasi, solarisaio等），I/O priorities (for newer Linux kernels), rate I/O, forked or threaded jobs等等。fio可以支持块设备和文件系统测试，广泛用于标准测试、QA、验证测试等，支持Linux, FreeBSD, NetBSD, OS X, OpenSolaris, AIX, HP-UX, Windows等操作系统。 fio安装 1# yum install -y fio 测试分类与测试命令 固定配置 12345-filename=tstfile 指定测试文件的name-size=20G 指定测试文件的size为20G-direct=1 指定测试IO为DIRECT IO-thread 指定使用thread模式-name=fio-tst-name 指定job name 测试bandwidth时 -ioengine=libaio/sync -bs=512k/1M/4M/16M -rw=write/read -iodepth=64 -iodepth_batch=8 -iodepth_batch_complete=8 测试iops时： -ioengine=libaio -bs=4k -runtime=300 -rw=randwrite/randread -iodepth=64 -iodepth_batch=1 -iodepth_batch_complete=1 命令示例： 12345测试bandwidth：fio -filename=tstfile -size=10G -direct=1 -ioengine=libaio -thread -bs=512k -rw=write -iodepth=64 -iodepth_batch_submit=8 -iodepth_batch_complete=8 -name=write-bw-tst测试iops：fio -filename=tstfile -size=10G -direct=1 -ioengine=libaio -thread -bs=4k -rw=randwrite -iodepth=64 -runtime=300 -iodepth_batch=1 -iodepth_batch_complete=1 -name=randwrite-iops-tst iozoneiozone是目前应用非常广泛的文件系统测试标准工具，它能够产生并测量各种的操作性能，包括read, write, re-read, re-write, read backwards, read strided, fread, fwrite, random read, pread ,mmap, aio_read, aio_write等操作。Iozone目前已经被移植到各种体系结构计算机和操作系统上，广泛用于文件系统性能测试、分析与评估的标准工具。 iozone安装 123# wget http://www.iozone.org/src/current/iozone-3-465.i386.rpm# rpm -ivh iozone-3-465.i386.rpm# ln -s /opt/iozone/bin/iozone /usr/bin/iozone 测试分类与测试命令 iozone参数说明： 123456789101112131415161718192021222324252627282930313233343536373839指定使用自动模式-a指定测试模式-i 0 -i 1 -i 2 0=write/rewrite 1=read/re-read 2=random read/random write指定自动模式测试文件的大小范围，会依次翻倍-n 1m -g 2G -n 1m : 指定测试文件的最小值为1m -g 2G : 指定测试文件的最大值为2G指定自动模式下记录块的大小范围，会依次翻倍；记录块大小类似于dd的bs-y 128k -q 16m -y 128k : 指定记录块的最小值为128k -q 16m : 指定记录块的最小值为16m指定使用DIRECT IO-I指定写用O_SYNC-o指定threads个数测试系统吞吐量-t指定创建Excel reqport-R指定创建的Excel文件的name-b指定测试使用的record size-r指定测试文件的size-s 测试DIRET IO / SYNC IO - 非throughput模式 不指定threads，测试单个线程的iozone性能 12iozone -a -i 0 -i 1 -i 2 -n 1m -g 10G -y 128k -q 16m -I -Rb iozone-directio-output.xlsiozone -a -i 0 -i 1 -i 2 -n 1m -g 10G -y 128k -q 16m -o -Rb iozone-syncio-output.xls 测试系统吞吐量 - throughput模式 指定threads=16，获取整个系统的throughput 12iozone -a -i 0 -i 1 -i 2 -r 16m -s 2G -I -t 16 -Rb iozone-directio-throughput-output.xlsiozone -a -i 0 -i 1 -i 2 -r 16m -s 2G -o -t 16 -Rb iozone-syncio-throughput-output.xls filebenchFilebench 是一款文件系统性能的自动化测试工具，它通过快速模拟真实应用服务器的负载来测试文件系统的性能。它不仅可以仿真文件系统微操作（如 copyfiles, createfiles, randomread, randomwrite ），而且可以仿真复杂的应用程序（如 varmail, fileserver, oltp, dss, webserver, webproxy ）。 Filebench 比较适合用来测试文件服务器性能，但同时也是一款负载自动生成工具，也可用于文件系统的性能。 filebench安装 1234567891011121314源码：https://github.com/filebench/filebench下载后在测试节点解压缩# yum install libtool automake# libtoolize# aclocal# autoheader# automake --add-missing# autoconf# yum install gcc flex bison# ./configure# make# make install 安装后，在目录 /usr/local/share/filebench/workloads/ 下有很多定义好的workload，可以直接拿来使用。配置里面的 $dir 为测试cephfs的目录，若文件后没有 run 命令，添加：run 测试分类 filebench有很多定义好的workload，如下： 1234567891011# ls /usr/local/share/filebench/workloads/compflow_demo.f filemicro_rwrite.f fivestreamreaddirect.f openfiles.f singlestreamwrite.fcopyfiles.f filemicro_rwritefsync.f fivestreamread.f randomfileaccess.f tpcso.fcreatefiles.f filemicro_seqread.f fivestreamwritedirect.f randomread.f varmail.fcvar_example.f filemicro_seqwrite.f fivestreamwrite.f randomrw.f videoserver.ffilemicro_create.f filemicro_seqwriterand.f listdirs.f randomwrite.f webproxy.ffilemicro_createfiles.f filemicro_seqwriterandvargam.f makedirs.f ratelimcopyfiles.f webserver.ffilemicro_createrand.f filemicro_seqwriterandvartab.f mongo.f removedirs.ffilemicro_delete.f filemicro_statfile.f netsfs.f singlestreamreaddirect.ffilemicro_rread.f filemicro_writefsync.f networkfs.f singlestreamread.ffilemicro_rwritedsync.f fileserver.f oltp.f singlestreamwritedirect.f 因为最新的filebench修改了变量的定义，所以这里面的一些workload并不能成功运行，只需选择可用的有代表性的workload测试即可。 参考 filesystem测试工具之filebench 结论 cephfs的direct IO性能有限 cephfs读写能跑满ceph cluster集群性能 客户端缓存和OSD缓存对读写的影响很大 cephfs稳定性测试为了测试cephfs是否能在线上提供服务，需要测试下其稳定性，这里采用两种方式测试。 读写数据模式针对读写数据模式，我们选择工具fio，在cephfs client端长时间运行，看会不会报错。 测试逻辑大概如下： 12345# fio循环测试读写while now &lt; time fio write 10G file fio read 10G file delete file 读写元数据模式针对读写元数据模式，我们采用自写脚本，大规模创建目录、文件、写很小数据到文件中，在cephfs client端长时间运行，看会不会报错。 测试逻辑大概如下： 1234567# 百万级别的文件个数while now &lt; time create dirs touch files write little data to each file delete files delete dirs 结论通过几天的连续测试，cephfs一切正常，这说明cephfs是可以应用到生产环境的。但在上亿级别的文件测试中，也遇到点问题，会在下面章节的问题里说明。 cephfs异常测试cephfs的功能依赖于MDS和Ceph Cluster，关键的元数据都通过MDS获取，这里测试的异常也主要基于MDS的异常进行分类的。 查看ceph MDS与interl和timeout相关的配置有： 123OPTION(mds_tick_interval, OPT_FLOAT, 5)OPTION(mds_mon_shutdown_timeout, OPT_DOUBLE, 5)OPTION(mds_op_complaint_time, OPT_FLOAT, 30) 所以这里测试对MDS stop/start的时间间隔取为：2s，10s，60s 测试分类 主从MDS 单MDS 测试中启停MDS service的命令为： 12# systemctl stop ceph-mds.target# systemctl start ceph-mds.target 测试结果 fio随机写单个文件 MDS模式 启停interval io影响 单MDS 2s, 10s 无 60s 约40s时影响IO 主从MDS 2s, 10s 无 60s 主从不同时停止时无影响 同时停止主从MDS时，影响与单MDS一致 下面展示了单MDS停止约60s的时候，对fio测试的影响： iozone测试direct IO MDS模式 启停interval io影响 单MDS 2s，10s，60s 对当前文件的IO影响同fio测试 对读写新文件的影响会立刻体现 主从MDS 2s，10s，60s 主从不同时停止时无影响 同时停止主从MDS时，影响与单MDS一致 结论 单MDS的情况下，短暂的MDS crush并不会影响客户端对一个file的读写 单MDS的情况下，MDS crush后，client端对没有缓存过caps的文件操作会hang住 主从MDS的情况下，只要有一个MDS正常，cephfs的服务就不会中断 主从MDS的情况下，两个MDS都crush后，影响与单MDS的一致 所以生产环境中，我们建议配置主从MDS的模式，提高cephfs的高可用性。 问题cephfs配置较大stripe unit的问题测试中，对于指定cephfs的laylout如下时，发现了一个cephfs的bug，已经提交给ceph社区. 1# setfattr -n ceph.dir.layout -v "stripe_unit=67108864 stripe_count=1 object_size=67108864" dir-64M-1-64M 详情参阅：http://tracker.ceph.com/issues/20528 最新更新 cephfs client端的配置限制了read message的最大size为16M。==所以实际使用中的 stripe_unit 不能大于16M== cephfs读写上亿级文件为了查看cephfs对大规模小文件应用的支持效果，我们这里通过脚本测试了5亿个文件的场景。 测试的脚本如下： 1234567891011121314151617#!/bin/bash# 5000个目录，每个目录10万个文件round=5000echo `date` &gt;&gt; /root/mds_testlogfor j in `seq $round`;do mkdir /mnt/test$j for i in `seq 100000`;do echo hello &gt; /mnt/test$j/file$&#123;i&#125;; donedonefor j in `seq $round`;do rm -rf /mnt/test$jdone 问题： 单线程运行耗时较长 单线程删除文件时，rm命令报No space left on device错误 单线程删除文件时，日志中报_send skipping beacon, heartbeat map not healthy 多线程并发创建、删除测试和深目录层级测试待验证 总结经过功能测试、性能测试、稳定性测试和异常测试后，我们得出如下结论： cephfs是production ready的，能满足基本生产环境对文件存储的需求 cephfs的主从MDS是稳定的 cephfs的direct IO性能有限，分析后明确是cephfs kernel client的IO处理逻辑限制的 cephfs是能跑满整个ceph cluster集群性能的 默认的stripe模式下(stripe unit=4M, stripe count=1, object size=4M)，cephfs的性能就挺好 受到cephfs client端的系统缓存影响，非direct IO的读写性能都会比较高，这个不具有太大参考意义]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>cephfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[filesystem测试工具之filebench]]></title>
    <url>%2F2017%2F07%2F08%2Ffs-testtool-filebench%2F</url>
    <content type="text"><![CDATA[简介Filebench 是一款文件系统性能的自动化测试工具，它通过快速模拟真实应用服务器的负载来测试文件系统的性能。它不仅可以仿真文件系统微操作（如 copyfiles, createfiles, randomread, randomwrite ），而且可以仿真复杂的应用程序（如 varmail, fileserver, oltp, dss, webserver, webproxy ）。 Filebench 比较适合用来测试文件服务器性能，但同时也是一款负载自动生成工具，也可用于文件系统的性能。 代码https://github.com/filebench/filebench 安装1234567891011121314151617181920212223242526272829# yum install libtool automake# libtoolize# aclocal# autoheader# automake --add-missing# autoconf# yum install gcc flex bison# ./configure# make# make install...----------------------------------------------------------------------▽Libraries have been installed in: /usr/local/lib/filebenchIf you ever happen to want to link against installed librariesin a given directory, LIBDIR, you must either use libtool, andspecify the full pathname of the library, or use the `-LLIBDIR'flag during linking and do at least one of the following: - add LIBDIR to the `LD_LIBRARY_PATH' environment variable during execution - add LIBDIR to the `LD_RUN_PATH' environment variable during linking - use the `-Wl,-rpath -Wl,LIBDIR' linker flag - have your system administrator add LIBDIR to `/etc/ld.so.conf'See any operating system documentation about shared libraries formore information, such as the ld(1) and ld.so(8) manual pages.----------------------------------------------------------------------... 运行在目录 /usr/local/share/filebench/workloads/ 下有很多定义好的workload，我们可以拿来使用配置里面的 $dir 为测试filesystem的目录，若文件后没有run &lt;secs&gt;命令，添加：run &lt;secs&gt; 1234567891011# ls /usr/local/share/filebench/workloads/compflow_demo.f filemicro_rwrite.f fivestreamreaddirect.f openfiles.f singlestreamwrite.fcopyfiles.f filemicro_rwritefsync.f fivestreamread.f randomfileaccess.f tpcso.fcreatefiles.f filemicro_seqread.f fivestreamwritedirect.f randomread.f varmail.fcvar_example.f filemicro_seqwrite.f fivestreamwrite.f randomrw.f videoserver.ffilemicro_create.f filemicro_seqwriterand.f listdirs.f randomwrite.f webproxy.ffilemicro_createfiles.f filemicro_seqwriterandvargam.f makedirs.f ratelimcopyfiles.f webserver.ffilemicro_createrand.f filemicro_seqwriterandvartab.f mongo.f removedirs.ffilemicro_delete.f filemicro_statfile.f netsfs.f singlestreamreaddirect.ffilemicro_rread.f filemicro_writefsync.f networkfs.f singlestreamread.ffilemicro_rwritedsync.f fileserver.f oltp.f singlestreamwritedirect.f 1234567891011121314151617181920# /usr/local/bin/filebench -f /usr/local/share/filebench/workloads/createfiles.fFilebench Version 1.5-alpha30.000: Allocated 173MB of shared memory0.001: Createfiles Version 3.0 personality successfully loaded0.001: Populating and pre-allocating filesets0.031: bigfileset populated: 50000 files, avg. dir. width = 100, avg. dir. depth = 2.3, 0 leafdirs, 781.250MB total size0.031: Removing bigfileset tree (if exists)0.033: Pre-allocating directories in bigfileset tree0.159: Pre-allocating files in bigfileset tree0.207: Waiting for pre-allocation to finish (in case of a parallel pre-allocation)0.207: Population and pre-allocation of filesets completed0.208: Starting 1 filecreate instances1.210: Running...17.211: Run took 16 seconds...17.211: Per-Operation Breakdownclosefile1 49985ops 3124ops/s 0.0mb/s 0.002ms/op [0.001ms - 0.238ms]writefile1 49985ops 3124ops/s 48.8mb/s 0.020ms/op [0.006ms - 2.307ms]createfile1 50000ops 3125ops/s 0.0mb/s 4.905ms/op [0.147ms - 1267.256ms]17.211: IO Summary: 149970 ops 9372.298 ops/s 0/3124 rd/wr 48.8mb/s 1.643ms/op17.211: Shutting down processes 输出解释： flowop name - 支持的flowop有很多 所有threads的ops 所有threads的ops / run time 所有threads的READ/WRITE带宽 所有threads的每个op的平均latency 测试中op的最小和最大latency IO Summary: 149970 ops ：所有flowop的总和 9372.298 ops/s ：所有flowop的总和 / run time 0/3124 rd/wr ：所有flowop中READ/WRITE的ops / run time 48.8mb/s ： 所有flowop的IO带宽 1.643ms/op ：所有flowop的每个op的平均latency 参考 https://github.com/filebench/filebench/wiki/Collected-metrics 写workload我们可以自己写workload文件，语法格式可参考：https://github.com/filebench/filebench/wiki/Workload-model-language 以createfiles.f为例，解释里面的含义： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# cat createfiles.f...// 下面是用户变量定义set $dir=/home/yangguanjun3/mike/tst1set $nfiles=50000set $meandirwidth=100set $meanfilesize=16kset $iosize=1mset $nthreads=16 // 设置退出模式，支持[ timeout | alldone | firstdone ]set mode quit firstdone // fileset：定义一组测试中用的files// name=bigfileset：必须指定 - fileset的名称，后面flowop中用到// path=$dir：必须指定 - 创建测试文件的目录// size=$meanfilesize：可选，关键字也可以为filesize，默认为1KB - 测试文件的size// entries=$nfiles：可选，默认位1024 - fileset中的file个数// dirwidth=$meandirwidth：可选，默认为0 - 每个目录中创建的file个数define fileset name=bigfileset,path=$dir,size=$meanfilesize,entries=$nfiles,dirwidth=$meandirwidth // process：定义处理过程// name=filecreate：必须指定 - 处理过程的名称// instances=1：可选，默认为1 - 处理过程的进程数define process name=filecreate,instances=1&#123;// thread：process中的一个thread// name=filecreatethread：必须指定 - 处理线程的名称// memsize=10m：必须指定 - 线程启动后初始化为0的内存大小，用于read/write flowop// instances=$nthreads：可选，默认为1 - 创建的线程数 thread name=filecreatethread,memsize=10m,instances=$nthreads &#123;// flowop：定义处理流程中的每一步// createfile/writewholefile/closefile：flowop的关键字，每个代表不同的操作// name=$name：flowop的名称// filesetname=bigfileset：指定op操作的fileset// fd=1：指定file descriptor的值，在应用允许文件被多次open的场景中有用// iosize=$iosize：指定读写的iosize flowop createfile name=createfile1,filesetname=bigfileset,fd=1 flowop writewholefile name=writefile1,fd=1,iosize=$iosize flowop closefile name=closefile1,fd=1 &#125;&#125;echo "Createfiles Version 3.0 personality successfully loaded”// 开始运行filebench测试// 格式：run [&lt;runtime&gt;]，&lt;runtime&gt;不指定的话，默认为60srun 60]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cephfs开启client端认证]]></title>
    <url>%2F2017%2F07%2F01%2Fcephfs-client-authentication%2F</url>
    <content type="text"><![CDATA[需求cephfs支持client端的authentication，来限制不同的用户访问不同的目录，或者后端的pool，但需要先开启ceph集群的认证。 环境123Ceph：Jewel 10.2.7Host：CentOS Linux release 7.2.1511 (Core)Kernel Version: Linux 3.10.0-327.el7.x86_64 打开ceph集群认证修改ceph.conf1234# vim /etc/ceph/ceph.confauth_cluster_required = cephxauth_service_required = cephxauth_client_required = cephx 检查ceph模块的keyring开启ceph认证后，各个模块都会受到自己的keyring的权限限制。默认各个模块的keyring都在目录/var/lib/ceph/下的对应子目录里。比如ceph osd-0的keyring： 123# cat /var/lib/ceph/osd/ceph-0/keyring[osd.0] key = AQACrQFZbvBLARAAWXWdeXgVd3SN7NmAYzXDtg== 在我们系统里，查看各个monitor的对应目录是没有keyring的，所以要自己手动创建，命令如下： 12# ceph-authtool --create-keyring /tmp/ceph.mon.keyring --gen-key -n mon. --cap mon 'allow *’# cp /tmp/ceph.mon.keyring /var/lib/ceph/mon/ceph-mon1/keyring 若有多个monitor节点，把文件/tmp/ceph.mon.keyring拷贝到其它节点的monitor目录即可 检查mds的keyring123# cat /var/lib/ceph/mds/ceph-mds1/keyring[mds.mds1] key = AQCJ8CNZhh7ZNxAAtKplOjgzBdEPpc/g4C2QWg== 若没有对应的keyring，通过下面的命令创建，{$id}是mds的name： 1# ceph auth get-or-create mds.&#123;$id&#125; mon 'allow rwx' osd 'allow *' mds 'allow *' -o /var/lib/ceph/mds/ceph-&#123;$id&#125;/keyring 重启ceph服务在对应的ceph服务节点重启各个服务，命令如下： 123# systemctl restart ceph-mon.target# systemctl restart ceph-osd.target# systemctl restart ceph-mds.target 创建auth client创建不同的client，赋予不同的权限访问cephfs 12# ceph auth get-or-create client.tst1 mon 'allow r' mds 'allow r, allow rw path=/tst1’ osd 'allow rw'# ceph auth get-or-create client.tst2 mon 'allow r' mds 'allow r, allow rw path=/tst2’ osd 'allow rw' 上诉命令解释： mon ‘allow r’ ：允许user从monitor读取数据；必须配置 mds ‘allow r, allow rw path=/tst1’：允许user从mds读取数据，允许user对目录/tst1读写；其中’allow r’必须配置，不然user不能从mds读取数据，mount会报permission error； osd ‘allow rw’ ：允许user从osd上读写数据；若不配置，用户只能从mds上获取FS的元数据信息，没法查看各个文件的数据； 注释： ceph client的auth还不是很完善，上面配置中限制了 client tst1能读写/tst1目录，client tst2能读写/tst2目录，但是user tst1/tst2都能看到并读取FS的所有文件，还没找到如何设置user只能访问到指定子目录的方法 osd还可以配置指定访问pool和namespace，比如配置osd &#39;allow rw pool=cephfs_data2&#39;，既是指定该user只能rw pool cephfs_data2； 结合命令：setfattr -n ceph.dir.layout -v &quot;pool=cephfs_data2&quot; /mnt/tst1/ 可以配置目录tst1使用cephfs_data2的pool，这样可以做到多租户的数据隔离（以pool为单位）。以admin用户登录，修改tst1目录的attr。 namespace说是可以指定user访问同一pool的不同namespace，可以做多租户的数据隔离，但还没找到如何配置使用。文档说明是只有librados支持，ceph client（rbd/rgw/cephfs）都还不支持。 检查ceph auth1234567891011# ceph auth list...client.tst1 key: AQCd+UBZxpi4EBAAUNyBDGdZbPgfd4oUb+u41A== caps: [mds] allow r, allow rw path=/tst1 caps: [mon] allow rclient.tst2 key: AQC5FUFZ6EycMBAAx9yD8DFmii2PIEv9YOVLUw== caps: [mds] allow r, allow rw path=/tst2 caps: [mon] allow r... 测试cephfs client auth12# mount -t ceph 10.10.2.1:6789:/tst1 /home/mike/tst1/ -o name=tst1,secret=AQdd+UBZxpi4EaAAUNyBDGdZbPgfd4oUb+u41A==# mount -t ceph 10.10.2.1:6789:/tst2 /home/mike/tst2/ -o name=tst2,secret=AQd5FUFZ6EycMaAAx9yD8DFmii2PIEv9YOVLUw== 验证两个目录都有读写权限，cd进去后可以正常读写。 1234# umount /home/mike/tst1/# umount /home/mike/tst2/# mount -t ceph 10.10.2.1:6789:/ /home/mike/tst1/ -o name=tst1,secret=AQdd+UBZxpi4EaAAUNyBDGdZbPgfd4oUb+u41A==# mount -t ceph 10.10.2.1:6789:/ /home/mike/tst2/ -o name=tst2,secret=AQd5FUFZ6EycMaAAx9yD8DFmii2PIEv9YOVLUw== 上诉命令把整个cephfs通过不同的user mount到不同的目录，进去可以发现，是可以看到整个cephfs的文件信息的，但是不能访问user没有rw权限的文件。 12345678910111213141516171819202122232425# cd /home/mike/tst1/ // 进入user tst1 mount的目录，该目录访问受到user tst1的权限限制# ls // 访问cephfs的根目录1.txt tst1 tst2# cat 1.txtcat: 1.txt: Operation not permitted# cd tst1# echo "hello tst1" &gt; tst1file# cat tst1filehello tst1# cd ../tst2# echo "hello tst1" &gt; tst1filebash: tst1file: Permission denied# cd /home/mike/tst2/ // 进入user tst2 mount的目录，该目录访问受到user tst2的权限限制# ls // 访问cephfs的根目录1.txt tst1 tst2# cat 1.txtcat: 1.txt: Operation not permitted# cd tst1# echo "hello tst2" &gt; tst2filebash: tst2file: Permission denied# cd ../tst2# echo "hello tst2" &gt; tst2file# cat tst2filehello tst2 所以从上面的测试中可以看出，若用户有对cephfs的目录权限控制，我们可以通过上述user的auth来实现。但这部分也不是很完善，还没找到能支持readonly访问某一目录的方法。 参考http://docs.ceph.com/docs/jewel/rados/configuration/auth-config-ref/http://docs.ceph.com/docs/jewel/cephfs/client-auth/]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>cephfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cephfs与rbd的direct IO测试分析]]></title>
    <url>%2F2017%2F06%2F26%2Fcephfs-dd-direct-io-tst-analysis%2F</url>
    <content type="text"><![CDATA[概述在测试cephfs的性能中，发现其direct IO性能较差，而rbd的direct IO性能就较好，很奇怪为什么，这里做测试对比和分析 性能测试rbd设备12345678# rbd create test -p cephfs_data3 --size=1024# rbd feature disable cephfs_data3/test exclusive-lock, object-map, fast-diff, deep-flatten# rbd map cephfs_data3/test # dd if=/dev/zero of=/dev/rbd0 bs=1024M count=1 oflag=direct1+0 records in1+0 records out1073741824 bytes (1.1 GB) copied, 1.72135 s, 624 MB/s cephfs123456# mount -t ceph 10.10.2.1:6789:/foo mike -o name=foo,secret=AQCddEtZC8n5KRAAPw5qd3BWLzlgqiEuRR5AYg==# cd mike/# dd if=/dev/zero of=file bs=1024M count=1 oflag=direct1+0 records in1+0 records out1073741824 bytes (1.1 GB) copied, 7.31779 s, 147 MB/s 结论：为什么对于direct IO，rbd与cephfs的差别这么大？ 分析cephfs代码分析cephfs client端写操作接口： 12345678910111213141516const struct file_operations ceph_file_fops = &#123; // cephfs中文件的file操作集 .open = ceph_open, .release = ceph_release, .llseek = ceph_llseek, .read_iter = ceph_read_iter, .write_iter = ceph_write_iter, .mmap = ceph_mmap, .fsync = ceph_fsync, .lock = ceph_lock, .flock = ceph_flock, .splice_read = generic_file_splice_read, .splice_write = iter_file_splice_write, .unlocked_ioctl = ceph_ioctl, .compat_ioctl = ceph_ioctl, .fallocate = ceph_fallocate,&#125;; 针对direct IO的处理过程如下： 12345678910111213141516171819202122232425262728293031323334353637static ssize_t ceph_write_iter(struct kiocb *iocb, struct iov_iter *from)&#123;... if (iocb-&gt;ki_flags &amp; IOCB_DIRECT) written = ceph_direct_read_write(iocb, &amp;data, snapc, // direct IO处理函数 &amp;prealloc_cf); else written = ceph_sync_write(iocb, &amp;data, pos, snapc);...&#125; static ssize_tceph_direct_read_write(struct kiocb *iocb, struct iov_iter *iter, struct ceph_snap_context *snapc, struct ceph_cap_flush **pcf)&#123;... while (iov_iter_count(iter) &gt; 0) &#123; // 循环发送数据... req = ceph_osdc_new_request(&amp;fsc-&gt;client-&gt;osdc, &amp;ci-&gt;i_layout, // 初始化osd request vino, pos, &amp;size, 0, /*include a 'startsync' command*/ write ? 2 : 1, write ? CEPH_OSD_OP_WRITE : CEPH_OSD_OP_READ, flags, snapc, ci-&gt;i_truncate_seq, ci-&gt;i_truncate_size, false);... ret = ceph_osdc_start_request(req-&gt;r_osdc, req, false); // 发送osd request写数据 if (!ret) ret = ceph_osdc_wait_request(&amp;fsc-&gt;client-&gt;osdc, req); // 等待osd request写返回... &#125;...&#125; 所以从上面的逻辑看：cephfs写direct IO是会被切分为一个个写osd的request，顺序的每个osd request都会等待写成功返回。这样就无法发挥出分布式集群的整体性能，所以cephfs的direct IO写性能较低。 测试验证按照 cephfs kernel client debug 方法 打开cephfs client端的log，测试一个12M的direct IO； 命令：dd if=/dev/zero of=file bs=12M count=1 oflag=direct log信息： 1234567891011121314151617181920212223[260262.367080] aio_write ffff8801f403b418 10000002393.fffffffffffffffe 0~12582912 getting caps. i_size 0[260262.367085] aio_write ffff8801f403b418 10000002393.fffffffffffffffe 0~12582912 got cap refs on Fwb // get caps花了5us[260262.367086] sync_direct_read_write (write) on file ffff88142f83ba00 0~12582912 // 第一个osd写4M[260262.367092] ceph_osdc_alloc_request req ffff881257eb8000[260262.367173] ----- ffff88142f83a200 to osd23 42=osd_op len 201+0+4194304 ——[260262.367177] wait_request_timeout req ffff881257eb8000 tid 4[260262.394196] ===== ffff88142f83b400 4 from osd23 43=osd_opreply len 182+0 (1765759831 0 0) ===== // 第二个osd写4M[260262.394264] ceph_osdc_alloc_request req ffff881257eb8000[260262.394379] ----- ffff88142f83b400 to osd3 42=osd_op len 201+0+4194304 ——[260262.394380] wait_request_timeout req ffff881257eb8000 tid 5[260262.433310] ===== ffff88142f83be00 1 from osd3 43=osd_opreply len 182+0 (93139192 0 0) ===== // 第三个osd写4M[260262.433426] ceph_osdc_alloc_request req ffff881257eb8000[260262.433534] ----- ffff88142f83a200 to osd26 42=osd_op len 201+0+4194304 ——[260262.433535] wait_request_timeout req ffff881257eb8000 tid 6[260262.464063] ===== ffff88142f83b700 1 from osd26 43=osd_opreply len 182+0 (3847263551 0 0) ===== [260262.464151] aio_write ffff8801f403b418 10000002393.fffffffffffffffe 0~12582912 dropping cap refs on Fwb 测试增大object size增大cephfs中file对应的object size和stripe unit都为64M，然后测试性能 1234567891011# touch file# setfattr -n ceph.file.layout.object_size -v 67108864 file# setfattr -n ceph.file.layout.stripe_unit -v 67108864 file# getfattr -n ceph.file.layout file# file: fileceph.file.layout="stripe_unit=67108864 stripe_count=1 object_size=67108864 pool=cephfs_data" # dd if=/dev/zero of=file bs=1024M count=1 oflag=direct1+0 records in1+0 records out1073741824 bytes (1.1 GB) copied, 6.77491 s, 158 MB/s 测试性能发现，对比4M的object size和stripe unit，性能并没有提升。 分析osd端的op时间开销获取cephfs文件的数据的location信息： 12345678910# cephfs file show_locationWARNING: This tool is deprecated. Use the layout.* xattrs to query and modify layouts.location.file_offset: 0location.object_offset:0location.object_no: 0location.object_size: 67108864location.object_name: 100000720de.00000000location.block_offset: 0location.block_size: 67108864location.osd: 25 // 数据存储在osd 25上 在osd 25上打开optracker： 123456修改/etc/ceph/ceph.conf，在osd域添加下面两行：osd_enable_op_tracker = trueosd_op_history_size = 100然后重启osd服务# systemctl restart ceph-osd@25.service 重新跑dd命令后，在osd端收集ops信息： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212# ceph daemon osd.25 dump_historic_ops&#123; "num to keep": 20, "duration to keep": 600, "Ops": [ &#123; "description": "osd_op(mds.0.75:987357 13.844f3494 200.00000000 [] snapc 0=[] ondisk+write+known_if_redirected+full_force e452)", // mds操作 "initiated_at": "2017-06-27 09:55:39.196572", "age": 8.391089, "duration": 0.001696, // 花费1ms "type_data": [ "commit sent; apply or cleanup", [ &#123; "time": "2017-06-27 09:55:39.196572", "event": "initiated" &#125;, &#123; "time": "2017-06-27 09:55:39.196629", "event": "queued_for_pg" &#125;, &#123; "time": "2017-06-27 09:55:39.196735", "event": "reached_pg" &#125;, &#123; "time": "2017-06-27 09:55:39.196799", "event": "started" &#125;, &#123; "time": "2017-06-27 09:55:39.196861", "event": "waiting for subops from 1,18" &#125;, &#123; "time": "2017-06-27 09:55:39.196962", "event": "commit_queued_for_journal_write" &#125;, &#123; "time": "2017-06-27 09:55:39.197011", "event": "write_thread_in_journal_buffer" &#125;, &#123; "time": "2017-06-27 09:55:39.197206", "event": "journaled_completion_queued" &#125;, &#123; "time": "2017-06-27 09:55:39.197280", "event": "op_commit" &#125;, &#123; "time": "2017-06-27 09:55:39.197738", "event": "op_applied" &#125;, &#123; "time": "2017-06-27 09:55:39.198125", "event": "sub_op_commit_rec from 1" &#125;, &#123; "time": "2017-06-27 09:55:39.198225", "event": "sub_op_commit_rec from 18" &#125;, &#123; "time": "2017-06-27 09:55:39.198236", "event": "commit_sent" &#125;, &#123; "time": "2017-06-27 09:55:39.198268", "event": "done" &#125; ] ] &#125;, &#123; "description": "osd_op(mds.0.75:987359 9.a30e34e0 100000720de.00000000 [] snapc 1=[] ondisk+write+known_if_redirected+full_force e452)", // mds操作 "initiated_at": "2017-06-27 09:55:39.200560", "age": 8.387101, "duration": 0.016664, // 花费16ms "type_data": [ "commit sent; apply or cleanup", [ &#123; "time": "2017-06-27 09:55:39.200560", "event": "initiated" &#125;, &#123; "time": "2017-06-27 09:55:39.200617", "event": "queued_for_pg" &#125;, &#123; "time": "2017-06-27 09:55:39.200683", "event": "reached_pg" &#125;, &#123; "time": "2017-06-27 09:55:39.200721", "event": "started" &#125;, &#123; "time": "2017-06-27 09:55:39.200813", "event": "waiting for subops from 7,15" &#125;, &#123; "time": "2017-06-27 09:55:39.200921", "event": "commit_queued_for_journal_write" &#125;, &#123; "time": "2017-06-27 09:55:39.200955", "event": "write_thread_in_journal_buffer" &#125;, &#123; "time": "2017-06-27 09:55:39.201047", "event": "journaled_completion_queued" &#125;, &#123; "time": "2017-06-27 09:55:39.201104", "event": "op_commit" &#125;, &#123; "time": "2017-06-27 09:55:39.201955", "event": "sub_op_commit_rec from 15" &#125;, &#123; "time": "2017-06-27 09:55:39.202224", "event": "sub_op_commit_rec from 7" &#125;, &#123; "time": "2017-06-27 09:55:39.202231", "event": "commit_sent" &#125;, &#123; "time": "2017-06-27 09:55:39.217210", "event": "op_applied" &#125;, &#123; "time": "2017-06-27 09:55:39.217224", "event": "done" &#125; ] ] &#125;, &#123; "description": "osd_op(client.372486.1:1048 9.a30e34e0 100000720de.00000000 [] snapc 1=[] ondisk+write+ordersnap e452)", "initiated_at": "2017-06-27 09:55:39.228517", "age": 8.359144, "duration": 0.450798, // 花费450ms "type_data": [ "commit sent; apply or cleanup", &#123; "client": "client.372486", "tid": 1048 &#125;, [ &#123; "time": "2017-06-27 09:55:39.228517”, // 收到请求，创建OpRequest "event": "initiated" &#125;, &#123; "time": "2017-06-27 09:55:39.345624”, // 花费117ms，simple messager，接收64MB数据的时延 "event": "queued_for_pg" &#125;, &#123; "time": "2017-06-27 09:55:39.345691", "event": "reached_pg" &#125;, &#123; "time": "2017-06-27 09:55:39.345738", "event": "started" &#125;, &#123; "time": "2017-06-27 09:55:39.348961", "event": "waiting for subops from 7,15" &#125;, &#123; "time": "2017-06-27 09:55:39.381379”, // 花费33ms，- FileJournal::submit_entry 这部分的延时应该是reserve filestore throttle等待的时间 "event": "commit_queued_for_journal_write" &#125;, &#123; "time": "2017-06-27 09:55:39.381441", "event": "write_thread_in_journal_buffer" &#125;, &#123; "time": "2017-06-27 09:55:39.524101”, // 花费43ms，这部分延时是写journal的花费时间 "event": "journaled_completion_queued" &#125;, &#123; "time": "2017-06-27 09:55:39.524162”, // commit到journal成功 "event": "op_commit" &#125;, &#123; "time": "2017-06-27 09:55:39.570412”, // 花费46ms，apply到filestore成功 "event": "op_applied" &#125;, &#123; "time": "2017-06-27 09:55:39.658194", "event": "sub_op_commit_rec from 7" &#125;, &#123; "time": "2017-06-27 09:55:39.679285”, // sub从发送到收到reply花费约330ms，subop commit返回 "event": "sub_op_commit_rec from 15" &#125;, &#123; "time": "2017-06-27 09:55:39.679298”, // 返回给客户端 "event": "commit_sent" &#125;, &#123; "time": "2017-06-27 09:55:39.679315", "event": "done" &#125; ] ] &#125; ]&#125; 结论：大的object size写，在osd这段处理时间很长，从 simple messager -&gt; pg -&gt; journal -&gt; filestore 每一步都是串行的，单simple messager接受64M的数据就花费了117ms（10G网络），所以性能比较低，很难提升； rbd设备rbd到object的映射创建一个object size为32M的rbd设备 1234# rbd create foxtst -p cephfs_data3 --size=1024 --object-size 32M# rbd feature disable cephfs_data3/foxtst exclusive-lock, object-map, fast-diff, deep-flatten# rbd map cephfs_data3/foxtst/dev/rbd0 先写64M数据到该rbd设备 1# dd if=/dev/zero of=/dev/rbd0 bs=64M count=1 oflag=direct 获取rbd设备的id 123# rados -p cephfs_data3 get rbd_id.foxtst myfile# cat myfile56092238e1f29 找到rbd设备对应的rados object，这里我们就写了64M数据，所以就能找到两个32M的objects 1234# rados ls -p cephfs_data3 | grep 56092238e1f29rbd_data.56092238e1f29.0000000000000001rbd_header.56092238e1f29rbd_data.56092238e1f29.0000000000000000 找到上述两个object的location 1234# ceph osd map cephfs_data3 rbd_data.56092238e1f29.0000000000000000osdmap e467 pool 'cephfs_data3' (15) object 'rbd_data.56092238e1f29.0000000000000000' -&gt; pg 15.47ba6daa (15.1aa) -&gt; up ([21,14,2], p21) acting ([21,14,2], p21)# ceph osd map cephfs_data3 rbd_data.56092238e1f29.0000000000000001osdmap e467 pool 'cephfs_data3' (15) object 'rbd_data.56092238e1f29.0000000000000001' -&gt; pg 15.46295241 (15.41) -&gt; up ([4,23,13], p4) acting ([4,23,13], p4) 性能测试在对应osd节点上打开op tracker后，开始测试 1# dd if=/dev/zero of=/dev/rbd0 bs=64M count=1 oflag=direct 在对应的osd节点上收集ops信息： 12# ceph daemon osd.21 dump_historic_ops &gt; osd-21-dump_historic_ops# ceph daemon osd.4 dump_historic_ops &gt; osd-4-dump_historic_ops 分析这些ops信息可以得到以下结果： 写单个32M object会拆分很多个ops，拆分规则与device的max_sectors_kb配置有关系，默认这个值为：512 写单个32M object的ops会顺序发给osd，但不是等待一个op返回后再发下一个 写单个32M object的ops是流水线的处理模式；simple messager -&gt; pg -&gt; journal -&gt; filestore，所以这样处理多个小请求的效率更高，不会有cephfs中大object写会在simple messager里等待数据都全部接受后再走下一步的问题 写两个32M object的ops是并行的，因为这里指定的bs=64M，对应到两个不同的osd上，不同osd上的ops没先后关系 如下图所示： 所以rbd设备的direct IO写的性能随着bs的增大是逐渐提高的。]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>rbd</tag>
        <tag>cephfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cephfs的stripe配置]]></title>
    <url>%2F2017%2F06%2F15%2Fcephfs-stripe%2F</url>
    <content type="text"><![CDATA[简介cephfs支持配置file layout，可以控制file分配到指定的ceph rados objects上，这些信息是写在file/dir的xattrs上。 文件的layout xattrs为：ceph.file.layout 目录的layout xattrs为：ceph.dir.layout 目录中的文件和子目录默认继承父目录的layout配置 支持的layout配置项有： pool file的数据存储在哪个RADOS pool里 namespacefile的数据存储在RADOS pool里的哪个namespace里，但现在rbd/rgw/cephfs都还不支持 stripe_unit 条带的大小，以Bytes为单位 stripe_count 条带的个数 比如，stripe_unit=524288，stripe_count=2，默认object size是4MB，则file写10MB的数据分配如下： 123456789101112131415161718192021222324252627282930 --- object set 0 --- --- object set 1 ---/---------\ /---------\ /---------\ /---------\ | obj 0 | | obj 1 | | obj 2 | | obj 3 ||=========| |=========| |=========| |=========|| stripe | | stripe | | stripe | | stripe || unit 0 | | unit 1 | | unit 0 | | unit 1 ||---------| |---------| |---------| |---------|| stripe | | stripe | | stripe | | stripe || unit 2 | | unit 3 | | unit 2 | | unit 3 ||---------| |---------| \=========/ \=========/| stripe | | stripe || unit 4 | | unit 5 | osd 16 osd 20|---------| |---------| | stripe | | stripe | | unit 6 | | unit 7 | |---------| |---------| | stripe | | stripe | | unit 8 | | unit 9 | |---------| |---------| | stripe | | stripe | | unit 10 | | unit 11 | |---------| |---------| | stripe | | stripe | | unit 12 | | unit 13 | |---------| |---------| | stripe | | stripe | | unit 14 | | unit 15 | \=========/ \=========/ osd 25 osd 3 配置file stripe以admin的user登录，配置dir的attribute 123# mount -t ceph 10.10.2.1:6789:/ /mnt/tstfs2/# mkdir /mnt/tstfs2/mike512K/# setfattr -n ceph.dir.layout -v "stripe_unit=524288 stripe_count=8 object_size=4194304 pool=cephfs_data2" /mnt/tstfs2/mike512K/ 配置目录的attribute，默认其子目录和文件都会集成该目录的 123456789101112# touch /mnt/tstfs2/mike512K/tstfile# getfattr -d -m ceph /mnt/tstfs2/mike512Kgetfattr: Removing leading '/' from absolute path names# file: mnt/tstfs2/mike512Kceph.dir.entries="1"ceph.dir.files="1"ceph.dir.rbytes="4194304000"ceph.dir.rctime="1495766140.09204154946"ceph.dir.rentries="2"ceph.dir.rfiles="1"ceph.dir.rsubdirs="1"ceph.dir.subdirs=“0" 验证file stripe查看file的location 12345678910111213141516171819202122# dd if=/dev/zero of= /mnt/tstfs2/mike512K/tstfile bs=4M count=100# cephfs /mnt/tstfs2/mike512K/tstfile show_locationWARNING: This tool is deprecated. Use the layout.* xattrs to query and modify layouts.location.file_offset: 0 // file的偏移location.object_offset:0 // object的偏移location.object_no: 0 // object的numberlocation.object_size: 4194304 // object size为4Mlocation.object_name: 10000002356.00000000 // object的namelocation.block_offset: 0 // block的偏移location.block_size: 524288 // block size为512klocation.osd: 0 // 存储在osd 0 上# cephfs /mnt/tstfs2/mike512K/tstfile show_location -l 524288WARNING: This tool is deprecated. Use the layout.* xattrs to query and modify layouts.location.file_offset: 524288 // file的偏移location.object_offset:0 // object的偏移location.object_no: 1 // object的numberlocation.object_size: 4194304 // object size为4Mlocation.object_name: 10000002356.00000001 // object的namelocation.block_offset: 0 // block的偏移location.block_size: 524288 // block size为512klocation.osd: 24 // 存储在osd 24 上 查看osd上的object 12345# cd /var/lib/ceph/osd/ceph-0/current/# find . -name "*10000002356.0000000*"./14.126_head/10000002356.00000000__head_8CE99726__e# ll -h ./14.126_head/10000002356.00000000__head_8CE99726__e-rw-r--r-- 1 ceph ceph 4.0M May 26 10:35 ./14.126_head/10000002356.00000000__head_8CE99726__e 参考http://docs.ceph.com/docs/jewel/architecture/#data-stripinghttp://docs.ceph.com/docs/jewel/cephfs/file-layouts/]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>cephfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对象存储跨区灾备 - RGW MultiSite]]></title>
    <url>%2F2017%2F06%2F12%2Fceph-radosgw-multisite%2F</url>
    <content type="text"><![CDATA[需求实现对象存储的跨区灾备，提供应用的高可用。 概述针对对象存储的跨区灾备，在Jewel版本之前，需要通过独立的radosgw-agent进程来实现，配置步骤参考：对象存储跨机房容灾调研 但在Jewel版本中，RGW直接通过radosgw进程实现对象存储的跨区灾备，不需要额外的radosgw-agent进程，同时一些概念也有所变化；例如替换region为zonegroup，提出新的realm，period概念等； 配置参考http://docs.ceph.com/docs/jewel/radosgw/multisite/ https://access.redhat.com/documentation/en/red-hat-ceph-storage/2/paged/object-gateway-guide-for-red-hat-enterprise-linux/chapter-8-multi-site ceph官网的配置multisite的文档不完善，还有一些错误；参考RedHat的文档比较好 环境Ceph版本：Jewel 10.2.3 Ceph Cluster：两个Ceph Cluster集群（虚拟机搭建） 配置前提两个Ceph Cluster集群，每个上面选择一台机器提供radosgw服务； cluster 节点 radosgw节点 Ceph集群1 ceph21, ceph22, ceph23 ceph21 Ceph集群2 ceph31, ceph32, ceph33 ceph31 规划对RGW MULTISITE来说，在一个realm里，需要配置一个master zonegroup，一个或多个secondary zonegroups（貌似可以配置多个独立的zonegroup）；在一个zonegroup中需要配置一个master zone，一个或多个secondary zones； 而在我们的应用中，我们只需要北京的两个sites做灾备，所以我们的配置如下： 配置一个realm，包含一个master zonegroup，里面配置ceph集群1作为master zone，ceph集群2作为secondary zone； 步骤ceph21节点配置master zone 创建需要的pools 12345678910111213141516root@ceph21:~/mikeyang/rgw# cat rgwPoolCreate.sh#!/bin/bashceph osd pool create .rgw.root 32ceph osd pool create bj-zone02.rgw.control 32 32ceph osd pool create bj-zone02.rgw.data.root 32 32ceph osd pool create bj-zone02.rgw.gc 32 32ceph osd pool create bj-zone02.rgw.log 32 32ceph osd pool create bj-zone02.rgw.intent-log 32 32ceph osd pool create bj-zone02.rgw.usage 32 32ceph osd pool create bj-zone02.rgw.users.keys 32 32ceph osd pool create bj-zone02.rgw.users.email 32 32ceph osd pool create bj-zone02.rgw.users.swift 32 32ceph osd pool create bj-zone02.rgw.users.uid 32 32ceph osd pool create bj-zone02.rgw.buckets.index 32 32ceph osd pool create bj-zone02.rgw.buckets.data 32 32ceph osd pool create bj-zone02.rgw.meta 32 32 创建realm，zonegroup和master zone 1234# create realm, zonegroup and zoneradosgw-admin realm create --rgw-realm=cloudin --defaultradosgw-admin zonegroup create --rgw-zonegroup=bj --endpoints=http://&lt;self-ip&gt;:80 --rgw-realm=cloudin --master --defaultradosgw-admin zone create --rgw-zonegroup=bj --rgw-zone=bj-zone02 --endpoints=http://&lt;self-ip&gt;:80 --default --master 删除default的zonegrou，zone，更新period 1234567# remove default zonegroup and zone, which maybe not neededradosgw-admin zonegroup remove --rgw-zonegroup=default --rgw-zone=defaultradosgw-admin period update --commitradosgw-admin zone delete --rgw-zone=defaultradosgw-admin period update --commitradosgw-admin zonegroup delete --rgw-zonegroup=defaultradosgw-admin period update --commit 创建同步需要的user 123radosgw-admin user create --uid=zone.user --display-name="Zone User" --systemradosgw-admin zone modify --rgw-zone=bj-zone02 --access-key=&#123;system-key&#125; --secret=&#123;secret&#125;radosgw-admin period update --commit 注释：–access-key={system-key} –secret={secret}，对应user create中的access-key, secret 修改ceph.conf，启动radosgw 1234567vim /etc/ceph/ceph.conf[client.rgw.ceph21] host = ceph21 rgw_frontends = "civetweb port=80" rgw_zone=bj-zone02 service radosgw start id=rgw.ceph21 ceph31节点配置secondary zone 创建需要的pools 12345678910111213141516root@ceph31:~/mikeyang/rgw# cat rgwPoolCreate.sh#!/bin/bashceph osd pool create .rgw.root 32ceph osd pool create bj-zone03.rgw.control 32 32ceph osd pool create bj-zone03.rgw.data.root 32 32ceph osd pool create bj-zone03.rgw.gc 32 32ceph osd pool create bj-zone03.rgw.log 32 32ceph osd pool create bj-zone03.rgw.intent-log 32 32ceph osd pool create bj-zone03.rgw.usage 32 32ceph osd pool create bj-zone03.rgw.users.keys 32 32ceph osd pool create bj-zone03.rgw.users.email 32 32ceph osd pool create bj-zone03.rgw.users.swift 32 32ceph osd pool create bj-zone03.rgw.users.uid 32 32ceph osd pool create bj-zone03.rgw.buckets.index 32 32ceph osd pool create bj-zone03.rgw.buckets.data 32 32ceph osd pool create bj-zone03.rgw.meta 32 32 获取master zone的realm，zonegroup，period信息 123radosgw-admin realm pull --url=http://&lt;master-zone-ip&gt;:80 --access-key=&#123;system-key&#125; --secret=&#123;secret&#125;radosgw-admin realm default --rgw-realm=cloudinradosgw-admin period pull --url=http://&lt;master-zone-ip&gt;:80 --access-key=&#123;system-key&#125; --secret=&#123;secret&#125; 注释：–access-key={system-key} –secret={secret}，这部分是master zone的同步user的access-key, secret 创建secondary zone 123radosgw-admin zone create --rgw-zonegroup=bj --rgw-zone=bj-zone03 --endpoints=http://&lt;self-ip&gt;:80 --access-key=&#123;system-key&#125; --secret=&#123;secret&#125;radosgw-admin zone delete --rgw-zone=defaultradosgw-admin period update --commit 注释：–access-key={system-key} –secret={secret}，这部分是master zone的同步user的access-key, secret 修改ceph.conf，启动radosgw 1234567vim /etc/ceph/ceph.conf[client.rgw.ceph31] host = ceph31 rgw_frontends = "civetweb port=80" rgw_zone=bj-zone03 service radosgw start id=rgw.ceph31 检查集群状态 master zone节点检查 12345678910111213root@ceph21:~/mikeyang/rgw# radosgw-admin sync status2016-10-26 11:18:45.124701 7fd18c502900 0 error in read_id for id : (2) No such file or directory2016-10-26 11:18:45.125156 7fd18c502900 0 error in read_id for id : (2) No such file or directory realm 0b64b20e-2a90-4fc4-a1d6-57fc67457564 (cloudin) zonegroup 1bfc8ccd-01ae-477e-a332-af4cb00d3f20 (bj) zone 9f621425-cd68-4d2f-b3e7-e85814caef2c (bj-zone02) metadata sync no sync (zone is master) data sync source: 249b96bd-8f86-4326-80e0-7fceca78dec1 (bj-zone03) syncing full sync: 0/128 shards incremental sync: 128/128 shards data is caught up with sourceroot@ceph21:~/mikeyang/rgw# secondary zone节点检查 123456789101112131415root@ceph31:~/mikeyang/rgw# radosgw-admin sync status2016-10-26 11:19:41.715683 7fa016bef900 0 error in read_id for id : (2) No such file or directory2016-10-26 11:19:41.716289 7fa016bef900 0 error in read_id for id : (2) No such file or directory realm 0b64b20e-2a90-4fc4-a1d6-57fc67457564 (cloudin) zonegroup 1bfc8ccd-01ae-477e-a332-af4cb00d3f20 (bj) zone 249b96bd-8f86-4326-80e0-7fceca78dec1 (bj-zone03) metadata sync syncing full sync: 0/64 shards metadata is caught up with master incremental sync: 64/64 shards data sync source: 9f621425-cd68-4d2f-b3e7-e85814caef2c (bj-zone02) syncing full sync: 0/128 shards incremental sync: 128/128 shards data is caught up with source 注释：上述输出中有两条error log输出，这个是radosgw的一个bug，还没修复，对正确性没有影响。 数据迁移之前说过，Jewel版本的radosgw对应的pool和使用方式都跟Hammer有不少区别，所以在从Hammer升级到Jewel后，需要考虑之前的对象存储数据迁移； 鉴于我们现在对象存储仅仅RDS在使用，数据量也不大，所以在中断对象存储服务的情况下，迁移是比较容易完成的，步骤如下： 导出现有的user信息； 12345678910111213141516171819202122232425262728293031323334353637openstack@BJBGP02-02-01:~$ radosgw-admin metadata list user[ "cloudInS3User"]openstack@BJBGP02-02-01:~$ radosgw-admin user info --uid=cloudInS3User&#123; "user_id": "cloudInS3User", "display_name": "user for s3", "email": "", "suspended": 0, "max_buckets": 1000, "auid": 0, "subusers": [], "keys": [ &#123; "user": "cloudInS3User", "access_key": "N8JYDX1HCAM55XDWGL10", "secret_key": "uRpR6EbR9YkyjfvdQmuJq0VgEx1a0KONl0NCKlJ9" &#125; ], "swift_keys": [], "caps": [], "op_mask": "read, write, delete", "default_placement": "", "placement_tags": [], "bucket_quota": &#123; "enabled": false, "max_size_kb": -1, "max_objects": -1 &#125;, "user_quota": &#123; "enabled": false, "max_size_kb": -1, "max_objects": -1 &#125;, "temp_url_keys": []&#125; 当前系统只有一个用于RDS备份的user：cloudInS3User 导出现有user下的bucket信息； 1234openstack@BJBGP02-02-01:~$ radosgw-admin metadata list bucket[ "database_backups"] 导出现有bucket下的对象； 123456 ictfox@YangGuanjun-MacBook-Pro:~$ s3cmd ls s3://database_backups 2016-09-01 08:22 276864 s3://database_backups/03f55add-fda5-404e-99b2-04b34b32ec56.xbstream.gz.enc 2016-09-05 03:14 279232 s3://database_backups/147a2c80-0f7d-4ecd-9c4a-e9a48b7cd703.xbstream.gz.enc... 2016-09-01 09:38 278320 s3://database_backups/dbfee332-5f4e-4596-bef6-63797a24d9e0.xbstream.gz.enc 2016-08-31 09:54 276144 s3://database_backups/f703131f-7dd7-43a6-9b20-7fe33a9fbe59.xbstream.gz.enc 然后通过命令把bucket下的对象保存下来：s3cmd get s3://database_backups/OBJECT LOCAL_FILE radosgw升级后恢复数据 1# radosgw-admin user create --uid="cloudInS3User" --display-name="user for s3" --access-key=N8JYDX1HCAM55XDWGL10 --secret=uRpR6EbR9YkyjfvdQmuJq0VgEx1a0KONl0NCKlJ9 通过测试的RDS实例创建一备份，会自动创建bucket: database_backups 然后恢复第3步中备份下来的各个对象文件 1# s3cmd put FILE [FILE...] s3://database_backups/]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>radosgw</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cephfs kernel client debug]]></title>
    <url>%2F2017%2F06%2F08%2Fcephfs-client-debug%2F</url>
    <content type="text"><![CDATA[获取源码12# uname -aLinux Server 3.10.0-327.el7.x86_64 #1 SMP Thu Nov 19 22:10:57 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux 从网上搜索下载Linux对应版本的源码，这里的源码版本为：3.10.0-327 查看cephfs client端模块123456789101112131415161718192021222324252627282930313233343536# lsmod | grep cephceph 202130 1libceph 189287 2 rbd,ceph# modinfo cephfilename: /lib/modules/3.10.0-327.el7.x86_64/kernel/fs/ceph/ceph.kolicense: GPLdescription: Ceph filesystem for Linuxauthor: Patience Warnick &lt;patience@newdream.net&gt;author: Yehuda Sadeh &lt;yehuda@hq.newdream.net&gt;author: Sage Weil &lt;sage@newdream.net&gt;alias: fs-cephrhelversion: 7.2srcversion: 268CE83A90FA60A7654BE27depends: libcephintree: Yvermagic: 3.10.0-327.el7.x86_64 SMP mod_unload modversionssigner: CentOS Linux kernel signing keysig_key: 79:AD:88:6A:11:3C:A0:22:35:26:33:6C:0F:82:5B:8A:94:29:6A:B3sig_hashalgo: sha25# modinfo libcephfilename: /lib/modules/3.10.0-327.el7.x86_64/kernel/net/ceph/libceph.kolicense: GPLdescription: Ceph filesystem for Linuxauthor: Patience Warnick &lt;patience@newdream.net&gt;author: Yehuda Sadeh &lt;yehuda@hq.newdream.net&gt;author: Sage Weil &lt;sage@newdream.net&gt;rhelversion: 7.2srcversion: BA3AB21E57822D9AC6B9463depends: libcrc32c,dns_resolverintree: Yvermagic: 3.10.0-327.el7.x86_64 SMP mod_unload modversionssigner: CentOS Linux kernel signing keysig_key: 79:AD:88:6A:11:3C:A0:22:35:26:33:6C:0F:82:5B:8A:94:29:6A:B3sig_hashalgo: sha256 修改cephfs源码12345678910111213141516171819202122到kernel源码目录# vim include/linux/ceph/ceph_debug.h...# if defined(DEBUG) || defined(CONFIG_DYNAMIC_DEBUG)/* 删除这块代码extern const char *ceph_file_part(const char *s, int len);# define dout(fmt, ...) \ pr_debug("%.*s %12.12s:%-4d : " fmt, \ 8 - (int)sizeof(KBUILD_MODNAME), " ", \ ceph_file_part(__FILE__, sizeof(__FILE__)), \ __LINE__, ##__VA_ARGS__)# else*//* faux printk call just to see any compiler warnings. */# define dout(fmt, ...) do &#123; \ if (1) \ // if(0) 修改为 if(1) printk(KERN_DEBUG fmt, ##__VA_ARGS__); \ &#125; while (0)# endif#else... 结合自己版本里的源码，随意修改，使dout()函数能正常打印log就ok； 123456[root@server linux-3.10.0-327.el7]# cp /boot/config-3.10.0-327.el7.x86_64 ./[root@server linux-3.10.0-327.el7]# mv config-3.10.0-327.el7.x86_64 .config[root@server linux-3.10.0-327.el7]# vim .config...CONFIG_CEPH_LIB_PRETTYDEBUG=y // 打开CEPH的相关DEBUG配置... 编译并替换cephfs的模块编译cephfs使用的kernel module 1234567[root@server linux-3.10.0-327.el7]# make[root@server linux-3.10.0-327.el7]# ll fs/ceph/ceph.ko-rw-r--r-- 1 root root 4901829 Jun 7 09:39 fs/ceph/ceph.ko[root@server linux-3.10.0-327.el7]# ll net/ceph/libceph.ko-rw-r--r-- 1 root root 4099969 Jun 7 09:40 net/ceph/libceph.ko[root@server linux-3.10.0-327.el7]# ll drivers/block/rbd.ko-rw-r--r-- 1 root root 966206 Jun 7 09:37 drivers/block/rbd.ko 替换cephfs的这几个模块 1234567891011121314[root@server linux-3.10.0-327.el7]# lsmod | grep rbdrbd 58242 0libceph 189287 2 rbd,ceph // rbd使用libceph module，替换libceph的话，先要删除rbd module[root@server linux-3.10.0-327.el7]# rmmod rbd[root@server linux-3.10.0-327.el7]# rmmod ceph[root@server linux-3.10.0-327.el7]# rmmod libceph[root@server linux-3.10.0-327.el7]# cp net/ceph/libceph.ko /lib/modules/3.10.0-327.el7.x86_64/kernel/net/ceph/libceph.ko[root@server linux-3.10.0-327.el7]# cp fs/ceph/ceph.ko /lib/modules/3.10.0-327.el7.x86_64/kernel/fs/ceph/ceph.ko[root@server linux-3.10.0-327.el7]# cp drivers/block/rbd.ko /lib/modules/3.10.0-327.el7.x86_64/kernel/drivers/block/rbd.ko[root@server linux-3.10.0-327.el7]# modprobe ceph[root@server linux-3.10.0-327.el7]# modprobe rbd 并不是每次修改cephfs的代码，都需要编译整个kernel的，后续仅仅需要编译对应的module即可 1[root@server linux-3.10.0-327.el7]# make M=fs/ceph 测试查看cephfs debug logmount cephfs并触发写操作 123# mount -t ceph 10.10.1.1:6789:/ /mnt/# cd /mnt# dd if=/dev/zero of=tstfile bs=4M count=1 oflag=direct 检查cephfs的kernel log 12345678910111213141516171819# dmesg -c &gt; /dev/null# dmesg -c &gt; ~/cephfs.log# vim ~/cephfs.log...[3034497.087430] aio_write ffff8801271b0e48 10000002393.fffffffffffffffe 0~4194304 got cap refs on Fwb[3034497.087432] sync_direct_write on file ffff882013dddf00 0~4194304[3034497.087449] ceph_msg_new ffff88135e1ec000 front 512[3034497.087450] ceph_msg_new ffff88135e1ec0f0 front 277[3034497.087451] mapping 0~4194304 osize 4194304 fl_su 4194304[3034497.087452] osize 4194304 / su 4194304 = su_per_object 1[3034497.087454] off 0 / su 4194304 = bl 0[3034497.087455] objset 0 * sc 1 = ono 0[3034497.087455] obj extent 0~4194304[3034497.087457] calc_layout objnum=0 0~4194304[3034497.087576] oid '10000002393.00000000' len 20[3034497.087578] build_request msg_size was 197[3034497.087580] __register_request ffff88111d798000 tid 8[3034497.087585] ceph_osdc_get_request ffff88111d798000 (was 1)... 配置kernel log文件若不想每次从dmesg里获取kernel debug log，在centos里可以配置到指定文件 123456# vim /etc/rsyslog.conf...kern.* /var/log/kern.log // 配置kernel log文件...# service rsyslog restart // 重启rsyslog服务 之后就可以在/var/log/kern.log里查看cephfs的debug log了]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>cephfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cephfs介绍和功能测试]]></title>
    <url>%2F2017%2F06%2F02%2Fcephfs-intro-fun%2F</url>
    <content type="text"><![CDATA[cephfs简介cephfs是ceph提供的兼容POSIX协议的文件系统，对比rbd和rgw功能，这个是ceph里最晚满足production ready的一个功能，它底层还是使用rados存储数据 cephfs的架构 使用cephfs的两种方式 cephfs kernel module cephfs-fuse 从上面的架构可以看出，cephfs-fuse的IO path比较长，性能会比cephfs kernel module的方式差一些； client端访问cephfs的流程 client端与mds节点通讯，获取metadata信息（metadata也存在osd上） client直接写数据到osd mds部署使用ceph-deploy部署ceph mds很方便，只需要简单的一条命令就搞定，不过它依赖之前ceph-deploy时候生成的一些配置和keyring文件； 在之前部署ceph集群的节点目录，执行ceph-deploy mds create： 12345# ceph-deploy --overwrite-conf mds create server1:mds-daemon-1// 去节点检查下daemon[root@server1 yangguanjun]# ps aux | grep ceph-mdsceph 1138 0.0 0.0 3011880 14301 ? Ssl 10:21 0:00 /usr/bin/ceph-mds -f --cluster ceph --id mds-daemon-2 --setuser ceph --setgroup ceph 创建cephfs创建第一个cephfs12345678# ceph osd pool create cephfs_data 512 512 // 创建data poolpool 'cephfs_data' created# ceph osd pool create cephfs_metadata 512 512 // 创建metadata poolpool 'cephfs_metadata' created# ceph fs new tstfs cephfs_metadata cephfs_data // 创建cephfsnew fs with metadata pool 10 and data pool 9# ceph fs lsname: tstfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ] 创建第二个cephfs默认cephfs是不支持多个fs的，这个还是试验阶段的feature，需要打开 enable_multiple 的flag 123456789101112# ceph osd pool create cephfs_metadata2 512 512pool 'cephfs_metadata2’ created# ceph osd pool create cephfs_data2 512 512pool 'cephfs_data2’ created# ceph fs new tstfs2 cephfs_metadata2 cephfs_data2Error EINVAL: Creation of multiple filesystems is disabled. To enable this experimental feature, use 'ceph fs flag set enable_multiple true'# ceph fs flag set enable_multiple trueWarning! This feature is experimental.It may cause problems up to and including data loss.Consult the documentation at ceph.com, and if unsure, do not proceed.Add --yes-i-really-mean-it if you are certain.# ceph fs flag set enable_multiple true --yes-i-really-mean-it# ceph fs new tstfs2 cephfs_metadata2 cephfs_data2new fs with metadata pool 11 and data pool 12 查看mds状态ceph的mds是一个单独的daemon，它只能服务于一个cephfs，若cephfs指定多个rank了，它只能服务于其中一个rank 12# ceph mds state8: tstfs-1/1/1 up tstfs2-0/0/1 up &#123;[tstfs:0]=mds-daemon-1=up:active&#125; 对输出解释如下： e8 : e标识epoch，8是epoch号 tstfs-1/1/1 up : tstfs是cephfs名字，后面的三个1分别是mds_map.in/mds_map.up/mds_map.max_mds，up是cephfs状态 {[tstfs:0]=mds-daemon-1=up:active} : [tstfs:0]指tstfs的rank 0，mds-daemon-1是服务tstfs的mds daemon name，up:active是cephfs的状态为 up &amp; active 从上面的输出可以看出，两个cephfs只有tstfs是active的，它的mds daemon为mds-daemon-1 在ceph-deploy节点添加mds-daemon-2-112# ceph mds state11: tstfs-1/1/1 up tstfs2-1/1/1 up &#123;[tstfs2:0]=mds-daemon-2-1=up:active,[tstfs:0]=mds-daemon-1=up:active&#125; 添加新的mds daemon后，它会自动服务于一个没有mds daemon的cephfs 在ceph-deploy节点添加mds-daemon-2-212# ceph mds state12: tstfs-1/1/1 up tstfs2-1/1/1 up &#123;[tstfs2:0]=mds-daemon-2=up:active,[tstfs:0]=mds-daemon=up:active&#125;, 1 up:standby 又添加一个新的mds daemon后，它会处于standby状态，若前两个mds daemon出问题，它会顶替上去，顶替的规则可以配置，详情参考文章：http://docs.ceph.com/docs/master/cephfs/standby/#configuring-standby-daemons 查看节点上的两个mds daemon进程 123[root@server2 yangguanjun]# ps aux | grep ceph-mdsceph 2362 0.0 0.0 3061884 14604 ? Ssl 10:26 0:00 /usr/bin/ceph-mds -f --cluster ceph --id mds-daemon-2-1 --setuser ceph --setgroup cephceph 3031 0.0 0.0 3390588 13872 ? Ssl 10:27 0:00 /usr/bin/ceph-mds -f --cluster ceph --id mds-daemon-2-2 --setuser ceph --setgroup ceph cephfs的使用mount &amp; umount1234# mount -t ceph 10.10.1.2:6789:/ /mnt/tstfs2/# umount /mnt/tstfs2# mount | grep tstfs210.10.1.1:6789:/ on /mnt/tstfs2 type ceph (rw,relatime) 是否支持多个cephfs？前面我们提到可以在一个ceph cluster里创建多个cephfs，指定不同的data/metadata pool，有不同的mds daemon服务，但如何使用不同的cephfs呢？ kernel cephfs 12# mount -t ceph 10.10.1.2:6789:/ /mnt/tstfs2/ -o mds_namespace=tstfsmount error 22 = Invalid argument 这个问题的bug信息：http://tracker.ceph.com/issues/18161 ceph-fuse 待验证 查看cephfs状态12345678910111213141516171819202122232425# ceph fs get tstfsFilesystem 'tstfs' (1)fs_name tstfsepoch 13flags 0created 2017-05-23 10:21:55.889234modified 2017-05-23 10:21:55.889234tableserver 0root 0session_timeout 60session_autoclose 300max_file_size 1099511627776last_failure 0last_failure_osd_epoch 0compat compat=&#123;&#125;,rocompat=&#123;&#125;,incompat=&#123;1=base v0.20,2=client writeable ranges,3=default file layouts on dirs,4=dir inode in separate object,5=mds uses versioned encoding,6=dirfrag is stored in omap,8=file layout v2&#125;max_mds 1in 0up &#123;0=4456&#125;faileddamagedstoppeddata_pools 9metadata_pool 10inline_data disabled4456: 10.10.1.1:6820/1655250084 'mds-daemon-1' mds.0.4 up:active seq 484 配置cephfs的multi mdscephfs的multi mds属性还不是production ready，不要用在生成环境哦，自己测试下玩玩就行 123456# ceph mds state13: tstfs-1/1/1 up tstfs2-1/1/1 up &#123;[tstfs2:0]=mds-daemon-2-1=up:active,[tstfs:0]=mds-daemon-1=up:active&#125;, 1 up:standby# ceph fs set tstfs allow_multimds true --yes-i-really-mean-it# ceph fs set tstfs max_mds 2# ceph mds state17: tstfs-2/2/2 up tstfs2-1/1/1 up &#123;[tstfs2:0]=mds-daemon-2-1=up:active,[tstfs:0]=mds-daemon-1=up:active,[tstfs:1]=mds-daemon-2-2=up:active&#125; 从上面输出可以看出，设置tstfs的max_mds为2后，它会自动寻找一个standby的mds daemon服务，现在看到的tstfs的信息为：tstfs-2/2/2 up和[tstfs:0]=mds-daemon-1=up:active,[tstfs:1]=mds-daemon-2-2=up:active 删除cephfs和mds12345678910111213141516171819202122机器上停止ceph mds服务# systemctl stop ceph-mds.target删除cephfs，有mds daemons的cephfs删除会报错，然后去mds daemon机器上停止mds服务即可# ceph fs rm tstfsError EINVAL: all MDS daemons must be inactive before removing filesystem# ceph fs rm tstfs2Error EPERM: this is a DESTRUCTIVE operation and will make data in your filesystem permanentlyinaccessible. Add --yes-i-really-mean-it if you are sure you wish to continue.# ceph fs rm tstfs2 --yes-i-really-mean-it# ceph fs rm tstfs --yes-i-really-mean-it删除ceph nonactive mds，mds的id默认从0开始，指定不存在的id并不会报错# ceph mds rm 0mds gid 0 dne# ceph mds rm 1mds gid 1 dne# ceph mds rm 2mds gid 2 dne删除cephfs使用的pool# ceph osd pool delete cephfs_metadata cephfs_metadata --yes-i-really-really-mean-it... 参考http://docs.ceph.com/docs/master/cephfs/https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/2/html-single/ceph_file_system_guide_technology_preview/]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>cephfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ceph PG Inconsistent Error处理]]></title>
    <url>%2F2017%2F05%2F31%2Fceph-pg-inconsistent-error%2F</url>
    <content type="text"><![CDATA[问题ceph集群状态为HEALTH_ERR，ceph -s显示有pg状态不一致，ceph health detail输出如下： 12345# ceph health detailHEALTH_ERR 2 pgs inconsistent; 1 pgs repair; 8 scrub errorspg 14.16a is active+clean+scrubbing+deep+inconsistent+repair, acting [1,27,16]pg 15.118 is active+clean+inconsistent, acting [0,15,27]8 scrub errors 分析 &amp; 解决 手动执行pg修复 ceph pg repair 14.16a ceph pg deep-scrub 14.16a 结果：集群状态依旧HEALTH_ERR 重启对应osd daemon systemctl restart ceph-osd@&lt;osdid&gt;.service 结果：集群状态依旧HEALTH_ERR 检查ceph-osd log 12345678910# vim ceph-osd.1.log-20170531.gz...2017-05-31 02:30:14.166348 7f7aeefff700 -1 log_channel(cluster) log [ERR] : 14.16a shard 27: soid 14:56a14fcc:::10000002380.0000061d:head candidate had a read error2017-05-31 02:30:14.166358 7f7aeefff700 -1 log_channel(cluster) log [ERR] : 14.16a shard 27: soid 14:56a7ab13:::10000002380.00000446:head candidate had a read error2017-05-31 02:30:14.166361 7f7aeefff700 -1 log_channel(cluster) log [ERR] : 14.16a shard 27: soid 14:56b34a7a:::10000002356.00000218:head candidate had a read error2017-05-31 02:30:14.166363 7f7aeefff700 -1 log_channel(cluster) log [ERR] : 14.16a shard 27: soid 14:56c557e8:::10000002380.0000007a:head candidate had a read error2017-05-31 02:30:14.166366 7f7aeefff700 -1 log_channel(cluster) log [ERR] : 14.16a shard 27: soid 14:56c8ceeb:::10000002380.00000854:head candidate had a read error2017-05-31 02:30:14.166372 7f7aeefff700 -1 log_channel(cluster) log [ERR] : 14.16a shard 27: soid 14:56f50799:::1000000237c.0000019a:head candidate had a read error2017-05-31 02:30:14.168485 7f7acebff700 -1 log_channel(cluster) log [ERR] : 14.16a deep-scrub 0 missing, 6 inconsistent objects2017-05-31 02:30:14.168499 7f7acebff700 -1 log_channel(cluster) log [ERR] : 14.16a deep-scrub 6 errors 上面可以看出pg 14.16a里有几个objects报告candidate had a read error 查看出错的object的md5值 12345678# md5sum /var/lib/ceph/osd/ceph-1/current/14.16a_head/10000002380.0000061d__head_33F2856A__edc191d3144c49077952ed059425d68b1 /var/lib/ceph/osd/ceph-1/current/14.16a_head/10000002380.0000061d__head_33F2856A__e# md5sum /var/lib/ceph/osd/ceph-16/current/14.16a_head/10000002380.0000061d__head_33F2856A__edc191d3144c49077952ed059425d68b1 /var/lib/ceph/osd/ceph-16/current/14.16a_head/10000002380.0000061d__head_33F2856A__e# md5sum /var/lib/ceph/osd/ceph-27/current/14.16a_head/10000002380.0000061d__head_33F2856A__emd5sum: /var/lib/ceph/osd/ceph-27/current/14.16a_head/10000002380.0000061d__head_33F2856A__e: Input/output error 上面得出ceph-27上的object获取md5值失败，报Input/output error，这里猜测其对应的磁盘有问题； 查看ceph-27上pg 14.16a里别的object的md5值，输出正常，则证明磁盘还能工作，可能是文件有损坏，也可能磁盘上有部分坏道； 删除md5sum报IO错误的文件，然后执行ceph pg repair 14.16a，pg状态恢复正常； 磁盘检查与恢复那磁盘是否有问题呢？执行以下步骤来检测并恢复： 12345678# systemctl stop ceph-osd@27.service# umount /dev/sdi1# badblocks -svn /dev/sdi1 -o badblocks.logChecking for bad blocks in non-destructive read-write modeFrom block 0 to 3906984774Checking for bad blocks (non-destructive read-write test)Testing with random pattern: 0.00% done, 16:22 elapsed. (223/0/0 errors) 上述检查会花费比较长时间，一般SATA盘的read速度为100-120MB/s，4TB大小的SATA盘，约需要4*1024*1024/120/3600 = 9.7小时； 不过你可以在别的窗口查看输出文件badblocks.log的内容，如果里面有信息，那证明你的磁盘确实有坏块，就需要尝试恢复检查了: 123456789101112131415# head -n 10 badblocks.log10624106251062610627106281062910630106311063210633kill badblocks检查进程# ps aux | pgrep badblocks | xargs kill -9[1]+ Killed badblocks -v /dev/sdi1 &gt; badblocks 通过badblocks修复坏块，注意会丢失坏块里的数据 12345678910111213141516# badblocks -ws /dev/sdi1 10633 10624Testing with pattern 0xaa: doneReading and comparing: doneTesting with pattern 0x55: doneReading and comparing: doneTesting with pattern 0xff: doneReading and comparing: doneTesting with pattern 0x00: doneReading and comparing: done# badblocks -svn /dev/sdi1 10633 10624Checking for bad blocks in non-destructive read-write modeFrom block 10624 to 10633Checking for bad blocks (non-destructive read-write test)Testing with random pattern: donePass completed, 0 bad blocks found. (0/0/0 errors) 通过上述步骤恢复了磁盘中的坏块，有如下两种情况： 坏块中有xfs的元数据信息 执行xfs_repair修复磁盘上的xfs系统，很可能报错； 没有办法，只能重新格式化磁盘，然后执行ceph的recovery了； 坏块中无xfs的元数据信息 通过xfs_repair修复磁盘上的xfs系统，报告done 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697# xfs_repair -f /dev/sdi1Phase 1 - find and verify superblock...Cannot get host filesystem geometry.Repair may fail if there is a sector size mismatch betweenthe image and the host filesystem. - reporting progress in intervals of 15 minutesPhase 2 - using internal log - zero log... - scan filesystem freespace and inode maps... - 15:23:37: scanning filesystem freespace - 32 of 32 allocation groups done - found root inode chunkPhase 3 - for each AG... - scan and clear agi unlinked lists... - 15:23:37: scanning agi unlinked lists - 32 of 32 allocation groups done - process known inodes and perform inode discovery... - agno = 15 - agno = 0 - agno = 30 - agno = 31 - agno = 16 - agno = 1 - agno = 2 - agno = 17 - agno = 18 - agno = 19 - agno = 3 - agno = 4 - agno = 5 - agno = 6 - agno = 7 - agno = 20 - agno = 21 - agno = 22 - agno = 23 - agno = 24 - agno = 25 - agno = 26 - agno = 8 - agno = 9 - agno = 10 - agno = 11 - agno = 12 - agno = 27 - agno = 28 - agno = 29 - agno = 13 - agno = 14 - 15:23:42: process known inodes and inode discovery - 3584 of 3584 inodes done - process newly discovered inodes... - 15:23:42: process newly discovered inodes - 32 of 32 allocation groups donePhase 4 - check for duplicate blocks... - setting up duplicate extent list... - 15:23:42: setting up duplicate extent list - 32 of 32 allocation groups done - check for inodes claiming duplicate blocks... - agno = 0 - agno = 1 - agno = 2 - agno = 3 - agno = 4 - agno = 5 - agno = 7 - agno = 6 - agno = 8 - agno = 11 - agno = 15 - agno = 19 - agno = 22 - agno = 24 - agno = 14 - agno = 28 - agno = 16 - agno = 18 - agno = 31 - agno = 20 - agno = 9 - agno = 10 - agno = 21 - agno = 23 - agno = 12 - agno = 13 - agno = 25 - agno = 26 - agno = 27 - agno = 17 - agno = 30 - agno = 29 - 15:23:42: check for inodes claiming duplicate blocks - 3584 of 3584 inodes donePhase 5 - rebuild AG headers and trees... - 15:23:42: rebuild AG headers and trees - 32 of 32 allocation groups done - reset superblock...Phase 6 - check inode connectivity... - resetting contents of realtime bitmap and summary inodes - traversing filesystem ... - traversal finished ... - moving disconnected inodes to lost+found ...Phase 7 - verify and correct link counts...done 这时mount，start ceph-osd damon后，ceph报HEALTH_OK，但其实osd上的部分数据已经丢失，这个等ceph的deep scrub触发后会自动修复； 参考http://ceph.com/planet/ceph-manually-repair-object/https://linux.cn/article-7961-1.html]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ceph BackoffThrottle分析]]></title>
    <url>%2F2017%2F05%2F25%2FCeph-BackoffThrottle%2F</url>
    <content type="text"><![CDATA[概述本文讨论下Ceph在Jewel中引入的 dynamic throttle：BackoffThrottle；分析后优化Ceph filestore，journal相关的throttle配置； 参考文章： http://blog.wjin.org/posts/ceph-dynamic-throttle.htmlhttps://fossies.org/linux/ceph/src/doc/dynamic-throttle.txt BackoffThrottleJewel引入了dynamic的throttle，就是代码中BackoffThrottle，现在filestore和Journal都是使用它来做throttle的； 123456789class FileStore&#123; BackoffThrottle throttle_ops, throttle_bytes;&#125; class JournalThrottle &#123; BackoffThrottle throttle;…&#125; BackoffThrottle定义和相关参数如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * BackoffThrottle * * Creates a throttle which gradually induces delays when get() is called * based on params low_threshhold, high_threshhold, expected_throughput, * high_multiple, and max_multiple. * * In [0, low_threshhold), we want no delay. * * In [low_threshhold, high_threshhold), delays should be injected based * on a line from 0 at low_threshhold to * high_multiple * (1/expected_throughput) at high_threshhold. * * In [high_threshhold, 1), we want delays injected based on a line from * (high_multiple * (1/expected_throughput)) at high_threshhold to * (high_multiple * (1/expected_throughput)) + * (max_multiple * (1/expected_throughput)) at 1. * * Let the current throttle ratio (current/max) be r, low_threshhold be l, * high_threshhold be h, high_delay (high_multiple / expected_throughput) be e, * and max_delay (max_muliple / expected_throughput) be m. * * delay = 0, r \in [0, l) * delay = (r - l) * (e / (h - l)), r \in [l, h) * delay = h + (r - h)((m - e)/(1 - h)) */class BackoffThrottle &#123;… /// see above, values are in [0, 1]. double low_threshhold = 0; double high_threshhold = 1; /// see above, values are in seconds double high_delay_per_count = 0; double max_delay_per_count = 0; /// Filled in in set_params double s0 = 0; ///&lt; e / (h - l), l != h, 0 otherwise double s1 = 0; ///&lt; (m - e)/(1 - h), 1 != h, 0 otherwise /// max uint64_t max = 0; uint64_t current = 0;…&#125; filestore throttle举例分析下面以使用BackoffThrottle的filestore throttle举例分析下其参数配置 filestore throttle的相关配置项1234567891011OPTION(filestore_expected_throughput_bytes, OPT_DOUBLE, 200 &lt;&lt; 20)OPTION(filestore_expected_throughput_ops, OPT_DOUBLE, 200) OPTION(filestore_queue_max_bytes, OPT_U64, 100 &lt;&lt; 20)OPTION(filestore_queue_max_ops, OPT_U64, 50) OPTION(filestore_queue_max_delay_multiple, OPT_DOUBLE, 0)OPTION(filestore_queue_high_delay_multiple, OPT_DOUBLE, 0) OPTION(filestore_queue_low_threshhold, OPT_DOUBLE, 0.3)OPTION(filestore_queue_high_threshhold, OPT_DOUBLE, 0.9) 根据配置项初始化BackoffThrottle1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253bool BackoffThrottle::set_params( double _low_threshhold, double _high_threshhold, double _expected_throughput, double _high_multiple, double _max_multiple, uint64_t _throttle_max, ostream *errstream)&#123; low_threshhold = _low_threshhold; high_threshhold = _high_threshhold; high_delay_per_count = _high_multiple / _expected_throughput; max_delay_per_count = _max_multiple / _expected_throughput; max = _throttle_max; if (high_threshhold - low_threshhold &gt; 0) &#123; s0 = high_delay_per_count / (high_threshhold - low_threshhold); &#125; else &#123; low_threshhold = high_threshhold; s0 = 0; &#125; if (1 - high_threshhold &gt; 0) &#123; s1 = (max_delay_per_count - high_delay_per_count) / (1 - high_threshhold); &#125; else &#123; high_threshhold = 1; s1 = 0; &#125;&#125; int FileStore::set_throttle_params()&#123; stringstream ss; bool valid = throttle_bytes.set_params( g_conf-&gt;filestore_queue_low_threshhold, g_conf-&gt;filestore_queue_high_threshhold, g_conf-&gt;filestore_expected_throughput_bytes, g_conf-&gt;filestore_queue_high_delay_multiple, g_conf-&gt;filestore_queue_max_delay_multiple, g_conf-&gt;filestore_queue_max_bytes, &amp;ss); valid &amp;= throttle_ops.set_params( g_conf-&gt;filestore_queue_low_threshhold, g_conf-&gt;filestore_queue_high_threshhold, g_conf-&gt;filestore_expected_throughput_ops, g_conf-&gt;filestore_queue_high_delay_multiple, g_conf-&gt;filestore_queue_max_delay_multiple, g_conf-&gt;filestore_queue_max_ops, &amp;ss);…&#125; 获取delay值123456789101112131415std::chrono::duration&lt;double&gt; BackoffThrottle::_get_delay(uint64_t c) const&#123; if (max == 0) return std::chrono::duration&lt;double&gt;(0); double r = ((double)current) / ((double)max); if (r &lt; low_threshhold) &#123; return std::chrono::duration&lt;double&gt;(0); &#125; else if (r &lt; high_threshhold) &#123; return c * std::chrono::duration&lt;double&gt;( (r - low_threshhold) * s0); &#125; else &#123; return c * std::chrono::duration&lt;double&gt;( high_delay_per_count + ((r - high_threshhold) * s1)); &#125;&#125; 如上述函数描述，分四种情况计算delay值： max = 0时：永远返回 0 current/max &lt; low_threshhold时：返回 0 low_threshhold &lt;= current/max &lt; high_threshhold时：计算一值 high_threshhold &lt;= current/max时：计算一值 如图所示，在第一个区间的时候，也就是压力不大的情况下，delay值为0，是不需要wait的。当压力增大，x落入第二个区间后，delay值开始起作用，并且逐步增大， 当压力过大的时候，会落入第三个区间，这时候delay值增加明显加快，wait值明显增大，尽量减慢io速度，减缓压力，故而得名dynamic throttle。 默认情况下filestore throttle分析filestore有bytes和ops两个throttle，这里以bytes为例分析： 默认情况下： 12filestore_queue_high_delay_multiple = 0filestore_queue_max_delay_multiple = 0 相当于BackoffThrottle中的值如下： 1234567low_threshhold = 0.3high_threshhold = 0.9high_delay_per_count = 0max_delay_per_count = 0s0 = 0s1 = 0max = 100 &lt;&lt; 20 所以默认配置下，是关闭dynamic delay的； 开启dynamic throttle参考最早的代码，配置： 12filestore_queue_high_delay_multiple = 2filestore_queue_max_delay_multiple = 10 其他使用默认值是，BackoffThrottle中的值如下： 1234567low_threshhold = 0.3high_threshhold = 0.9high_delay_per_count = 2/(200 &lt;&lt; 20)max_delay_per_count = 10/(200 &lt;&lt; 20)s0 = (2/(200 &lt;&lt; 20))/0.6s1 = (8/(200 &lt;&lt; 20))/0.1max = 100 &lt;&lt; 20 则此时的delay分为如下几种： c：op-&gt;bytes，即一次请求的数据量current：当前filestore queue的数据量，初始化为 0，每次调用：throttle_bytes.get(o-&gt;bytes)；{ current + = c;} current/max &lt; low_threshhold时： 此时 current &lt; (30 &lt;&lt; 20)；delay = 0 low_threshhold &lt;= current/max &lt; high_threshhold时： 此时 (30 &lt;&lt; 20) &lt;= current &lt; (90 &lt;&lt; 20) delay = c ((current/max - 0.3) s0) a）current = 30 &lt;&lt; 20时：delay = 0 b）current = 90 &lt;&lt; 20时：delay = c / (100 &lt;&lt; 20) high_threshhold &lt;= current/max时: 此时 (90 &lt;&lt; 20) &lt; current delay = c (2/(200 &lt;&lt; 20) + (current/max - 0.9) s1) a）current = 90 &lt;&lt; 20时：delay = c / (100 &lt;&lt; 20) b）current = 100 &lt;&lt; 20时：delay = 5 * c / (100 &lt;&lt; 20) 当前配置下的dynamic throttle配置如下： 123456filestore_expected_throughput_bytes = 536870912 // 512Mfilestore_queue_max_bytes= 1048576000 // 1000Mfilestore_queue_low_threshhold = 0.6filestore_queue_high_threshhold = 0.9 // 默认值filestore_queue_high_delay_multiple = 2filestore_queue_max_delay_multiple = 10 BackoffThrottle中的值如下： 1234567low_threshhold = 0.6high_threshhold = 0.9high_delay_per_count = 2/(512 &lt;&lt; 20)max_delay_per_count = 10/(512 &lt;&lt; 20)s0 = (2/(512 &lt;&lt; 20))/0.3s1 = (8/(512 &lt;&lt; 20))/0.1max = 1000 &lt;&lt; 20 则此时的delay分为如下几种： current/max &lt; low_threshhold时：此时 current &lt; (600 &lt;&lt; 20)；delay = 0 low_threshhold &lt;= current/max &lt; high_threshhold时： 此时 (600 &lt;&lt; 20) &lt;= current &lt; (900 &lt;&lt; 20) delay = c ((current/max - 0.6) s0) a）current = 600 &lt;&lt; 20时：delay = 0 b）current = 900 &lt;&lt; 20时：delay = c / (256 &lt;&lt; 20) high_threshhold &lt;= current/max时: 此时 (900 &lt;&lt; 20) &lt; current delay = c (2/(512 &lt;&lt; 20) + (current/max - 0.9) s1) a）current = 900 &lt;&lt; 20时：delay = c / (256 &lt;&lt; 20) b）current = 1000 &lt;&lt; 20时：delay = 5 * c / (256 &lt;&lt; 20) 结论：这里的参数配置不是很合理；600M之前的delay都是0；后续随着current的增大，delay的值小于默认时候的值，可能会加大filestore的压力；]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>osd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ceph PG Lock and RWState]]></title>
    <url>%2F2017%2F05%2F24%2FCeph-PGLock-RWState%2F</url>
    <content type="text"><![CDATA[概述在RWG的压力测试中，经常看到op的如下信息：”event”: “waiting for rw locks” 并且在RWG的压力很大时，这个waiting event会出现很多次，导致op的latency很高，所以需要分析下为什么会有这个waiting for rw locks的event？它与PG Lock的关系？ PG Lock从 Ceph OSD op_shardedwq 中分析知道： OSD的osd_op_tp在处理OPRequest的时候就会先获取PG Lock； OSD的osd_op_tp在调用OSD::dequeue_op()返回后会释放PG Lock； OSD::dequeue_op的调用过程如下： 12345678OSD::dequeue_op() |-- ReplicatedPG::do_request() |-- ReplicatedPG::do_op() //在op类型为CEPH_MSG_OSD_OP时 |-- ReplicatedPG::execute_ctx() // 之前会会先获取RW Lock，失败后把op重新加入work queue |-- ReplicatedPG::issue_repop() // 写case，读的话在ReplicatedPG::prepare_transaction()里处理 |-- ReplicatedBackend::submit_transaction() |-- ReplicatedBackend::issue_op() // 发送rep ops FileStore::queue_transactions() // 有journal时把transaction放到journal的work queue里 从上面的调用可以看出，OSD的osd_op_tp里的线程处理过程在获取PG Lock后，只会把op组装为transaction后交给FileStore的work queue，然后就返回释放PG Lock了；返回后写的数据可能还没有写到journal和disk，即还没有commit/apply成功；所以针对同一个object的操作，虽说对它的操作有PG Lock，但也可能在PG Lock释放后，对object的实际操作RW还没有完成； 这就引入了object的读写锁，即struct ObjectContext里的struct RWState，通过它来互斥对同一object的读写，但允许对同一object的同时多读、同时多写（详细见下面RWState的分析）； 对于同时多写，因为有PG Lock，所以多写并不会引起data consistency问题（PG Lock保证多写是顺序的），多个写提交到FileStore后可以并发或合并（待分析）； RWStateRWState的定义 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112struct ObjectContext &#123;... struct RWState &#123; enum State &#123; RWNONE, // 初始状态，count = 0时设置 RWREAD, // read lock RWWRITE, // write lock RWEXCL, // exclusive lock &#125;;... list&lt;OpRequestRef&gt; waiters; ///&lt; ops waiting on state change int count; ///&lt; number of readers or writers State state:4; ///&lt; rw state /// if set, restart backfill when we can get a read lock bool recovery_read_marker:1; /// if set, requeue snaptrim on lock release bool snaptrimmer_write_marker:1;... bool get_read(OpRequestRef op) &#123; if (get_read_lock()) &#123; return true; &#125; // else waiters.push_back(op); return false; &#125; /// this function adjusts the counts if necessary bool get_read_lock() &#123; // 获取read lock // don't starve anybody! if (!waiters.empty()) &#123; return false; &#125; switch (state) &#123; case RWNONE: assert(count == 0); state = RWREAD; // fall through case RWREAD: // 支持同时多读 count++; return true; case RWWRITE: return false; case RWEXCL: return false; default: assert(0 == "unhandled case"); return false; &#125; &#125; bool get_write(OpRequestRef op, bool greedy=false) &#123; if (get_write_lock(greedy)) &#123; return true; &#125; // else if (op) waiters.push_back(op); return false; &#125; bool get_write_lock(bool greedy=false) &#123; // 获取write lock if (!greedy) &#123; // don't starve anybody! if (!waiters.empty() || recovery_read_marker) &#123; return false; &#125; &#125; switch (state) &#123; case RWNONE: assert(count == 0); state = RWWRITE; // fall through case RWWRITE: // 支持同时多写 count++; return true; case RWREAD: return false; case RWEXCL: return false; default: assert(0 == "unhandled case"); return false; &#125; &#125; bool get_excl_lock() &#123; // 获取exclusive lock switch (state) &#123; case RWNONE: assert(count == 0); state = RWEXCL; count = 1; return true; case RWWRITE: return false; case RWREAD: return false; case RWEXCL: return false; default: assert(0 == "unhandled case"); return false; &#125; &#125; bool get_excl(OpRequestRef op) &#123; if (get_excl_lock()) &#123; return true; &#125; // else if (op) waiters.push_back(op); return false; &#125;... &#125; rwstate;...&#125;; 从上面的定义中看出，RW lock支持同时多读、同时多写；在do_op中，会根据op类型，尝试获取rw locks： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/** do_op - do an op * pg lock will be held (if multithreaded) * osd_lock NOT held. */void ReplicatedPG::do_op(OpRequestRef&amp; op)&#123;... &#125; else if (!get_rw_locks(write_ordered, ctx)) &#123; // 获取rw lock失败 dout(20) &lt;&lt; __func__ &lt;&lt; " waiting for rw locks " &lt;&lt; dendl; op-&gt;mark_delayed("waiting for rw locks"); close_op_ctx(ctx); // close op ctx，会把当前op重新加入queue中 return; &#125;...&#125; -- get_rw_locks [ReplicatedPG] /** * Grabs locks for OpContext, should be cleaned up in close_op_ctx * * @param ctx [in,out] ctx to get locks for * @return true on success, false if we are queued */ bool get_rw_locks(bool write_ordered, OpContext *ctx) &#123; /* If snapset_obc, !obc-&gt;obs-&gt;exists and we will always take the * snapdir lock *before* the head lock. Since all callers will do * this (read or write) if we get the first we will be guaranteed * to get the second. */ if (write_ordered &amp;&amp; ctx-&gt;op-&gt;may_read()) &#123; // 根据op的flag设置lock type ctx-&gt;lock_type = ObjectContext::RWState::RWEXCL; &#125; else if (write_ordered) &#123; ctx-&gt;lock_type = ObjectContext::RWState::RWWRITE; &#125; else &#123; assert(ctx-&gt;op-&gt;may_read()); ctx-&gt;lock_type = ObjectContext::RWState::RWREAD; &#125; if (ctx-&gt;snapset_obc) &#123; assert(!ctx-&gt;obc-&gt;obs.exists); if (!ctx-&gt;lock_manager.get_lock_type( ctx-&gt;lock_type, ctx-&gt;snapset_obc-&gt;obs.oi.soid, ctx-&gt;snapset_obc, ctx-&gt;op)) &#123; ctx-&gt;lock_type = ObjectContext::RWState::RWNONE; return false; &#125; &#125; if (ctx-&gt;lock_manager.get_lock_type( // 尝试获取lock ctx-&gt;lock_type, ctx-&gt;obc-&gt;obs.oi.soid, ctx-&gt;obc, ctx-&gt;op)) &#123; return true; &#125; else &#123; assert(!ctx-&gt;snapset_obc); ctx-&gt;lock_type = ObjectContext::RWState::RWNONE; return false; &#125; &#125; close_op_ctx的调用关系如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596osd/ReplicatedPG.h-- close_op_ctx [ReplicatedPG] /** * Cleans up OpContext * * @param ctx [in] ctx to clean up */ void close_op_ctx(OpContext *ctx) &#123; release_object_locks(ctx-&gt;lock_manager); // 调用 ctx-&gt;op_t.reset(); for (auto p = ctx-&gt;on_finish.begin(); p != ctx-&gt;on_finish.end(); ctx-&gt;on_finish.erase(p++)) &#123; (*p)(); &#125; delete ctx; &#125; -- release_object_locks [ReplicatedPG] /** * Releases locks * * @param manager [in] manager with locks to release */ void release_object_locks( ObcLockManager &amp;lock_manager) &#123; list&lt;pair&lt;hobject_t, list&lt;OpRequestRef&gt; &gt; &gt; to_req; bool requeue_recovery = false; bool requeue_snaptrim = false; lock_manager.put_locks( // 调用获取to_req &amp;to_req, &amp;requeue_recovery, &amp;requeue_snaptrim); if (requeue_recovery) osd-&gt;recovery_wq.queue(this); if (requeue_snaptrim) queue_snap_trim(); if (!to_req.empty()) &#123; // to_req不为空 // requeue at front of scrub blocking queue if we are blocked by scrub for (auto &amp;&amp;p: to_req) &#123; if (scrubber.write_blocked_by_scrub( p.first.get_head(), get_sort_bitwise())) &#123; waiting_for_active.splice( waiting_for_active.begin(), p.second, p.second.begin(), p.second.end()); &#125; else &#123; requeue_ops(p.second); // 把request重新加入队列 &#125; &#125; &#125; &#125; osd/osd_types.h-- put_locks [ObcLockManager] void put_locks( list&lt;pair&lt;hobject_t, list&lt;OpRequestRef&gt; &gt; &gt; *to_requeue, bool *requeue_recovery, bool *requeue_snaptrimmer) &#123; for (auto p: locks) &#123; list&lt;OpRequestRef&gt; _to_requeue; p.second.obc-&gt;put_lock_type( p.second.type, &amp;_to_requeue, requeue_recovery, requeue_snaptrimmer); if (to_requeue) &#123; to_requeue-&gt;push_back( make_pair( p.second.obc-&gt;obs.oi.soid, std::move(_to_requeue))); &#125; &#125; locks.clear(); &#125; -- put_lock_type [ObjectContext] void put_lock_type( ObjectContext::RWState::State type, list&lt;OpRequestRef&gt; *to_wake, bool *requeue_recovery, bool *requeue_snaptrimmer) &#123; switch (type) &#123; case ObjectContext::RWState::RWWRITE: return put_write(to_wake, requeue_recovery, requeue_snaptrimmer); case ObjectContext::RWState::RWREAD: return put_read(to_wake); case ObjectContext::RWState::RWEXCL: return put_excl(to_wake, requeue_recovery, requeue_snaptrimmer); default: assert(0 == "invalid lock type"); &#125; &#125; 系统调用close_op_ctx()函数的地方有： 123456789101112131415161718192021222324252627281 166 osd/ReplicatedPG.cc &lt;&lt;finish&gt;&gt; ctx-&gt;pg-&gt;close_op_ctx(ctx); 2 2099 osd/ReplicatedPG.cc &lt;&lt;do_op&gt;&gt; close_op_ctx(ctx); 3 2105 osd/ReplicatedPG.cc &lt;&lt;do_op&gt;&gt; close_op_ctx(ctx); 4 2134 osd/ReplicatedPG.cc &lt;&lt;do_op&gt;&gt; close_op_ctx(ctx); 5 2970 osd/ReplicatedPG.cc &lt;&lt;execute_ctx&gt;&gt; close_op_ctx(ctx); 6 3112 osd/ReplicatedPG.cc &lt;&lt;reply_ctx&gt;&gt; close_op_ctx(ctx); 7 3119 osd/ReplicatedPG.cc &lt;&lt;reply_ctx&gt;&gt; close_op_ctx(ctx); 8 3422 osd/ReplicatedPG.cc &lt;&lt;trim_object&gt;&gt; close_op_ctx(ctx.release()); 9 3430 osd/ReplicatedPG.cc &lt;&lt;trim_object&gt;&gt; close_op_ctx(ctx.release());10 6888 osd/ReplicatedPG.cc &lt;&lt;complete_read_ctx&gt;&gt; close_op_ctx(ctx);11 8174 osd/ReplicatedPG.cc &lt;&lt;try_flush_mark_clean&gt;&gt; close_op_ctx(ctx.release());12 8180 osd/ReplicatedPG.cc &lt;&lt;try_flush_mark_clean&gt;&gt; close_op_ctx(ctx.release());13 10153 osd/ReplicatedPG.cc &lt;&lt;on_change&gt;&gt; close_op_ctx(i-&gt;second);14 12183 osd/ReplicatedPG.cc &lt;&lt;agent_maybe_evict&gt;&gt; close_op_ctx(ctx.release()); 所以分析得出在op无论是出错、锁竞争、操作完成的情况下，都会调用close_op_ctx()，它会把等待在object的RWState锁上的op重新入队处理；]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>osd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ceph osd_enable_op_tracker配置分析]]></title>
    <url>%2F2017%2F05%2F23%2FCeph-osd_enable_op_tracker%2F</url>
    <content type="text"><![CDATA[概述本文分析Ceph OpTracker相关实现，提供osd_enable_op_tracker的配置分析，并做相关测试对比性能； 代码分析op_tracker定义osd_enable_op_tracker对应的配置是osd中的op_tracker，每个osd会初始化一个op_tracker，代码如下： 123456789101112131415161718192021## op_tracker的定义class OSD : public Dispatcher, public md_config_obs_t &#123;... // -- op tracking -- OpTracker op_tracker;...&#125;; ## op_tracker的初始化OSD::OSD(...) :... op_tracker(cct, cct-&gt;_conf-&gt;osd_enable_op_tracker, // 是否enable op tracker cct-&gt;_conf-&gt;osd_num_op_tracker_shard), // op tracker shard数，每个shard一个mutex锁，多个shard可以避免op更新的性能瓶颈问题...&#123; monc-&gt;set_messenger(client_messenger); op_tracker.set_complaint_and_threshold(cct-&gt;_conf-&gt;osd_op_complaint_time, // op complaint time配置，超出后有warning log cct-&gt;_conf-&gt;osd_op_log_threshold); // op log的阈值 op_tracker.set_history_size_and_duration(cct-&gt;_conf-&gt;osd_op_history_size, // op history的最大size cct-&gt;_conf-&gt;osd_op_history_duration); // op history的最长保留时间&#125; OpTracker的定义如下，里面维护了一个sharded list，会把op映射到对应的list上，每个shard有一个mutex锁，保证这里的track操作不会成为性能瓶颈； 每个list的单位是TrackedOp 12345678910111213class OpTracker &#123;... struct ShardedTrackingData &#123; Mutex ops_in_flight_lock_sharded; xlist&lt;TrackedOp *&gt; ops_in_flight_sharded; explicit ShardedTrackingData(string lock_name): ops_in_flight_lock_sharded(lock_name.c_str()) &#123;&#125; &#125;; vector&lt;ShardedTrackingData*&gt; sharded_in_flight_list; uint32_t num_optracker_shards; OpHistory history;...&#125; TrackedOp定义TrackedOp是一个tracked的operation，它会指向所属的OpTracker，定义如下： 123456789class TrackedOp &#123;private: friend class OpHistory; friend class OpTracker; xlist&lt;TrackedOp*&gt;::item xitem;protected: OpTracker *tracker; /// the tracker we are associated with...&#125; ceph osd中的每个op都是TrackedOp的子类； 1234struct OpRequest : public TrackedOp &#123; friend class OpTracker;...&#125; OpRequest会在osd dispatch的时候初始化，会根据是否enable op tracker初始化对应值： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849osd/OSD.cc &lt;&lt;ms_fast_dispatch&gt;&gt; OpRequestRef op = op_tracker.create_request&lt;OpRequest, Message*&gt;(m); osd/OSD.cc &lt;&lt;_dispatch&gt;&gt; OpRequestRef op = op_tracker.create_request&lt;OpRequest, Message*&gt;(m); class OpTracker &#123;... template &lt;typename T, typename U&gt; typename T::Ref create_request(U params) &#123; typename T::Ref retval(new T(params, this), // 初始化OpRequest类 RemoveOnDelete(this)); retval-&gt;tracking_start(); // OpRequest放到OpTracker的shard list里，设置 is_tracked = true；后于OpReuest类的初始化 return retval; &#125;&#125;; class TrackedOp &#123;... void tracking_start() &#123; if (tracker-&gt;register_inflight_op(&amp;xitem)) &#123; events.push_back(make_pair(initiated_at, "initiated")); is_tracked = true; &#125; &#125;&#125; OpRequest::OpRequest(Message *req, OpTracker *tracker) : TrackedOp(tracker, req-&gt;get_recv_stamp()), // TrackedOp的is_tracked默认为false rmw_flags(0), request(req), hit_flag_points(0), latest_flag_point(0), send_map_update(false), sent_epoch(0), hitset_inserted(false) &#123; if (req-&gt;get_priority() &lt; tracker-&gt;cct-&gt;_conf-&gt;osd_client_op_priority) &#123; // don't warn as quickly for low priority ops warn_interval_multiplier = tracker-&gt;cct-&gt;_conf-&gt;osd_recovery_op_warn_multiple; &#125; if (req-&gt;get_type() == CEPH_MSG_OSD_OP) &#123; reqid = static_cast&lt;MOSDOp*&gt;(req)-&gt;get_reqid(); &#125; else if (req-&gt;get_type() == MSG_OSD_SUBOP) &#123; reqid = static_cast&lt;MOSDSubOp*&gt;(req)-&gt;reqid; &#125; else if (req-&gt;get_type() == MSG_OSD_REPOP) &#123; reqid = static_cast&lt;MOSDRepOp*&gt;(req)-&gt;reqid; &#125; tracker-&gt;mark_event(this, "header_read", request-&gt;get_recv_stamp()); //??? 因为此时is_tracked为false，所以这些event log不会打印出来 tracker-&gt;mark_event(this, "throttled", request-&gt;get_throttle_stamp()); tracker-&gt;mark_event(this, "all_read", request-&gt;get_recv_complete_stamp()); tracker-&gt;mark_event(this, "dispatched", request-&gt;get_dispatch_stamp());&#125; OpTracker记录op在创建一个op request的时候，就会把op分到OpTracker的不同shard list上 1234567891011121314151617bool OpTracker::register_inflight_op(xlist&lt;TrackedOp*&gt;::item *i)&#123; RWLock::RLocker l(lock); if (!tracking_enabled) return false; uint64_t current_seq = seq.inc(); uint32_t shard_index = current_seq % num_optracker_shards; ShardedTrackingData* sdata = sharded_in_flight_list[shard_index]; assert(NULL != sdata); &#123; Mutex::Locker locker(sdata-&gt;ops_in_flight_lock_sharded); sdata-&gt;ops_in_flight_sharded.push_back(i); sdata-&gt;ops_in_flight_sharded.back()-&gt;seq = current_seq; &#125; return true;&#125; 之后可以通过ceph daemon &lt;osd.id&gt; dump_ops_in_flight/dump_historic_ops查看该osd上的op情况； TrackedOp mark event当开启了op tracker，TrackedOp就会根据配置的debug等级把一些event打印到osd的log文件里，这个操作会影响系统性能；但也可以帮我们分析出一个op哪个阶段比较费时，找出系统瓶颈； 1234567891011121314151617181920212223242526272829void TrackedOp::mark_event(const string &amp;event)&#123; if (!is_tracked) return; utime_t now = ceph_clock_now(g_ceph_context); &#123; Mutex::Locker l(lock); events.push_back(make_pair(now, event)); // events记录一个op所有的event信息 &#125; tracker-&gt;mark_event(this, event); // mark event，打印log _event_marked(); // 可以自己实现event mark后的操作，现在为空&#125; void OpTracker::mark_event(TrackedOp *op, const string &amp;dest, utime_t time)&#123; if (!op-&gt;is_tracked) return; return _mark_event(op, dest, time);&#125;void OpTracker::_mark_event(TrackedOp *op, const string &amp;evt, utime_t time)&#123; dout(5); *_dout &lt;&lt; "seq: " &lt;&lt; op-&gt;seq &lt;&lt; ", time: " &lt;&lt; time &lt;&lt; ", event: " &lt;&lt; evt &lt;&lt; ", op: "; op-&gt;_dump_op_descriptor_unlocked(*_dout); *_dout &lt;&lt; dendl;&#125; 性能测试基于之前搭建的RGW cosbench，对比测试了osd_enable_op_tracker为false/true的情况下的性能，结果如下： 测试参数：两个cosbench driver，写一个bucket 5,000,000个objects，workers=100，runtime=2000； debug_optracker = 0/0 &amp; osd_enable_op_tracker = false先写两个bucket的结果如下： Op-Type OOp-Count OByte-Count OAvg-ResTime OAvg-ProcTime OThroughput OBandwidth OSucc-Ratio 1 write 2.57 mops 82.15 GB 77.79 ms 77.64 ms 1284.26 op/s 41.1 MB/S 100% 2 write 1.32 mops 42.28 GB 151.05 ms 150.8 ms 661.69 op/s 21.17 MB/S 100% 【第二次写入的性能较差，感觉应该是测试误差导致的，或者是当时有别的io影响，没再重复测试】 删除之前写的bucket的结果如下： Op-Type OOp-Count OByte-Count OAvg-ResTime OAvg-ProcTime OThroughput OBandwidth OSucc-Ratio 1 cleanup-delete 5 mops 0 B 8.03 ms 8.03 ms 12468.13 op/s 0 B/S 100% 2 cleanup-delete 5 mops 0 B 4.79 ms 4.79 ms 20898.72 op/s 0 B/S 100% debug_optracker = 0/0 &amp; osd_enable_op_tracker = true先写两个bucket的结果如下： Op-Type OOp-Count OByte-Count OAvg-ResTime OAvg-ProcTime OThroughput OBandwidth OSucc-Ratio 1 write 2.67 mops 85.47 GB 74.79 ms 74.59 ms 1335.65 op/s 42.74 MB/S 100% 2 write 2.47 mops 79.02 GB 80.91 ms 80.76 ms 1234.71 op/s 39.51 MB/S 100% 删除之前写的bucket的结果如下： Op-Type OOp-Count OByte-Count OAvg-ResTime OAvg-ProcTime OThroughput OBandwidth OSucc-Ratio 1 cleanup-delete 5 mops 0 B 10.3 ms 10.3 ms 9709.31 op/s 0 B/S 100% 2 cleanup-delete 5 mops 0 B 8.63 ms 8.63 ms 11581.92 op/s 0 B/S 100% 【删除的相应时间比之前还是慢的比较多，不清晰是测试误差，还是别的原因】 结论从上述两项的对比可以看出，false/true的测试结果在误差范围内，印证了osd_enable_op_tracker设置为true对Ceph系统性能没什么影响； 打开debug_optracker为了追踪op在osd中各个阶段的时间开销，我们可以打开debug_optracker，然后在osd的log中查到对应的event log； 打开关闭debug_optracker的方法： 12ceph daemon &lt;osd-id&gt; config set debug_optracker 5\/5 // 打开debug logceph daemon &lt;osd-id&gt; config set debug_optracker 0\/0 // 关闭debug log 之后可以在对应osd的log文件中看到如下log，以其中一个write op为例： 123456789101112132017-05-09 11:15:58.925362 7f556af0f700 5 -- op tracker -- seq: 682036, time: 2017-05-09 11:15:58.925362, event: queued_for_pg, op: osd_op(client.1027809.0:366525835 31.36c4e4e0 (undecoded) ondisk+write+known_if_redirected e2611)2017-05-09 11:15:58.925388 7f55b27ff700 5 -- op tracker -- seq: 682036, time: 2017-05-09 11:15:58.925388, event: reached_pg, op: osd_op(client.1027809.0:366525835 31.36c4e4e0 (undecoded) ondisk+write+known_if_redirected e2611)2017-05-09 11:15:58.925509 7f55b27ff700 5 -- op tracker -- seq: 682036, time: 2017-05-09 11:15:58.925509, event: started, op: osd_op(client.1027809.0:366525835 31.36c4e4e0 f31f020f-aaa2-4570-b6f7-19258064e5b5.1027809.13_myobjects449924 [create 0~0 [excl],setxattr user.rgw.idtag (55),writefull 0~32000,setxattr user.rgw.manifest (600),setxattr user.rgw.acl (145),setxattr user.rgw.content_type (25),setxattr user.rgw.etag (33),call rgw.obj_store_pg_ver,setxattr user.rgw.source_zone (4)] snapc 0=[] ondisk+write+known_if_redirected e2611)2017-05-09 11:15:58.925595 7f55b27ff700 5 -- op tracker -- seq: 682036, time: 2017-05-09 11:15:58.925595, event: waiting for subops from 29,76, op: osd_op(client.1027809.0:366525835 31.36c4e4e0 f31f020f-aaa2-4570-b6f7-19258064e5b5.1027809.13_myobjects449924 [create 0~0 [excl],setxattr user.rgw.idtag (55),writefull 0~32000,setxattr user.rgw.manifest (600),setxattr user.rgw.acl (145),setxattr user.rgw.content_type (25),setxattr user.rgw.etag (33),call rgw.obj_store_pg_ver,setxattr user.rgw.source_zone (4)] snapc 0=[] ondisk+write+known_if_redirected e2611)2017-05-09 11:15:58.925735 7f55b27ff700 5 -- op tracker -- seq: 682036, time: 2017-05-09 11:15:58.925735, event: commit_queued_for_journal_write, op: osd_op(client.1027809.0:366525835 31.36c4e4e0 f31f020f-aaa2-4570-b6f7-19258064e5b5.1027809.13_myobjects449924 [create 0~0 [excl],setxattr user.rgw.idtag (55),writefull 0~32000,setxattr user.rgw.manifest (600),setxattr user.rgw.acl (145),setxattr user.rgw.content_type (25),setxattr user.rgw.etag (33),call rgw.obj_store_pg_ver,setxattr user.rgw.source_zone (4)] snapc 0=[] ondisk+write+known_if_redirected e2611)2017-05-09 11:15:58.925773 7f561cfec700 5 -- op tracker -- seq: 682036, time: 2017-05-09 11:15:58.925772, event: write_thread_in_journal_buffer, op: osd_op(client.1027809.0:366525835 31.36c4e4e0 f31f020f-aaa2-4570-b6f7-19258064e5b5.1027809.13_myobjects449924 [create 0~0 [excl],setxattr user.rgw.idtag (55),writefull 0~32000,setxattr user.rgw.manifest (600),setxattr user.rgw.acl (145),setxattr user.rgw.content_type (25),setxattr user.rgw.etag (33),call rgw.obj_store_pg_ver,setxattr user.rgw.source_zone (4)] snapc 0=[] ondisk+write+known_if_redirected e2611)2017-05-09 11:15:58.925947 7f561c7eb700 5 -- op tracker -- seq: 682036, time: 2017-05-09 11:15:58.925946, event: journaled_completion_queued, op: osd_op(client.1027809.0:366525835 31.36c4e4e0 f31f020f-aaa2-4570-b6f7-19258064e5b5.1027809.13_myobjects449924 [create 0~0 [excl],setxattr user.rgw.idtag (55),writefull 0~32000,setxattr user.rgw.manifest (600),setxattr user.rgw.acl (145),setxattr user.rgw.content_type (25),setxattr user.rgw.etag (33),call rgw.obj_store_pg_ver,setxattr user.rgw.source_zone (4)] snapc 0=[] ondisk+write+known_if_redirected e2611)2017-05-09 11:15:58.925962 7f5614fff700 5 -- op tracker -- seq: 682036, time: 2017-05-09 11:15:58.925961, event: op_commit, op: osd_op(client.1027809.0:366525835 31.36c4e4e0 f31f020f-aaa2-4570-b6f7-19258064e5b5.1027809.13_myobjects449924 [create 0~0 [excl],setxattr user.rgw.idtag (55),writefull 0~32000,setxattr user.rgw.manifest (600),setxattr user.rgw.acl (145),setxattr user.rgw.content_type (25),setxattr user.rgw.etag (33),call rgw.obj_store_pg_ver,setxattr user.rgw.source_zone (4)] snapc 0=[] ondisk+write+known_if_redirected e2611)2017-05-09 11:15:58.926400 7f55d3be3700 5 -- op tracker -- seq: 682036, time: 2017-05-09 11:15:58.926400, event: sub_op_commit_rec from 76, op: osd_op(client.1027809.0:366525835 31.36c4e4e0 f31f020f-aaa2-4570-b6f7-19258064e5b5.1027809.13_myobjects449924 [create 0~0 [excl],setxattr user.rgw.idtag (55),writefull 0~32000,setxattr user.rgw.manifest (600),setxattr user.rgw.acl (145),setxattr user.rgw.content_type (25),setxattr user.rgw.etag (33),call rgw.obj_store_pg_ver,setxattr user.rgw.source_zone (4)] snapc 0=[] ondisk+write+known_if_redirected e2611)2017-05-09 11:15:58.926440 7f55b27ff700 5 -- op tracker -- seq: 682036, time: 2017-05-09 11:15:58.926439, event: sub_op_commit_rec from 29, op: osd_op(client.1027809.0:366525835 31.36c4e4e0 f31f020f-aaa2-4570-b6f7-19258064e5b5.1027809.13_myobjects449924 [create 0~0 [excl],setxattr user.rgw.idtag (55),writefull 0~32000,setxattr user.rgw.manifest (600),setxattr user.rgw.acl (145),setxattr user.rgw.content_type (25),setxattr user.rgw.etag (33),call rgw.obj_store_pg_ver,setxattr user.rgw.source_zone (4)] snapc 0=[] ondisk+write+known_if_redirected e2611)2017-05-09 11:15:58.926456 7f55b27ff700 5 -- op tracker -- seq: 682036, time: 2017-05-09 11:15:58.926456, event: commit_sent, op: osd_op(client.1027809.0:366525835 31.36c4e4e0 f31f020f-aaa2-4570-b6f7-19258064e5b5.1027809.13_myobjects449924 [create 0~0 [excl],setxattr user.rgw.idtag (55),writefull 0~32000,setxattr user.rgw.manifest (600),setxattr user.rgw.acl (145),setxattr user.rgw.content_type (25),setxattr user.rgw.etag (33),call rgw.obj_store_pg_ver,setxattr user.rgw.source_zone (4)] snapc 0=[] ondisk+write+known_if_redirected e2611)2017-05-09 11:15:58.926793 7f5613ffd700 5 -- op tracker -- seq: 682036, time: 2017-05-09 11:15:58.926792, event: op_applied, op: osd_op(client.1027809.0:366525835 31.36c4e4e0 f31f020f-aaa2-4570-b6f7-19258064e5b5.1027809.13_myobjects449924 [create 0~0 [excl],setxattr user.rgw.idtag (55),writefull 0~32000,setxattr user.rgw.manifest (600),setxattr user.rgw.acl (145),setxattr user.rgw.content_type (25),setxattr user.rgw.etag (33),call rgw.obj_store_pg_ver,setxattr user.rgw.source_zone (4)] snapc 0=[] ondisk+write+known_if_redirected e2611)2017-05-09 11:15:58.926825 7f5613ffd700 5 -- op tracker -- seq: 682036, time: 2017-05-09 11:15:58.926824, event: done, op: osd_op(client.1027809.0:366525835 31.36c4e4e0 f31f020f-aaa2-4570-b6f7-19258064e5b5.1027809.13_myobjects449924 [create 0~0 [excl],setxattr user.rgw.idtag (55),writefull 0~32000,setxattr user.rgw.manifest (600),setxattr user.rgw.acl (145),setxattr user.rgw.content_type (25),setxattr user.rgw.etag (33),call rgw.obj_store_pg_ver,setxattr user.rgw.source_zone (4)] snapc 0=[] ondisk+write+known_if_redirected e2611) 打开debug_optracker会有大量的osd log写入，严重影响osd的性能，所以这里只是建议在调试osd性能的时候偶尔打开收集下信息，然后分析osd的性能瓶颈； dump_historic_ops在不打开debug_optracker的情况下，也可以获取部分OpRequest的event信息，这就是通过dump_historic_ops获取；然后就可以分析op的各个时间段开销了； 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293# ceph daemon osd.11 dump_historic_ops&#123; "num to keep": 20, "duration to keep": 600, "Ops": [ &#123; "description": "osd_op(client.1027809.0:458241652 17.e6e072e1 .dir.f31f020f-aaa2-4570-b6f7-19258064e5b5.859142.3.77 [] snapc 0=[] ack+ondisk+write+known_if_redirected e2850)", "initiated_at": "2017-05-09 18:54:50.987435", "age": 412.664363, "duration": 0.338724, "type_data": [ "commit sent; apply or cleanup", &#123; "client": "client.1027809", "tid": 458241652 &#125;, [ &#123; "time": "2017-05-09 18:54:50.987435", "event": "initiated" &#125;, &#123; "time": "2017-05-09 18:54:50.987453", "event": "queued_for_pg" &#125;, &#123; "time": "2017-05-09 18:54:50.987483", "event": "reached_pg" &#125;, &#123; "time": "2017-05-09 18:54:50.987520", "event": "waiting for rw locks" &#125;, &#123; "time": "2017-05-09 18:54:51.052956", "event": "reached_pg" &#125;, &#123; "time": "2017-05-09 18:54:51.052986", "event": "waiting for rw locks" &#125;, &#123; "time": "2017-05-09 18:54:51.197523", "event": "reached_pg" &#125;, &#123; "time": "2017-05-09 18:54:51.197562", "event": "started" &#125;, &#123; "time": "2017-05-09 18:54:51.197812", "event": "waiting for subops from 33,43" &#125;, &#123; "time": "2017-05-09 18:54:51.198068", "event": "commit_queued_for_journal_write" &#125;, &#123; "time": "2017-05-09 18:54:51.198094", "event": "write_thread_in_journal_buffer" &#125;, &#123; "time": "2017-05-09 18:54:51.198195", "event": "journaled_completion_queued" &#125;, &#123; "time": "2017-05-09 18:54:51.198330", "event": "op_commit" &#125;, &#123; "time": "2017-05-09 18:54:51.198454", "event": "sub_op_commit_rec from 43" &#125;, &#123; "time": "2017-05-09 18:54:51.198851", "event": "sub_op_commit_rec from 33" &#125;, &#123; "time": "2017-05-09 18:54:51.198865", "event": "commit_sent" &#125;, &#123; "time": "2017-05-09 18:54:51.326034", "event": "op_applied" &#125;, &#123; "time": "2017-05-09 18:54:51.326158", "event": "done" &#125; ] ] &#125;,...]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>osd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Hexo + pages github搭建个人blog站点]]></title>
    <url>%2F2017%2F05%2F21%2Fhexo-pagesgithub-build-website%2F</url>
    <content type="text"><![CDATA[之前都是把工作学习中写的文章记录在Evernote上，方便自己搜索，然后在CSDN上开了个blog，会发一些整理过的文章；后来感觉有必要自己搭建个技术blog，记录自己的总结，方便自己也有利于他人，就开始搜索如何方便快捷搭建一个blog；首先搜索到的是Jekyll + pages github，学习了半天后发现，想找个喜欢的模板不容易，搭建也不是分分钟的事情，后来就搜索到了Hexo + pages github的方案，应用后发现很便捷，这里记录下来。 创建github库github相信大多数朋友都用过，这里介绍的是github提供的pages功能，可以很方便的搭建个人的静态网站，特别适合简介的blog系统；并且默认提供的二级域名就可以访问； 在自己的github账号下创建如下命名的repository：&lt;your-github-name&gt;.github.io，一切选择默认即可；其中&lt;your-github-name&gt;即是你登录github账号的名字；注意：别的名称的repository重命名为上述名称的repository不行 下载repository123$ mkdir YourGithubName.github.io$ cd YourGithubName.github.io$ git clone &lt;git@github.com:YourGithubName/YourGithubName.github.io.git&gt; 注：上述YourGithubName替换为你自己的github库名字 为了能有权限上传代码到repository，你需要生成自己的ssh-key，并填写到github的配置里； 首先本地生成ssh key 12$ ssh-keygen // 一路回车，选择默认值即可$ cat ~/.ssh/id_rsa.pub 更新ssh key到github 步骤为： 右上角头像 -&gt; Settings -&gt; SSH and GPG keys -&gt; New SSH key 然后填写Title，Key即可； Title 自己随便命名，Key 为上步骤中cat ~/.ssh/id_rsa.pub的值 Hexo初始化repository目录上述我们clone下路的repository还是空目录一个，为了搭建网站，搜索我们需要的就是通过hexo命令来初始化该目录 首先安装hexo，请参考：https://hexo.io/zh-cn/docs/#安装 123$ npm install -g hexo-cli$ npm install hexo --save$ npm install hexo-deployer-git --save // hexo deploy到git时需要 初始化repository目录 12345$ cd YourGithubName.github.io$ hexo init .$ npm install$ ls_config.yml node_modules package.json scaffolds source themes 这里我们主要关注的是： _config.yml 文件：整个hexo的配置文件 source 目录：所有网站相关页面的目录，后续写的markdown文件都应该放在source/_post/目录下 themes 目录：hexo模板的目录，里面每个子目录都是一个hexo的模板 网站测试初始化repository目录后，即可在本地测试下网站的功能步骤如下： 启动hexo server 123$ hexo serverINFO Start processingINFO Hexo is running at http://localhost:4000/. Press Ctrl+C to stop. 本地浏览器查看 本地浏览器里输入: http://localhost:4000/ 访问 Hexo模板Hexo包含了很多美观的模板，我们可以随便选择自己喜欢的Hexo模板来搭建网站Hexo模板：https://hexo.io/themes/我选择的是Next模板：https://github.com/idhyt/hexo-theme-next还有一款简洁的模板：https://github.com/tufu9441/maupassant-hexo.git 选择好一个模板后，就可以把它下载到我们的repository里 123$ git clone https://github.com/idhyt/hexo-theme-next.git themes/next$ ls themes/nextREADME.en.md README.md _config.yml bower.json languages layout scripts source test theme里有自己的_config.yml文件，里面是主题相关的配置参数，详情请参考具体模板的README.md 配置repository目录下的_config.yml文件，指定使用模板： 12345678910$ vim _config.xml...# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: landscape...然后修改上面的theme为next即可theme : next 启动hexo server，本地测试下外观 ;) 更新blog网站和选用的模式都确定后，下一步就是更新我们自己的blog了，通过hexo也是异常简单 12$ hexo new mypageINFO Created: ~/YourGithubName.github.io/source/_posts/mypage.md 默认这里生产的mypage.md是没有日期格式的，这个可以在_config.yml里配置new_post_name 12345$ vim _config.yml...# Writingnew_post_name: :year-:month-:day-:title.md # File name of new posts... 当然这里我们也可以自己在source/_posts/目录下创建markdown文件，填写自己的内容，为了支持分类和tag，建议markdown文件加入以下头部： 12345678---title: "my first blog"date: 2016-08-11 17:31:17tags: [tag1, tag2]categories: life---blog正文 若你在 source 目录下没有看到 tags, categories 等目录，可以通过如下命令创建: 12$ hexo new page tags$ hexo new page categories 以上的目录需求跟你使用的模板息息相关，详情请仔细阅读hexo帮助文档和你选择模板的README.md。 更新网站内容到github当本地网站测试满意后，我们就可以更新网站内容到pages github了 首先要配置_config.yml文件制定github repository 123456$ vim _config.yml# 添加deploy配置deploy: type: git repo: git@github.com:ictfox/ictfox.github.io.git branch: master 然后执行hexo命令部署 123$ hexo clean$ hexo generate$ hexo deploy 我写了个脚本来做部署这个事情，免得每次忘记执行某一步 123456789101112131415161718$ cat deploy.sh#!/bin/bashhexo cleanif [[ $? -ne 0 ]]; then echo "hexo clean Error!" return 1fihexo generateif [[ $? -ne 0 ]]; then echo "hexo generate Error!" return 1fihexo deployif [[ $? -ne 0 ]]; then echo "hexo deploy Error!" return 1fi 更新成功后，我们就可以通过浏览器访问github的二级域名查看网站了，默认为：YourGithubName.github.io 申请域名国内比较便捷的申请域名的服务是阿里和腾讯提供的 阿里 https://wanwang.aliyun.com/domain/ 腾讯 https://dnspod.qcloud.com/ 他们都需要注册对应的账户，登录后即可查询自己喜欢的域名是否还没有注册，然后按照流程注册即可。我对比发现腾讯的价格便宜，就选择了腾讯的服务。申请后可以在账号里看到自己的域名信息。 域名绑定鉴于国内对cn域名的使用需要审核，这里依com域名为例，cn域名审核后的步骤一样； github添加CNAME文件1$ echo 'www.yangguanjun.com' &gt; source/CNAME 然后执行deploy.py脚本提交到github repository 解析com域名在域名提供商的console界面里，找到域名后面的解析按钮，在里面添加CNAME的解析记录 然后访问域名就可以了 ;) 其他功能我选择的Next模板里，默认的 搜索 和 评论 功能是关闭的，而这两个还是挺有用的，自己可以配置打开 搜索功能Next主题里支持 tinysou 和 swiftype 两种搜索功能，我这里实践了 swiftype，感觉还是不错的 配置文件修改： 1234$ vim themes/next/_config.yml...swiftype_key: 'your swiftype key'... swiftype官网：https://swiftype.com/参考文章：http://prozhuchen.com/2015/10/03/Hexo%E5%8D%9A%E5%AE%A2%E7%AC%AC%E5%9B%9B%E7%AB%99/ 评论功能Next主题里支持 duoshuo 和 disqus 两种评论功能，duoshuo 马上就要关闭支持了，我就选择了国外的 disqus，但也有个问题，这个disqus是被墙的 ;(，不翻墙的用户是刷不出来的。。。 配置文件修改： 1234$ vim themes/next/_config.yml...disqus_shortname: ictfox... Disqus官网：https://disqus.com/参考文章：http://www.lmnsyunhao.cn/2017/03/29/hexo-disqus-comments/ 参考文章https://hexo.io/zh-cn/docs/http://www.jianshu.com/p/834d7cc0668dhttps://wsgzao.github.io/post/hexo-guide/https://linghucong.js.org/2016/04/15/2016-04-15-hexo-github-pages-blog/]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ceph配置参数详解]]></title>
    <url>%2F2017%2F05%2F15%2FCeph-configuration%2F</url>
    <content type="text"><![CDATA[概述Ceph的配置参数很多，从网上也能搜索到一大批的调优参数，但这些参数为什么这么设置？设置为这样是否合理？解释的并不多本文从当前我们的ceph.conf文件入手，解释其中的每一项配置，做为以后参数调优和新人学习的依据； 参数详解1，一些固定配置参数123fsid = 6d529c3d-5745-4fa5-be5f-3962a8e8687cmon_initial_members = mon1, mon2, mon3mon_host = 10.10.40.67,10.10.40.68,10.10.40.69 以上通常是通过ceph-deploy生成的，都是ceph monitor相关的参数，不用修改； 2，网络配置参数12public_network = 10.10.40.0/24 默认值 ""cluster_network = 10.10.41.0/24 默认值 "" public network：monitor与osd，client与monitor，client与osd通信的网络，最好配置为带宽较高的万兆网络；cluster network：OSD之间通信的网络，一般配置为带宽较高的万兆网络； 参考：http://docs.ceph.com/docs/master/rados/configuration/network-config-ref/ 3，pool size配置参数12osd_pool_default_size = 3 默认值 3osd_pool_default_min_size = 1 默认值 0 // 0 means no specific default; ceph will use size-size/2 这两个是创建ceph pool的时候的默认size参数，一般配置为3和1，3副本能足够保证数据的可靠性； 4，认证配置参数123auth_service_required = none 默认值 "cephx"auth_client_required = none 默认值 "cephx, none"auth_cluster_required = none 默认值 "cephx" 以上是Ceph authentication的配置参数，默认值为开启ceph认证；在内部使用的ceph集群中一般配置为none，即不使用认证，这样能适当加快ceph集群访问速度； 5，osd down out配置参数12345mon_osd_down_out_interval = 864000 默认值 300 // secondsmon_osd_min_down_reporters = 2 默认值 2mon_osd_report_timeout = 900 默认值 900osd_heartbeat_interval = 15 默认值 6osd_heartbeat_grace = 60 默认值 20 mon_osd_down_out_interval：ceph标记一个osd为down and out的最大时间间隔mon_osd_min_down_reporters：mon标记一个osd为down的最小reporters个数（报告该osd为down的其他osd为一个reporter）mon_osd_report_timeout：mon标记一个osd为down的最长等待时间osd_heartbeat_interval：osd发送heartbeat给其他osd的间隔时间（同一PG之间的osd才会有heartbeat）osd_heartbeat_grace：osd报告其他osd为down的最大时间间隔，grace调大，也有副作用，如果某个osd异常退出，等待其他osd上报的时间必须为grace，在这段时间段内，这个osd负责的pg的io会hang住，所以尽量不要将grace调的太大。 基于实际情况合理配置上述参数，能减少或及时发现osd变为down（降低IO hang住的时间和概率），延长osd变为down and out的时间（防止网络抖动造成的数据recovery）； 参考：http://docs.ceph.com/docs/master/rados/configuration/mon-osd-interaction/http://blog.wjin.org/posts/ceph-osd-heartbeat.html 6，objecter配置参数12objecter_inflight_ops = 10240 默认值 1024objecter_inflight_op_bytes = 1048576000 默认值 100M osd client端objecter的throttle配置，它的配置会影响librbd，RGW端的性能； 配置建议：调大这两个值 7，ceph rgw配置参数12345678910rgw_frontends = "civetweb port=10080 num_threads=2000" 默认值 "fastcgi, civetweb port=7480"rgw_thread_pool_size = 512 默认值 100rgw_override_bucket_index_max_shards = 20 默认值 0 rgw_max_chunk_size = 1048576 默认值 512 * 1024rgw_cache_lru_size = 1000000 默认值 10000 // num of entries in rgw cachergw_bucket_default_quota_max_objects = 10000000 默认值 -1 // number of objects allowed rgw_dns_name = object-storage.ffan.com 默认值rgw_swift_url = http://object-storage.ffan.com 默认值 rgw_frontends：rgw的前端配置，一般配置为使用轻量级的civetweb；prot为访问rgw的端口，根据实际情况配置；num_threads为civetweb的线程数；rgw_thread_pool_size：rgw前端web的线程数，与rgw_frontends中的num_threads含义一致，但num_threads 优于rgw_thread_pool_size的配置，两个只需要配置一个即可；rgw_override_bucket_index_max_shards：rgw bucket index object的最大shards数，增大这个值能减少bucket index object的访问时间，但也会加大bucket的ls时间；rgw_max_chunk_size：rgw最大chunk size，针对大文件的对象存储场景可以把这个值调大；rgw_cache_lru_size：rgw的lru cache size，对于读较多的应用场景，调大这个值能加快rgw的响应速度；rgw_bucket_default_quota_max_objects：配合该参数限制一个bucket的最大objects个数； 参考：http://docs.ceph.com/docs/jewel/install/install-ceph-gateway/http://ceph-users.ceph.narkive.com/mdB90g7R/rgw-increase-the-first-chunk-sizehttps://access.redhat.com/solutions/2122231 8，debug配置参数1234567891011121314151617181920212223242526272829debug_lockdep = 0/0debug_context = 0/0debug_crush = 0/0debug_buffer = 0/0debug_timer = 0/0debug_filer = 0/0debug_objecter = 0/0debug_rados = 0/0debug_rbd = 0/0debug_journaler = 0/0debug_objectcatcher = 0/0debug_client = 0/0debug_osd = 0/0debug_optracker = 0/0debug_objclass = 0/0debug_filestore = 0/0debug_journal = 0/0debug_ms = 0/0debug_mon = 0/0debug_monc = 0/0debug_tp = 0/0debug_auth = 0/0debug_finisher = 0/0debug_heartbeatmap = 0/0debug_perfcounter = 0/0debug_asok = 0/0debug_throttle = 0/0debug_paxos = 0/0debug_rgw = 0/0 关闭了所有的debug信息，能一定程度加快ceph集群速度，但也会丢失一些关键log，出问题的时候不好分析； 参考：http://www.10tiao.com/html/362/201609/2654062487/1.html 9，osd op配置参数123456osd_enable_op_tracker = false 默认值 trueosd_num_op_tracker_shard = 32 默认值 32osd_op_threads = 10 默认值 2osd_disk_threads = 1 默认值 1osd_op_num_shards = 32 默认值 5osd_op_num_threads_per_shard = 2 默认值 2 osd_enable_op_tracker：追踪osd op状态的配置参数，默认为true；不建议关闭，关闭后osd的 slow_request，ops_in_flight，historic_ops 无法正常统计； 1234# ceph daemon /var/run/ceph/ceph-osd.0.asok dump_ops_in_flightop_tracker tracking is not enabled now, so no ops are tracked currently, even those get stuck. Please enable "osd_enable_op_tracker", and the tracker will start to track new ops received afterwards.# ceph daemon /var/run/ceph/ceph-osd.0.asok dump_historic_opsop_tracker tracking is not enabled now, so no ops are tracked currently, even those get stuck. Please enable "osd_enable_op_tracker", and the tracker will start to track new ops received afterwards. 打开op tracker后，若集群iops很高，osd_num_op_tracker_shard可以适当调大，因为每个shard都有个独立的mutex锁； 123456789101112class OpTracker &#123;... struct ShardedTrackingData &#123; Mutex ops_in_flight_lock_sharded; xlist&lt;TrackedOp *&gt; ops_in_flight_sharded; explicit ShardedTrackingData(string lock_name): ops_in_flight_lock_sharded(lock_name.c_str()) &#123;&#125; &#125;; vector&lt;ShardedTrackingData*&gt; sharded_in_flight_list; uint32_t num_optracker_shards;...&#125;; osd_op_threads：对应的work queue有peering_wq（osd peering请求），recovery_gen_wq（PG recovery请求）；osd_disk_threads：对应的work queue为 remove_wq（PG remove请求）；osd_op_num_shards和osd_op_num_threads_per_shard：对应的thread pool为osd_op_tp，work queue为op_shardedwq； 处理的请求包括： OpRequestRef PGSnapTrim PGScrub 调大osd_op_num_shards可以增大osd ops的处理线程数，增大并发性，提升OSD性能； 10，osd client message配置参数12osd_client_message_size_cap = 1048576000 默认值 500*1024L*1024L // client data allowed in-memory (in bytes)osd_client_message_cap = 10000 默认值 100 // num client messages allowed in-memory 这个是osd端收到client messages的capacity配置，配置大的话能提升osd的处理能力，但会占用较多的系统内存； 配置建议：服务器内存足够大的时候，适当增大这两个值 11，osd scrub配置参数1234567891011osd_scrub_begin_hour = 2 默认值 0osd_scrub_end_hour = 6 默认值 24 // The time in seconds that scrubbing sleeps between two consecutive scrubsosd_scrub_sleep = 2 默认值 0 // sleep between [deep]scrub ops osd_scrub_load_threshold = 5 默认值 0.5 // chunky scrub配置的最小/最大objects数，以下是默认值osd_scrub_chunk_min = 5osd_scrub_chunk_max = 25 Ceph osd scrub是保证ceph数据一致性的机制，scrub以PG为单位，但每次scrub回获取PG lock，所以它可能会影响PG正常的IO； Ceph后来引入了chunky的scrub模式，每次scrub只会选取PG的一部分objects，完成后释放PG lock，并把下一次的PG scrub加入队列；这样能很好的减少PG scrub时候占用PG lock的时间，避免过多影响PG正常的IO； 同理，引入的osd_scrub_sleep参数会让线程在每次scrub前释放PG lock，然后睡眠一段时间，也能很好的减少scrub对PG正常IO的影响； 配置建议： osd_scrub_begin_hour和osd_scrub_end_hour：OSD Scrub的开始结束时间，根据具体业务指定； osd_scrub_sleep：osd在每次执行scrub时的睡眠时间；有个bug跟这个配置有关，建议关闭； osd_scrub_load_threshold：osd开启scrub的系统load阈值，根据系统的load average值配置该参数； osd_scrub_chunk_min和osd_scrub_chunk_max：根据PG中object的个数配置；针对RGW全是小文件的情况，这两个值需要调大； 参考：http://www.jianshu.com/p/ea2296e1555chttp://tracker.ceph.com/issues/19497 12，osd thread timeout配置参数12345osd_op_thread_timeout = 580 默认值 15osd_op_thread_suicide_timeout = 600 默认值 150 osd_recovery_thread_timeout = 580 默认值 30osd_recovery_thread_suicide_timeout = 600 默认值 300 osd_op_thread_timeout和osd_op_thread_suicide_timeout关联的work queue为： op_shardedwq - 关联的请求为：OpRequestRef，PGSnapTrim，PGScrub peering_wq - 关联的请求为：osd peering osd_recovery_thread_timeout和osd_recovery_thread_suicide_timeout关联的work queue为： recovery_wq - 关联的请求为：PG recovery Ceph的work queue都有个基类WorkQueue_，定义如下： 1234567891011/// Pool of threads that share work submitted to multiple work queues.class ThreadPool : public md_config_obs_t &#123;... /// Basic interface to a work queue used by the worker threads. struct WorkQueue_ &#123; string name; time_t timeout_interval, suicide_interval; WorkQueue_(string n, time_t ti, time_t sti) : name(n), timeout_interval(ti), suicide_interval(sti) &#123; &#125;... 这里的timeout_interval和suicide_interval分别对应上面所述的配置timeout和suicide_timeout；当thread处理work queue中的一个请求时，会受到这两个timeout时间的限制： timeout_interval - 到时间后设置m_unhealthy_workers+1 suicide_interval - 到时间后调用assert，OSD进程crush 对应的处理函数为： 123456789101112131415161718bool HeartbeatMap::_check(const heartbeat_handle_d *h, const char *who, time_t now)&#123; bool healthy = true; time_t was; was = h-&gt;timeout.read(); if (was &amp;&amp; was &lt; now) &#123; ldout(m_cct, 1) &lt;&lt; who &lt;&lt; " '" &lt;&lt; h-&gt;name &lt;&lt; "'" &lt;&lt; " had timed out after " &lt;&lt; h-&gt;grace &lt;&lt; dendl; healthy = false; &#125; was = h-&gt;suicide_timeout.read(); if (was &amp;&amp; was &lt; now) &#123; ldout(m_cct, 1) &lt;&lt; who &lt;&lt; " '" &lt;&lt; h-&gt;name &lt;&lt; "'" &lt;&lt; " had suicide timed out after " &lt;&lt; h-&gt;suicide_grace &lt;&lt; dendl; assert(0 == "hit suicide timeout"); &#125; return healthy;&#125; 当前仅有RGW添加了worker的perfcounter，所以也只有RGW可以通过perf dump查看total/unhealthy的worker信息： 123[root@ yangguanjun]# ceph daemon /var/run/ceph/ceph-client.rgw.rgwdaemon.asok perf dump | grep worker "total_workers": 32, "unhealthy_workers": 0 对应的配置项为： 12345678910111213OPTION(rgw_num_async_rados_threads, OPT_INT, 32) // num of threads to use for async rados operations``` **配置建议：**- `*_thread_timeout`：这个值配置越小越能及时发现处理慢的请求，所以不建议配置很大；特别是针对速度快的设备，建议调小该值；- `*_thread_suicide_timeout`：这个值配置小了会导致超时后的OSD crush，所以建议调大；特别是在对应的throttle调大后，更应该调大该值；### 13，fielstore op thread配置参数```shfilestore_op_threads = 10 默认值 2filestore_op_thread_timeout = 580 默认值 60filestore_op_thread_suicide_timeout = 600 默认值 180 filestore_op_threads：对应的thread pool为op_tp，对应的work queue为op_wq；filestore的所有请求都经过op_wq处理；增大该参数能提升filestore的处理能力，提升filestore的性能；配合filestore的throttle一起调整； filestore_op_thread_timeout和filestore_op_thread_suicide_timeout关联的work queue为：op_wq 配置的含义与上一节中的thread_timeout/thread_suicide_timeout保持一致； 13，filestore merge/split配置参数12filestore_merge_threshold = -1 默认值 10filestore_split_multiple = 16000 默认值 2 这两个参数是管理filestore的目录分裂/合并的，filestore的每个目录允许的最大文件数为：filestore_split_multiple * abs(filestore_merge_threshold) * 16 在RGW的小文件应用场景，会很容易达到默认配置的文件数（320），若在写的过程中触发了filestore的分裂，则会非常影响filestore的性能； 每次filestore的目录分裂，会依据如下规则分裂为多层目录，最底层16个子目录：例如PG 31.4C0, hash结尾是4C0，若该目录分裂，会分裂为 DIR_0/DIR_C/DIR_4/{DIR_0, DIR_F}； 原始目录下的object会根据规则放到不同的子目录里，object的名称格式为: *__head_xxxxX4C0_*，分裂时候X是几，就放进子目录DIR_X里。比如object：*__head_xxxxA4C0_*, 就放进子目录 DIR_0/DIR_C/DIR_4/DIR_A 里； 解决办法： 增大merge/split配置参数的值，使单个目录容纳更多的文件； filestore_merge_threshold配置为负数；这样会提前触发目录的预分裂，避免目录在某一时间段的集中分裂，详细机制没有调研； 创建pool时指定expected-num-objects；这样会依据目录分裂规则，在创建pool的时候就创建分裂的子目录，避免了目录分裂对filestore性能的影响； 参考：http://docs.ceph.com/docs/master/rados/configuration/filestore-config-ref/http://docs.ceph.com/docs/jewel/rados/operations/pools/#create-a-poolhttp://blog.csdn.net/for_tech/article/details/51251936http://ivanjobs.github.io/page3/ 14，filestore fd cache配置参数12filestore_fd_cache_shards = 32 默认值 16 // FD number of shardsfilestore_fd_cache_size = 32768 默认值 128 // FD lru size filestore的fd cache是加速访问filestore里的file的，在非一次性写入的应用场景，增大配置可以很明显的提升filestore的性能； 15，filestore sync配置参数1234filestore_wbthrottle_enable = false 默认值 true SSD的时候建议关闭filestore_min_sync_interval = 5 默认值 0.01 s 最小同步间隔秒数，sync fs的数据到disk，FileStore::sync_entry()filestore_max_sync_interval = 10 默认值 5 s 最大同步间隔秒数，sync fs的数据到disk，FileStore::sync_entry()filestore_commit_timeout = 3000 默认值 600 s FileStore::sync_entry() 里 new SyncEntryTimeout(m_filestore_commit_timeout) filestore_wbthrottle_enable的配置是关于filestore writeback throttle的，即我们说的filestore处理workqueue op_wq的数据量阈值；默认值是true，开启后XFS相关的配置参数有： 123456OPTION(filestore_wbthrottle_xfs_bytes_start_flusher, OPT_U64, 41943040)OPTION(filestore_wbthrottle_xfs_bytes_hard_limit, OPT_U64, 419430400)OPTION(filestore_wbthrottle_xfs_ios_start_flusher, OPT_U64, 500)OPTION(filestore_wbthrottle_xfs_ios_hard_limit, OPT_U64, 5000)OPTION(filestore_wbthrottle_xfs_inodes_start_flusher, OPT_U64, 500)OPTION(filestore_wbthrottle_xfs_inodes_hard_limit, OPT_U64, 5000) 若使用普通HDD，可以保持其为true；针对SSD，建议将其关闭，不开启writeback throttle； filestore_min_sync_interval和filestore_max_sync_interval是配置filestore flush outstanding IO到disk的时间间隔的；增大配置可以让系统做尽可能多的IO merge，减少filestore写磁盘的压力，但也会增大page cache占用内存的开销，增大数据丢失的可能性； filestore_commit_timeout是配置filestore sync entry到disk的超时时间，在filestore压力很大时，调大这个值能尽量避免IO超时导致OSD crush； 16，filestore throttle配置参数1234567891011filestore_expected_throughput_bytes = 536870912 默认值 200MB /// Expected filestore throughput in B/sfilestore_expected_throughput_ops = 2500 默认值 200 /// Expected filestore throughput in ops/sfilestore_queue_max_bytes= 1048576000 默认值 100MBfilestore_queue_max_ops = 5000 默认值 50 /// Use above to inject delays intended to keep the op queue between low and highfilestore_queue_low_threshhold = 0.6 默认值 0.3filestore_queue_high_threshhold = 0.9 默认值 0.9 filestore_queue_high_delay_multiple = 2 默认值 0 /// Filestore high delay multiple. Defaults to 0 (disabled)filestore_queue_max_delay_multiple = 10 默认值 0 /// Filestore max delay multiple. Defaults to 0 (disabled) 在jewel版本里，引入了dynamic throttle，来平滑普通throttle带来的长尾效应问题； 一般在使用普通磁盘时，之前的throttle机制即可很好的工作，所以这里默认filestore_queue_high_delay_multiple和filestore_queue_max_delay_multiple都为0； 针对高速磁盘，需要在部署之前，通过小工具ceph_smalliobenchfs来测试下，获取合适的配置参数； 123456789101112131415161718192021222324252627BackoffThrottle的介绍如下：/*** BackoffThrottle** Creates a throttle which gradually induces delays when get() is called* based on params low_threshhold, high_threshhold, expected_throughput,* high_multiple, and max_multiple.** In [0, low_threshhold), we want no delay.** In [low_threshhold, high_threshhold), delays should be injected based* on a line from 0 at low_threshhold to* high_multiple * (1/expected_throughput) at high_threshhold.** In [high_threshhold, 1), we want delays injected based on a line from* (high_multiple * (1/expected_throughput)) at high_threshhold to* (high_multiple * (1/expected_throughput)) +* (max_multiple * (1/expected_throughput)) at 1.** Let the current throttle ratio (current/max) be r, low_threshhold be l,* high_threshhold be h, high_delay (high_multiple / expected_throughput) be e,* and max_delay (max_muliple / expected_throughput) be m.** delay = 0, r \in [0, l)* delay = (r - l) * (e / (h - l)), r \in [l, h)* delay = h + (r - h)((m - e)/(1 - h))*/ 参考：http://docs.ceph.com/docs/jewel/dev/osd_internals/osd_throttles/http://blog.wjin.org/posts/ceph-dynamic-throttle.htmlhttps://github.com/ceph/ceph/blob/master/src/doc/dynamic-throttle.txtCeph BackoffThrottle分析 17，filestore finisher threads配置参数12filestore_ondisk_finisher_threads = 2 默认值 1filestore_apply_finisher_threads = 2 默认值 1 这两个参数定义filestore commit/apply的finisher处理线程数，默认都为1，任何IO commit/apply完成后，都需要经过对应的ondisk/apply finisher thread处理； 在使用普通HDD时，磁盘性能是瓶颈，单个finisher thread就能处理好；但在使用高速磁盘的时候，IO完成比较快，单个finisher thread不能处理这么多的IO commit/apply reply，它会成为瓶颈；所以在jewel版本里引入了finisher thread pool的配置，这里一般配置为2即可； 18，journal配置参数123456789journal_max_write_bytes=1048576000 默认值 10M journal_max_write_entries=5000 默认值 100 journal_throttle_high_multiple = 2 默认值 0 /// Multiple over expected at high_threshhold. Defaults to 0 (disabled).journal_throttle_max_multiple = 10 默认值 0 /// Multiple over expected at max. Defaults to 0 (disabled)./// Target range for journal fullnessOPTION(journal_throttle_low_threshhold, OPT_DOUBLE, 0.6)OPTION(journal_throttle_high_threshhold, OPT_DOUBLE, 0.9) journal_max_write_bytes和journal_max_write_entries是journal一次write的数据量和entries限制；针对SSD分区做journal的情况，这两个值要增大，这样能增大journal的吞吐量； journal_throttle_high_multiple和journal_throttle_max_multiple是JournalThrottle的配置参数，JournalThrottle是BackoffThrottle的封装类，所以JournalThrottle与我们在filestore throttle介绍的dynamic throttle工作原理一样； 12345678910111213int FileJournal::set_throttle_params()&#123; stringstream ss; bool valid = throttle.set_params( g_conf-&gt;journal_throttle_low_threshhold, g_conf-&gt;journal_throttle_high_threshhold, g_conf-&gt;filestore_expected_throughput_bytes, g_conf-&gt;journal_throttle_high_multiple, g_conf-&gt;journal_throttle_max_multiple, header.max_size - get_top(), &amp;ss);...&#125; 从上述代码中看出相关的配置参数有： journal_throttle_low_threshhold journal_throttle_high_threshhold filestore_expected_throughput_bytes 19，rbd cache配置参数123456[client]rbd_cache_size = 134217728 默认值 32M // cache size in bytesrbd_cache_max_dirty = 100663296 默认值 24M // dirty limit in bytes - set to 0 for write-through cachingrbd_cache_target_dirty = 67108864 默认值 16M // target dirty limit in bytesrbd_cache_writethrough_until_flush = true 默认值 true // whether to make writeback caching writethrough until flush is called, to be sure the user of librbd will send flushs so that writeback is saferbd_cache_max_dirty_age = 5 默认值 1.0 // seconds in cache before writeback starts rbd_cache_size：client端每个rbd image的cache size，不需要太大，可以调整为64M，不然会比较占client端内存；参照默认值，根据rbd_cache_size的大小调整rbd_cache_max_dirty和rbd_cache_target_dirty； rbd_cache_max_dirty：在writeback模式下cache的最大bytes数，默认是24MB；当该值为0时，表示使用writethrough模式； rbd_cache_target_dirty：在writeback模式下cache向ceph集群写入的bytes阀值，默认16MB；注意该值一定要小于rbd_cache_max_dirty值 rbd_cache_writethrough_until_flush：在内核触发flush cache到ceph集群前rbd cache一直是writethrough模式，直到flush后rbd cache变成writeback模式；rbd_cache_max_dirty_age：标记OSDC端ObjectCacher中entry在cache中的最长时间； 参考https://my.oschina.net/linuxhunter/blog/541997]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>osd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对象存储跨机房容灾调研]]></title>
    <url>%2F2017%2F05%2F09%2Frgw-multi-region%2F</url>
    <content type="text"><![CDATA[概述随着机房的增加和客户对可靠性的要求提高，我们需要提供数据的跨机房容灾，本文档结合Ceph Radosgw的实现描述对象存储的跨机房容灾。 当前状况当前应用中，每个机房的Ceph都是独立的cluster，彼此之间没有任何关系。 多个机房都独立的提供对象存储功能，每个Ceph Radosgw都有自己独立的命名空间和存储空间。 这样带来两个问题：1，针对Radosgw来说，我们的业务没法提供统一的命名空间； 2，没有机房级别的容灾，若一个机房Radosgw无法访问，则机房提供的对象存储瘫痪； 方案针对上述中的情况和需求，Ceph的Radosgw原生支持的联合架构，支持Region，Zone级别的备份同步，能很好的解决这个问题。 Ceph要求 版本Ceph version 0.67以后即可，我们当前使用的0.94.5支持该feature 每个Ceph cluster必须配置启动RADOSGW 概念解释 Region: A region represents a logical geographic area and contains one or more zones. A cluster with multiple regions must specify a master region. Zone: A zone is a logical grouping of one or more Ceph Object Gateway instance(s). A region has a master zone that processes client requests.注：Only write objects to the master zone in a region. You may read objects from secondary zones. Currently, the Gateway does not prevent you from writing to a secondary zone, but DON’T DO IT. 应用模式1同一Region内的不同Zones之间数据同步，如下图所示： 特点： 主zone支持读写操作 从zone支持读操作 主从zone之间同步数据 用途： 主从zone之间数据备份，提供跨机房容灾 从zone分担主zone的读数据压力，提高集群扩展性 应用模式2不同Regions之间的数据同步，如下图所示： 特点： 主从Region都支持读写操作 主从Region之间只同步元数据信息 主从Region之间数据是独立的 用途： 多个Regions一起来提供统一命名空间 不同Region服务不同地区的客户，提高集群扩展性 展望在10.2 jewel版本中，radosgw增加了一个新特性，也就是多中心同步。 所谓多中心同步，也就是不同radosgw实例之间可以自动进行同步的特性。 为了便于管理，又进行了逻辑上的组织，也即realm、zonegroup、zone和radosgw。这是个从上至下的组织结构，也就是realm包括若干个zonegroup，其中一个是master，其它是secondary。每个zonegroup下包括一个master zone，其它都是secondary zone。 zone可以认为是一个实际上的集群，由至少一个radosgw实例提供服务，但是对外服务地址是一致的。 同步分两种，也就是元数据同步和数据同步。zonegroup之间只会同步元数据，而同一个zonegroup下的zone之间是同步元数据和数据。这里的元数据是指用户和桶，数据是用户的对象数据。 概念： Zone: A zone is logical grouping of one or more Ceph Object Gateway instances. There will be one Zone that should be designated as the master zone in a zonegroup, which will handle all bucket and user creation. Zonegroup: A zonegroup consists of multiple zones, this approximately corresponds to what used to be called as a region in pre Jewel releases for federated deployments. There should be a master zonegroup that will handle changes to the system configuration. Zonegroup map: A zonegroup map is a configuration structure that holds the map of the entire system, ie. which zonegroup is the master, relationships between different zonegroups and certain configurables like storage policies. Realm: A realm is a container for zonegroups, this allows for separation of zonegroups themselves between clusters. It is possible to create multiple realms, making it easier to run completely different configurations in the same cluster. Period: A period holds the configuration structure for the current state of the realm. Every period contains a unique id and an epoch. A period’s epoch is incremented on every commit operation. Every realm has an associated current period, holding the current state of configuration of the zonegroups and storage policies. Any configuration change for a non master zone will increment the period’s epoch. Changing the master zone to a different zone will trigger the following changes: - A new period is generated with a new period id and epoch of 1 - Realm’s current period is updated to point to the newly generated period id - Realm’s epoch is incremented 参考资料http://docs.ceph.com/docs/master/radosgw/federated-config/ http://qinghua.github.io/ceph-radosgw-replication/ http://www.cnphp6.com/archives/79981 http://lyang.top/2016/01/18/Radosgw-%E5%A4%9A-zone-%E9%83%A8%E7%BD%B2%E4%BB%A5%E5%8F%8A-radosgw_agent-%E5%90%8C%E6%AD%A5/]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>radosgw</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[radosgw配置user使用指定data pool]]></title>
    <url>%2F2017%2F05%2F08%2Frgw-user-config-datapool%2F</url>
    <content type="text"><![CDATA[概述在使用radosgw提供s3服务的时候，有时候我们会需要指定user使用不同的data pool，或者不同的data index pool； 在Jewel版本里，我们可以通过配置placement target来实现； 首先介绍下Jewel版本里radosgw的几个概念： 缩略语 全称 Realm 域，同一域内的账户在其下属的zone上是通用的。一个域下只能有一个主zonegroup，从的zonegroup可以是0或多个。 Period 表示域的有效期，在域的结构发生变化时，其Period会相应变化。 Zonegroup Zone的集合，等价于之前的Region。一个Zonegroup下只能有一个主Zone。主Zone和从Zone可以部署在同一集群上，也可以部署在不同的集群上。 Zone 表示独立的一个对象存储区域。 代码解析zone placementRadosgw里的user bucket数据存放哪里，是有user里的placement确定的，每个placement info可以指定三个pool，如下所示： 1234567struct RGWZonePlacementInfo &#123; string index_pool; string data_pool; string data_extra_pool; /* if not set we should use data_pool */ RGWBucketIndexType index_type;...&#125; 每个zone里可以用多个placement，定义如下： 1234567891011121314151617struct RGWZoneParams : RGWSystemMetaObj &#123; rgw_bucket domain_root; rgw_bucket metadata_heap; rgw_bucket control_pool; // 每个zone里的下面一些pool是只能配置一个的 rgw_bucket gc_pool; rgw_bucket log_pool; rgw_bucket intent_log_pool; rgw_bucket usage_log_pool; rgw_bucket user_keys_pool; rgw_bucket user_email_pool; rgw_bucket user_swift_pool; rgw_bucket user_uid_pool; RGWAccessKey system_key; map&lt;string, RGWZonePlacementInfo&gt; placement_pools; // zone里支持配置多个placement...&#125; zonegroup placementzonegroup里的placement target的定义如下，每个zonegroup可以配置多个placement targets，每个placement targets有name，tags（对应user info里的配置）标识； 这里的placement target name都对应着zone placement_pools里的string 关键字； 123456789101112131415161718struct RGWZoneGroupPlacementTarget &#123; string name; set&lt;string&gt; tags; // 可以有多个tags，对应RGWUserInfo里的placement_tags...&#125;; struct RGWZoneGroup : public RGWSystemMetaObj &#123; string api_name; list&lt;string&gt; endpoints; bool is_master; string master_zone; map&lt;string, RGWZone&gt; zones; // 配置多个zones map&lt;string, RGWZoneGroupPlacementTarget&gt; placement_targets; // zonegroup支持配置多个placement targets string default_placement;...&#125;; user info在每个user里，可以配置default placement，也支持配置多个placement tags； user支持在创建bucket的时候指定placement； 123456789struct RGWUserInfo&#123; uint64_t auid; rgw_user user_id;... string default_placement; // 指定默认的placement list&lt;string&gt; placement_tags; // tags里的值需配置为placement target里tags集合里的值...&#125;; 操作步骤下面通过增加一个test的placement target，让rgw user miketest使用pool default.rgw.buckets.data.tst 作为其bucket的data pool； 创建pool根据需要通过ceph osd pool 命令创建要使用的pool，例如： 12# ceph osd pool create default.rgw.buckets.data.tst 1024 1024# ceph osd pool set &lt;poolname&gt; crush_ruleset 2 // 根据需要配置pool的crush rule 添加zone的placement可以先list下当前的zone的placement 123456789101112# radosgw-admin zone placement list --rgw-zone=default[ &#123; "key": "default-placement", "val": &#123; "index_pool": "default.rgw.buckets.index", "data_pool": "default.rgw.buckets.data", "data_extra_pool": "default.rgw.buckets.non-ec", "index_type": 0 &#125; &#125;] zone添加placement的命令如下（指定zone placement的各个pool）： 123456789101112131415161718192021# radosgw-admin zone placement add --rgw-zone=default --placement-id=test --index_pool=default.rgw.buckets.index --data_pool=default.rgw.buckets.data.tst --data_extra_pool=default.rgw.buckets.non-ec[ &#123; "key": "default-placement", "val": &#123; "index_pool": "default.rgw.buckets.index", "data_pool": "default.rgw.buckets.data", "data_extra_pool": "default.rgw.buckets.non-ec", "index_type": 0 &#125; &#125;, &#123; "key": "test", "val": &#123; "index_pool": "default.rgw.buckets.index", "data_pool": "default.rgw.buckets.data.tst", // 这里仅仅指定了data_pool为新的pool "data_extra_pool": "default.rgw.buckets.non-ec", "index_type": 0 &#125; &#125;] 也可以通过如下命令来添加zone的placement： 123# radosgw-admin zone get --rgw-zone=default &gt; zone.info# vim zone.info // 添加 key为test的placement# radosgw-admin zone set --rgw-zone=default &lt; zone.info 添加zonegroup的placement查看当前的zonegroup的placement 12345678910# radosgw-admin zonegroup placement list[ &#123; "key": "default-placement", "val": &#123; "name": "default-placement", "tags": [] &#125; &#125;] zonegroup添加新的placement 1234567891011121314151617# radosgw-admin zonegroup placement add --rgw-zonegroup=default --placement-id=test[ &#123; "key": "default-placement", "val": &#123; "name": "default-placement", "tags": [] &#125; &#125;, &#123; "key": "test", "val": &#123; "name": "test", "tags": [] &#125; &#125;] 更新user info查看user的信息 1234567891011121314151617181920212223242526272829303132333435# radosgw-admin user info --uid=miketest&#123; "user_id": "miketest", "display_name": "miketest", "email": "", "suspended": 0, "max_buckets": 1000, "auid": 0, "subusers": [], "keys": [ &#123; "user": "miketest", "access_key": "7J82YY2ODPQ22337N392", "secret_key": "IgRmuo11OrxRlWa7TEicHSbbUFNYP2MDwZ8St2PS" &#125; ], "swift_keys": [], "caps": [], "op_mask": "read, write, delete", "default_placement": "default-placement", "placement_tags": [ "default-placement" ], "bucket_quota": &#123; "enabled": false, "max_size_kb": -1, "max_objects": -1 &#125;, "user_quota": &#123; "enabled": false, "max_size_kb": -1, "max_objects": -1 &#125;, "temp_url_keys": []&#125; 导出user info，修改后导入新的信息，添加新的placement_tags： 123456789101112131415161718192021222324252627282930313233343536373839# radosgw-admin metadata get user:miketest &gt; miketest.userinfo# vim miketest.userinfo# radosgw-admin metadata put user:miketest &lt; miketest.userinfo# radosgw-admin user info --uid=miketest&#123; "user_id": "miketest", "display_name": "miketest", "email": "", "suspended": 0, "max_buckets": 1000, "auid": 0, "subusers": [], "keys": [ &#123; "user": "miketest", "access_key": "7J82YY2ODPQ22337N392", "secret_key": "IgRmuo11OrxRlWa7TEicHSbbUFNYP2MDwZ8St2PS" &#125; ], "swift_keys": [], "caps": [], "op_mask": "read, write, delete", "default_placement": "default-placement", // 这里也可以配置default为 test placement "placement_tags": [ "default-placement", "test" // 添加了名为test的placement ], "bucket_quota": &#123; "enabled": false, "max_size_kb": -1, "max_objects": -1 &#125;, "user_quota": &#123; "enabled": false, "max_size_kb": -1, "max_objects": -1 &#125;, "temp_url_keys": []&#125; 更新zonegroup map更新完zonegroup的placement后，虽说也可以查看到新的placement，但这些并没有更新到zonegroup map里； 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566# radosgw-admin zonegroup placement list[ &#123; "key": "default-placement", "val": &#123; "name": "default-placement", "tags": [] &#125; &#125;, &#123; "key": "test", "val": &#123; "name": "test", "tags": [] &#125; &#125;] # radosgw-admin zonegroup-map get&#123; "zonegroups": [ &#123; "key": "342414eb-ee98-40b9-b9d5-971ad14d2dd0", "val": &#123; "id": "342414eb-ee98-40b9-b9d5-971ad14d2dd0", "name": "default", "api_name": "", "is_master": "true", "endpoints": [], "hostnames": [], "hostnames_s3website": [], "master_zone": "f31f020f-aaa2-4570-b6f7-19258064e5b5", "zones": [ &#123; "id": "f31f020f-aaa2-4570-b6f7-19258064e5b5", "name": "default", "endpoints": [], "log_meta": "false", "log_data": "false", "bucket_index_max_shards": 0, "read_only": "false" &#125; ], "placement_targets": [ // 看到这里并没有新加的test placement &#123; "name": "default-placement", "tags": [] &#125; ], "default_placement": "default-placement", "realm_id": "7eeb01eb-3774-4abe-ba93-c205cc3c83af" &#125; &#125; ], "master_zonegroup": "342414eb-ee98-40b9-b9d5-971ad14d2dd0", "bucket_quota": &#123; "enabled": false, "max_size_kb": -1, "max_objects": -1 &#125;, "user_quota": &#123; "enabled": false, "max_size_kb": -1, "max_objects": -1 &#125;&#125; 执行zonegroup map更新：radosgw-admin period update --commit 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970# radosgw-admin period update --commit&#123; "id": "bd50c829-f7eb-4aa9-ad0a-efa814dfc3cc", "epoch": 2, "predecessor_uuid": "da2e2b0d-a17a-46fb-8369-6cb373ffd08d", "sync_status": [... ], "period_map": &#123; "id": "bd50c829-f7eb-4aa9-ad0a-efa814dfc3cc", "zonegroups": [ &#123; "id": "342414eb-ee98-40b9-b9d5-971ad14d2dd0", "name": "default", "api_name": "", "is_master": "true", "endpoints": [], "hostnames": [], "hostnames_s3website": [], "master_zone": "f31f020f-aaa2-4570-b6f7-19258064e5b5", "zones": [ &#123; "id": "f31f020f-aaa2-4570-b6f7-19258064e5b5", "name": "default", "endpoints": [], "log_meta": "true", "log_data": "false", "bucket_index_max_shards": 0, "read_only": "false" &#125; ], "placement_targets": [ &#123; "name": "default-placement", "tags": [] &#125;, &#123; "name": "test", // 更新后有 test placement "tags": [] &#125; ], "default_placement": "default-placement", "realm_id": "7eeb01eb-3774-4abe-ba93-c205cc3c83af" &#125; ], "short_zone_ids": [ &#123; "key": "f31f020f-aaa2-4570-b6f7-19258064e5b5", "val": 568702538 &#125; ] &#125;, "master_zonegroup": "342414eb-ee98-40b9-b9d5-971ad14d2dd0", "master_zone": "f31f020f-aaa2-4570-b6f7-19258064e5b5", "period_config": &#123; "bucket_quota": &#123; "enabled": false, "max_size_kb": -1, "max_objects": -1 &#125;, "user_quota": &#123; "enabled": false, "max_size_kb": -1, "max_objects": -1 &#125; &#125;, "realm_id": "7eeb01eb-3774-4abe-ba93-c205cc3c83af", "realm_name": "default", "realm_epoch": 2&#125; 重启radosgw服务重启radosgw服务，Centos里的命令为： 1# systemctl restart ceph-radosgw.target 测试通过s3cmd就可以测试，命令如下： 123✗ s3cmd mb s3://foxtst --bucket-location=:testBucket 's3://foxtst/' created✗ s3cmd put pg_by_osd.sh s3://foxtst 在ceph集群里，可以查看到上传的文件pg_by_osd.sh在指定的pool里：default.rgw.buckets.data.tst 12# rados ls -p default.rgw.buckets.data.tstf31f020f-aaa2-4570-b6f7-19258064e5b5.1224751.1_pg_by_osd.sh 参考http://docs.ceph.com/docs/jewel/radosgw/multisite/http://www.jianshu.com/p/31a6f8df9a8fhttp://ceph.org.cn/2017/03/24/ceph-rgwj%E4%BB%A5%E4%B8%8A%E7%89%88%E6%9C%AC%E7%9A%84%E7%94%A8%E6%88%B7placement%E8%AE%BE%E7%BD%AE-by_%E7%A7%A6%E7%89%A7%E7%BE%8A/]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>radosgw</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ceph OSD op_shardedwq分析]]></title>
    <url>%2F2017%2F05%2F02%2FCeph-OSD-op_shardedwq%2F</url>
    <content type="text"><![CDATA[概述Ceph OSD处理OP，snap trim，scrub的是相同的work queue - osd::op_shardedwq 研究该shardedwq，有利于我们对snap trim和scrub的配置参数调整； 相关数据结构这里主要涉及到两个数据结构： class PGQueueable class ShardedOpWQ class PGQueueable这个是封装PG一些请求的class，相关的操作有： OpRequestRef PGSnapTrim PGScrub 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class PGQueueable &#123; typedef boost::variant&lt; OpRequestRef, PGSnapTrim, PGScrub &gt; QVariant; // 定义队列处理的三种请求 QVariant qvariant; int cost; unsigned priority; utime_t start_time; entity_inst_t owner; struct RunVis : public boost::static_visitor&lt;&gt; &#123; OSD *osd; PGRef &amp;pg; ThreadPool::TPHandle &amp;handle; RunVis(OSD *osd, PGRef &amp;pg, ThreadPool::TPHandle &amp;handle) : osd(osd), pg(pg), handle(handle) &#123;&#125; void operator()(OpRequestRef &amp;op); void operator()(PGSnapTrim &amp;op); void operator()(PGScrub &amp;op); &#125;; public: // cppcheck-suppress noExplicitConstructor PGQueueable(OpRequestRef op) // 处理OpRequest : qvariant(op), cost(op-&gt;get_req()-&gt;get_cost()), priority(op-&gt;get_req()-&gt;get_priority()), start_time(op-&gt;get_req()-&gt;get_recv_stamp()), owner(op-&gt;get_req()-&gt;get_source_inst()) &#123;&#125; PGQueueable( // 处理PGSnapTrim const PGSnapTrim &amp;op, int cost, unsigned priority, utime_t start_time, const entity_inst_t &amp;owner) : qvariant(op), cost(cost), priority(priority), start_time(start_time), owner(owner) &#123;&#125; PGQueueable( // 处理PGScrub const PGScrub &amp;op, int cost, unsigned priority, utime_t start_time, const entity_inst_t &amp;owner) : qvariant(op), cost(cost), priority(priority), start_time(start_time), owner(owner) &#123;&#125;... void run(OSD *osd, PGRef &amp;pg, ThreadPool::TPHandle &amp;handle) &#123; RunVis v(osd, pg, handle); boost::apply_visitor(v, qvariant); &#125;...&#125;; class ShardedOpWQ这个是OSD中shard相关线程的work queue类，用来处理PGQueueable封装的三类PG操作； 12345678910111213141516171819202122232425262728293031323334353637383940414243class OSD : public Dispatcher, public md_config_obs_t&#123;... friend class PGQueueable; class ShardedOpWQ: public ShardedThreadPool::ShardedWQ &lt; pair &lt;PGRef, PGQueueable&gt; &gt; &#123; struct ShardData &#123; Mutex sdata_lock; Cond sdata_cond; Mutex sdata_op_ordering_lock; map&lt;PG*, list&lt;PGQueueable&gt; &gt; pg_for_processing; std::unique_ptr&lt;OpQueue&lt; pair&lt;PGRef, PGQueueable&gt;, entity_inst_t&gt;&gt; pqueue; ShardData( string lock_name, string ordering_lock, uint64_t max_tok_per_prio, uint64_t min_cost, CephContext *cct, io_queue opqueue) : sdata_lock(lock_name.c_str(), false, true, false, cct), sdata_op_ordering_lock(ordering_lock.c_str(), false, true, false, cct) &#123; if (opqueue == weightedpriority) &#123; pqueue = std::unique_ptr &lt;WeightedPriorityQueue&lt; pair&lt;PGRef, PGQueueable&gt;, entity_inst_t&gt;&gt;( new WeightedPriorityQueue&lt; pair&lt;PGRef, PGQueueable&gt;, entity_inst_t&gt;( max_tok_per_prio, min_cost)); &#125; else if (opqueue == prioritized) &#123; pqueue = std::unique_ptr &lt;PrioritizedQueue&lt; pair&lt;PGRef, PGQueueable&gt;, entity_inst_t&gt;&gt;( new PrioritizedQueue&lt; pair&lt;PGRef, PGQueueable&gt;, entity_inst_t&gt;( max_tok_per_prio, min_cost)); &#125; &#125; &#125;; vector&lt;ShardData*&gt; shard_list; OSD *osd; uint32_t num_shards; // 值为cct-&gt;_conf-&gt;osd_op_num_shards... void _process(uint32_t thread_index, heartbeat_handle_d *hb); void _enqueue(pair &lt;PGRef, PGQueueable&gt; item); void _enqueue_front(pair &lt;PGRef, PGQueueable&gt; item);... &#125; op_shardedwq;...&#125; op_shardedwq对应的thread pool为：osd_op_tp osd_op_tp的初始化在OSD的初始化类中： 12osd_op_tp(cct, "OSD::osd_op_tp", "tp_osd_tp", cct-&gt;_conf-&gt;osd_op_num_threads_per_shard * cct-&gt;_conf-&gt;osd_op_num_shards), 这里相关的配置参数有： osd_op_num_threads_per_shard，默认值为 2 osd_op_num_shards，默认值为 5 PG会根据一定的映射模式映射到不同的shard上，然后由该shard对应的thread处理请求； ShardedOpWQ的处理函数该sharded的work queue的process函数如下： 123456789101112void OSD::ShardedOpWQ::_process(uint32_t thread_index, heartbeat_handle_d *hb ) &#123; pair&lt;PGRef, PGQueueable&gt; item = sdata-&gt;pqueue-&gt;dequeue(); boost::optional&lt;PGQueueable&gt; op; (item.first)-&gt;lock_suspend_timeout(tp_handle); // 获取pg lock op-&gt;run(osd, item.first, tp_handle); // 根据不同类型操作调用不同函数... (item.first)-&gt;unlock(); // 释放pg lock&#125; 从上面可以看出在调用实际的处理函数前，就先获取了PG lock；处理返回后释放PG lock； osd::opshardedwq的_process()函数会根据request的类型，调用不同的函数处理： OSD::dequeue_op() ReplicatedPG::snap_trimmer() PG::scrub() 在文件osd/OSD.cc中有这三类操作的不同处理函数定义： 1234567891011void PGQueueable::RunVis::operator()(OpRequestRef &amp;op) &#123; return osd-&gt;dequeue_op(pg, op, handle);&#125; void PGQueueable::RunVis::operator()(PGSnapTrim &amp;op) &#123; return pg-&gt;snap_trimmer(op.epoch_queued);&#125; void PGQueueable::RunVis::operator()(PGScrub &amp;op) &#123; return pg-&gt;scrub(op.epoch_queued, handle);&#125; OSD操作的处理函数123456789101112131415161718192021222324252627282930313233343536373839/** NOTE: dequeue called in worker thread, with pg lock*/void OSD::dequeue_op( PGRef pg, OpRequestRef op, ThreadPool::TPHandle &amp;handle)&#123;... op-&gt;mark_reached_pg(); pg-&gt;do_request(op, handle); // PG op处理函数...&#125;``` ### snap trim的处理函数```c++void ReplicatedPG::snap_trimmer(epoch_t queued)&#123; if (g_conf-&gt;osd_snap_trim_sleep &gt; 0) &#123; unlock(); // 释放pg lock utime_t t; t.set_from_double(g_conf-&gt;osd_snap_trim_sleep); t.sleep(); // sleep osd_snap_trim_sleep 秒 lock(); // 获取pg lock dout(20) &lt;&lt; __func__ &lt;&lt; " slept for " &lt;&lt; t &lt;&lt; dendl; &#125;... if (is_primary()) &#123;... snap_trimmer_machine.process_event(SnapTrim());... &#125; else if (is_active() &amp;&amp; last_complete_ondisk.epoch &gt; info.history.last_epoch_started) &#123; // replica collection trimming snap_trimmer_machine.process_event(SnapTrim()); &#125; return;&#125; PG scrub的处理函数12345678910111213141516171819202122232425/* Scrub:* PG_STATE_SCRUBBING is set when the scrub is queued** scrub will be chunky if all OSDs in PG support chunky scrub* scrub will fail if OSDs are too old.*/void PG::scrub(epoch_t queued, ThreadPool::TPHandle &amp;handle)&#123; if (g_conf-&gt;osd_scrub_sleep &gt; 0 &amp;&amp; (scrubber.state == PG::Scrubber::NEW_CHUNK || scrubber.state == PG::Scrubber::INACTIVE)) &#123; dout(20) &lt;&lt; __func__ &lt;&lt; " state is INACTIVE|NEW_CHUNK, sleeping" &lt;&lt; dendl; unlock(); // 释放pg lock utime_t t; t.set_from_double(g_conf-&gt;osd_scrub_sleep); handle.suspend_tp_timeout(); t.sleep(); // sleep osd_scrub_sleep 秒 handle.reset_tp_timeout(); lock(); // 获取pg lock dout(20) &lt;&lt; __func__ &lt;&lt; " slept for " &lt;&lt; t &lt;&lt; dendl; &#125;... chunky_scrub(handle);&#125; 分析Ceph PG lock的粒度从函数OSD::ShardedOpWQ::_process()中看出，thread在区分具体的PG请求前就获取了PG lock，在return前释放PG lock；这个PG lock的粒度还是挺大的，若snap trim和scrub占用了PG lock太久，会影响到OSD PG正常的IO操作； OSD PG相关的OP类型有（OSD::dequeue_op()函数处理）： CEPH_MSG_OSD_OP MSG_OSD_SUBOP MSG_OSD_SUBOPREPLY MSG_OSD_PG_BACKFILL MSG_OSD_REP_SCRUB MSG_OSD_PG_UPDATE_LOG_MISSING MSG_OSD_PG_UPDATE_LOG_MISSING_REPLY osd_snap_trim_sleep和osd_scrub_sleep配置从上面看g_conf-&gt;osd_snap_trim_sleep和g_conf-&gt;osd_scrub_sleep配置为非0后，能让snap trim和scrub在每次执行前睡眠一段时间（不是random时间），这样能一定程度上降低这两个操作对PG IO ops的影响（获取PG lock）； 如果设置了osd_snap_trim_sleep或osd_scrub_sleep为非0，处理的线程会sleep，这样虽说释放了PG lock，但是占用了一个PG的一个处理线程，所以才有贴出来的ceph bug - http://tracker.ceph.com/issues/19497 现在我们配置的是： osd_op_num_shards = 30 osd_op_num_threads_per_shard = 2 //默认值 所以一旦某个shard对应的一个thread被占用了，对应处理该shard的只有一个thread了，这样就有可能影响映射到该shard上的PG的正常IO了。 参考资料http://blog.wjin.org/posts/ceph-scrub-mechanism.html]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>osd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Diamond+Graphite+Grafana部署Ceph监控]]></title>
    <url>%2F2017%2F04%2F16%2Fceph-monitor-with-grafana%2F</url>
    <content type="text"><![CDATA[概述Ceph的监控有很多，基于实际需求和调研，我们决定使用Diamond+Graphite+Grafana部署Ceph的监控，它可以很直观的看到Ceph的信息，便于做Ceph系统的性能分析； Diamond 基于python开发的收集系统metrics的工具，包含cpu，disk，network等系统基本信息，也可以定制化开发收集特定的metrics； https://github.com/python-diamond/Diamond Graphite 一个Python编写的企业级开源监控工具，它本身不负责收集数据； http://graphite.readthedocs.io/en/latest/overview.html Grafana 一款采用go 语言编写的开源应用，主要用于大规模指标数据的可视化展现，基于商业友好的Apache License 2.0 开源协议；界面和定制化较Graphite强很多，支持Graphite作为其数据源； https://github.com/grafana/grafana 定制化针对需求，我们可以disable/enable dimaond的一些监控项，也可以写python代码定制化自己的监控项。 我们的修改如下： 编写了Diamond，Graphite，Grafana的部署脚本 定制化了Diamond的Ceph系统监控 配置了cpu，disk，network的监控项 部署部署一个Ceph监控系统，需要两类节点： Diamond运行的节点：收集metrics Graphite+Grafana运行的节点：存储各个Diamond节点收集的metrics，提供可视化指标数据的展示 展示Grafana上可以根据需求定制化模板，我们定制后的Ceph集群监控界面如下图：]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>grafana</tag>
        <tag>diamond</tag>
        <tag>graphite</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis集群方案]]></title>
    <url>%2F2017%2F03%2F10%2Fredis-cluster-trove%2F</url>
    <content type="text"><![CDATA[概述Redis 3.0版本中加入集群的功能，但3.0版本一直在2015年才发布正式版。各大企业在3.0版本还没发布前为了解决Redis的存储瓶颈，纷纷推出了各自的Redis集群方案。 这些方案的核心思想是把数据分片（sharding）存储在多个Redis实例中，每一片就是一个Redis实例。下面介绍Redis的集群方案。 集群方案客户端分片客户端分片是把分片的逻辑放在Redis客户端实现，通过Redis客户端预先定义好的路由规则，把对Key的访问转发到不同的Redis实例中，最后把返回结果汇集。 这种方案的模式如下图所示： 客户端分片的优点： 所有的逻辑都是可控的，不依赖于第三方分布式中间件 开发人员清楚怎么实现分片、路由的规则，不用担心踩坑客户端分片的缺点： 这是一种静态的分片方案，需要增加或者减少Redis实例的数量，需要手工调整分片的程序。 可运维性差，集群的数据出了任何问题都需要运维人员和开发人员一起合作，减缓了解决问题的速度，增加了跨部门沟通的成本。 在不同的客户端程序中，维护相同的分片逻辑成本巨大。 TwemproxyTwemproxy是由Twitter开源的Redis代理，其基本原理是：Redis客户端把请求发送到Twemproxy，Twemproxy根据路由规则发送到正确的Redis实例，最后Twemproxy把结果汇集返回给客户端。 Twemproxy通过引入一个代理层，将多个Redis实例进行统一管理，使Redis客户端只需要在Twemproxy上进行操作，而不需要关心后面有多少个Redis实例，从而实现了Redis集群。 Twemproxy集群架构如下图所示： Twemproxy的优点： 客户端像连接Redis实例一样连接Twemproxy，不需要改任何的代码逻辑 支持无效Redis实例的自动删除 Twemproxy与Redis实例保持连接，减少了客户端与Redis实例的连接数Twemproxy的缺点： 由于Redis客户端的每个请求都经过Twemproxy代理才能到达Redis服务器，这个过程中会产生性能损失 没有友好的监控管理后台界面，不利于运维监控 最大的问题是Twemproxy无法平滑地增加Redis实例，对于运维人员来说，当因为业务需要增加Redis实例时工作量非常大Twemproxy作为最被广泛使用、最久经考验、稳定性最高的Redis代理，在业界被广泛使用。 CodisTwemproxy不能平滑增加Redis实例的问题带来了很大的不便，于是豌豆荚自主研发了Codis，一个支持平滑增加Redis实例的Redis代理软件，其基于Go和C语言开发，并于2014年11月在GitHub上开源。 Codis包含下面4个部分： Codis Proxy： Redis客户端连接到Redis实例的代理，实现了Redis的协议，Redis客户端连接到Codis Proxy进行各种操作。Codis Proxy是无状态的，可以用Keepalived等负载均衡软件部署多个Codis Proxy实现高可用。 Codis Redis： Codis项目维护的Redis分支，添加了slot和原子的数据迁移命令。Codis上层的 Codis Proxy和Codisconfig只有与这个版本的Redis通信才能正常运行。 Codisconfig： Codis管理工具。可以执行添加删除CodisRedis节点、添加删除Codis Proxy、数据迁移等操作。另外，Codisconfig自带了HTTP server，里面集成了一个管理界面，方便运维人员观察Codis集群的状态和进行相关的操作，极大提高了运维的方便性，弥补了Twemproxy的缺点。 ZooKeeper： 分布式的、开源的应用程序协调服务，是Hadoop和Hbase的重要组件，其为分布式应用提供一致性服务，提供的功能包括：配置维护、名字服务、分布式同步、组服务等。Codis依赖于ZooKeeper存储数据路由表的信息和Codis Proxy节点的元信息。另外，Codisconfig发起的命令都会通过ZooKeeper同步到Codis Proxy的节点。 Codis的架构如下图所示： Redis ClusterRedis Cluster在Redis 3.0开始支持，采用无中心的方式，为了维护集群状态统一，节点之间需要互相交换消息，Redis采用交换消息的方式被称为 Gossip ，基本思想是节点之间互相交换信息最终所有节点达到一致，更多关于 Gossip 可参考 https://en.wikipedia.org/wiki/Gossip_protocol Redis 集群是一个提供在多个Redis间节点间共享数据的程序集。 Redis 集群并不支持处理多个keys的命令，因为这需要在不同的节点间移动数据，从而达不到像Redis那样的性能，在高负载的情况下可能会导致不可预料的错误. Redis 集群特点 自动分割数据到不同的节点上。 整个集群的部分节点失败或者不可达的情况下能够继续处理命令。 可线性扩展到上千个节点 可使数据自动路由到多个节点 实现了多个节点间的数据共享 可支持动态增加或删除节点 可保证某些节点无法提供服务时不影响整个集群的操作 不保证数据的强一致性 支持Redis所有处理单个数据库键的命令 不支持对多个数据库键的操作，比如MSET、SUNION 不能使用 SELECT 命令，集群只使用默认的0号数据库 Redis 集群数据共享Redis 集群使用数据分片（sharding）而非一致性哈希（consistency hashing）来实现： 一个 Redis 集群包含 16384 个哈希槽（hash slot）， 数据库中的每个键都属于这 16384 个哈希槽的其中一个， 集群使用公式 CRC16(key) % 16384 来计算键 key 属于哪个槽， 其中 CRC16(key) 语句用于计算键 key 的 CRC16 校验和 。 集群中的每个节点负责处理一部分哈希槽。 举个例子， 一个集群可以有三个节点， 其中： 节点 A 负责处理 0 号至 5500 号哈希槽。 节点 B 负责处理 5501 号至 11000 号哈希槽。 节点 C 负责处理 11001 号至 16384 号哈希槽。 这种将哈希槽分布到不同节点的做法使得用户可以很容易地向集群中添加或者删除节点。 比如说： 如果用户将新节点 D 添加到集群中， 那么集群只需要将节点 A 、B 、 C 中的某些槽移动到节点 D 就可以了。 与此类似， 如果用户要从集群中移除节点 A ， 那么集群只需要将节点 A 中的所有哈希槽移动到节点 B 和节点 C ， 然后再移除空白（不包含任何哈希槽）的节点 A 就可以了。因为将一个哈希槽从一个节点移动到另一个节点不会造成节点阻塞， 所以无论是添加新节点还是移除已存在节点， 又或者改变某个节点包含的哈希槽数量， 都不会造成集群下线。 Redis 集群中的主从复制为了使得集群在一部分节点下线或者无法与集群的大多数（majority）节点进行通讯的情况下， 仍然可以正常运作， Redis 集群对节点使用了主从复制功能： 集群中的每个节点都有 1 个至 N 个复制品（replica）， 其中一个复制品为主节点（master）， 而其余的 N-1 个复制品为从节点（slave）。 在之前列举的节点 A 、B 、C 的例子中， 如果节点 B 下线了， 那么集群将无法正常运行， 因为集群找不到节点来处理 5501 号至 11000号的哈希槽。 另一方面， 假如在创建集群的时候（或者至少在节点 B 下线之前）， 我们为主节点 B 添加了从节点 B1 ， 那么当主节点 B 下线的时候， 集群就会将 B1 设置为新的主节点， 并让它代替下线的主节点 B ， 继续处理 5501 号至 11000 号的哈希槽， 这样集群就不会因为主节点 B 的下线而无法正常运作了。 不过如果节点 B 和 B1 都下线的话， Redis 集群还是会停止运作。 Redis 集群的一致性保证（guarantee）Redis 集群不保证数据的强一致性（strong consistency）： 在特定条件下， Redis 集群可能会丢失已经被执行过的写命令。 使用异步复制（asynchronous replication）是 Redis 集群可能会丢失写命令的其中一个原因。 考虑以下这个写命令的例子： 客户端向主节点 B 发送一条写命令。 主节点 B 执行写命令，并向客户端返回命令回复。 主节点 B 将刚刚执行的写命令复制给它的从节点 B1 、 B2 和 B3 。 如你所见， 主节点对命令的复制工作发生在返回命令回复之后， 因为如果每次处理命令请求都需要等待复制操作完成的话， 那么主节点处理命令请求的速度将极大地降低 —— 我们必须在性能和一致性之间做出权衡。 如果真的有必要的话， Redis 集群可能会在将来提供同步地（synchronou）执行写命令的方法。 Redis 集群另外一种可能会丢失命令的情况是， 集群出现网络分裂（network partition）， 并且一个客户端与至少包括一个主节点在内的少数（minority）实例被孤立。 举个例子， 假设集群包含 A 、 B 、 C 、 A1 、 B1 、 C1 六个节点， 其中 A 、B 、C 为主节点， 而 A1 、B1 、C1 分别为三个主节点的从节点， 另外还有一个客户端 Z1 。 假设集群中发生网络分裂， 那么集群可能会分裂为两方， 大多数（majority）的一方包含节点 A 、C 、A1 、B1 和 C1 ， 而少数（minority）的一方则包含节点 B 和客户端 Z1 。 在网络分裂期间， 主节点 B 仍然会接受 Z1 发送的写命令： 如果网络分裂出现的时间很短， 那么集群会继续正常运行； 但是， 如果网络分裂出现的时间足够长， 使得大多数一方将从节点 B1 设置为新的主节点， 并使用 B1 来代替原来的主节点 B ， 那么 Z1 发送给主节点 B 的写命令将丢失。注意， 在网络分裂出现期间， 客户端 Z1 可以向主节点 B 发送写命令的最大时间是有限制的， 这一时间限制称为节点超时时间（node timeout）， 是 Redis 集群的一个重要的配置选项： 对于大多数一方来说， 如果一个主节点未能在节点超时时间所设定的时限内重新联系上集群， 那么集群会将这个主节点视为下线， 并使用从节点来代替这个主节点继续工作。 对于少数一方， 如果一个主节点未能在节点超时时间所设定的时限内重新联系上集群， 那么它将停止处理写命令， 并向客户端报告错误。 Redis 集群的工作流程Redis把所有的Key分成了16384个slot，每个Redis实例负责其中一部分slot。集群中的所有信息（节点、端口、slot等），都通过节点之间定期的数据交换而更新。 Redis客户端在任意一个Redis实例发出请求，如果所需数据不在该实例中，通过重定向命令引导客户端访问所需的实例。 如上图所示，Redis集群内的机器定期交换数据，工作流程如下： Redis客户端在Redis2实例上访问某个数据。 在Redis2内发现这个数据是在Redis1这个实例中，给Redis客户端发送一个重定向的命令。 Redis客户端收到重定向命令后，访问Redis1实例获取所需的数据。 Trove支持的Redis集群Trove在Liberty里引入Redis Cluster支持，用户可以创建一个Redis Cluster，但每个Redis实例都不是高可用的。 通过Trove，可以创建Redis集群的方案： 1，主从Redis实例可以通过Keepalived实现高可用方案，如下图所示： 2，Redis ClusterRedis 3.0后的版本支持，通过Trove可以创建Redis Cluster，添加Redis Nodes，删除Redis Nodes； 但每个Redis节点没有高可用，任何一个Redis节点挂掉，Redis Cluster就会有数据丢失； Redis Cluster如下图所示： 3，Redis Cluster主从模式Redis Cluster 为了保证数据的高可用性，加入了主从模式，一个主节点对应一个或多个从节点，主节点提供数据存取，从节点则是从主节点拉取数据备份，当这个主节点挂掉后，就会有这个从节点选取一个来充当主节点，从而保证集群不会挂掉。 Trove里不支持该模式，需要修改Trove代码实现 主从Redis节点的灾备模式如下图所示： 参考资料https://www.zybuluo.com/phper/note/195558http://redisdoc.com/topic/cluster-tutorial.htmlhttp://redisdoc.com/topic/cluster-spec.htmlhttp://redis.io/topics/cluster-tutorial]]></content>
      <categories>
        <category>trove</category>
      </categories>
      <tags>
        <tag>trove</tag>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis服务调研]]></title>
    <url>%2F2017%2F03%2F01%2Fredis-service-invest%2F</url>
    <content type="text"><![CDATA[Redis持久化Redis支持两种数据持久化方式：RDB方式和AOF方式。前者会根据配置的规则定时将内存中的数据持久化到硬盘上，后者则是在每次执行写命令之后将命令记录下来。两种持久化方式可以单独使用，但是通常会将两者结合使用。 RDB方式RDB方式的持久化是通过快照的方式完成的。当符合某种规则时，会将内存中的数据全量生成一份副本存储到硬盘上，这个过程称作”快照”，Redis会在以下几种情况下对数据进行快照： 根据配置规则进行自动快照 用户执行SAVE, BGSAVE命令 执行FLUSHALL命令 执行复制（replication）时 Redis允许用户自定义快照条件，当满足条件时自动执行快照，快照规则的配置方式如下： 123save 900 1save 300 10save 60 10000 每个快照条件独占一行，他们之间是或（||）关系，只要满足任何一个就进行快照。上面配置save后的第一个参数T是时间，单位是秒，第二个参数M是更改的键的个数，含义是：当时间T内被更改的键的个数大于M时，自动进行快照。比如save 900 1的含义是15分钟内(900s)被更改的键的个数大于1时，自动进行快照操作。 执行SAVE或BGSAVE命令除了让Redis自动进行快照外，当我们需要重启，迁移，备份Redis时，我们也可以手动执行SAVE或BGSAVE命令主动进行快照操作。 SAVE命令 当执行SAVE命令时，Redis同步进行快照操作，期间会阻塞所有来自客户端的请求，所以放数据库数据较多时，应该避免使用该命令。 BGSAVE命令 从命令名字就能看出来，这个命令与SAVE命令的区别就在于该命令的快照操作是在后台异步进行的，进行快照操作的同时还能处理来自客户端的请求。执行BGSAVE命令后Redis会马上返回OK表示开始进行快照操作，如果想知道快照操作是否已经完成，可以使用LASTSAVE命令返回最近一次成功执行快照的时间，返回结果是一个Unix时间戳。 快照原理Redis默认会将快照文件存储在Redis当前进程的工作目录的dump.rdb文件中，可以通过配置文件中的dir和dbfilename两个参数分别指定快照文件的存储路径和文件名，例如： 12dbfilename dump.rdbdir /opt/soft/redis-3.0.4/cache 快照执行的过程如下： Redis使用fork函数复制一份当前进程（父进程）的副本（子进程）； 父进程继续处理来自客户端的请求，子进程开始将内存中的数据写入硬盘中的临时文件； 当子进程写完所有的数据后，用该临时文件替换旧的RDB文件，至此，一次快照操作完成。 AOF方式在使用Redis存储非临时数据时，一般都需要打开AOF持久化来降低进程终止导致的数据丢失，AOF可以将Redis执行的每一条写命令追加到硬盘文件中，这一过程显然会降低Redis的性能，但是大部分情况下这个影响是可以接受的，另外，使用较快的硬盘能提高AOF的性能。 开启AOF默认情况下，Redis没有开启AOF（append only file）持久化功能，可以通过在配置文件中作如下配置启用： 1appendonly yes 开启之后，Redis每执行一条写命令就会将该命令写入硬盘中的AOF文件。AOF文件保存路径和RDB文件路径是一致的，都是通过dir参数配置，默认文件名是：appendonly.aof，可以通过配置appendonlyfilename参数修改，例如： 1appendonlyfilename appendonly.aof 同步硬盘数据虽然每次执行更改数据库的内容时，AOF都会记录执行的命令，但是由于操作系统本身的硬盘缓存的缘故，AOF文件的内容并没有真正地写入硬盘，在默认情况下，操作系统会每隔30s将硬盘缓存中的数据同步到硬盘，但是为了防止系统异常退出而导致丢数据的情况发生，我们还可以在Redis的配置文件中配置这个同步的频率： 123# appendfsync alwaysappendfsync everysec# appendfsync no 第一行表示每次AOF写入一个命令都会执行同步操作，这是最安全也是最慢的方式；第二行表示每秒钟进行一次同步操作，一般来说使用这种方式已经足够；第三行表示不主动进行同步操作，这是最不安全的方式。 Redis Info信息INFO命令以一种易于理解和阅读的格式，返回关于Redis服务器的各种信息和统计数值。 通过给定可选的参数 section ，可以让命令只返回某一部分的信息: server: Redis服务器的一般信息 clients: 客户端的连接部分 memory: 内存消耗相关信息 persistence: RDB和AOF相关信息 stats: 一般统计 replication: 主/从复制信息 cpu: 统计CPU的消耗 commandstats: Redis命令统计 cluster: Redis集群信息 keyspace: 数据库的相关统计 它也可以采取以下值: all: 返回所有信息 default: 返回默认设置的信息如果没有使用任何参数时，默认为default。 Redis的configuration group可以配置Redis支持的配置参数，参考文件：trove/trove/templates/redis/validation-rules.json 里面可配置的参数很多，我们可以选择部分支持的参数提供给用户配置。 阿里提供的配置参数有： 参数 值 maxmemory-policy volatile-lru hash-max-ziplist-entries 512 hash-max-ziplist-value 64 list-max-ziplist-entries 512 list-max-ziplist-value 64 set-max-intset-entries 512 zset-max-ziplist-entries 128 zset-max-ziplist-value 64 notify-keyspace-events 注释：上表中红色的部分是我们提供的redis 3.2.5版本中没有的，在新版本中的配置参数为：list-max-ziplist-size 青云支持的Redis配置参数更多： 配置redis, 3.2.5版本的config parameters： 1# trove-manage --config-file /opt/etc/trove/trove.conf db_load_datastore_config_parameters redis 3.2.5 /opt/openstack/trove/trove/templates/redis/validation-rules.json 创建Redis对应的configuration group： 1# trove configuration-create redisCGroup '&#123;"requirepass": "mypass", "hll-sparse-max-bytes": 200&#125;' --datastore redis --datastore_version 3.2.5 Attach configuration group到实例： 1# trove configuration-attach &lt;instance&gt; &lt;configuration&gt; Detach configuration group到实例： 1# trove configuration-detach &lt;instance&gt; Redis的备份和恢复Redis的备份查看Trove代码，Redis备份的是RDB的dbfile，执行备份的步骤如下： 保存当前数据到dbfile：执行 redis-cli BGSAVE命令 等待BGSAVE完成，通过redis-cli LASTSAVE获取dbfile的最新保存时间，变化的话即代表第1步BGSAVE完成 获取dbfile文件，通过cat | zip | encrypt，把文件上传到对象存储S3中 Redis的恢复Reids是通过之前备份的dbfile恢复数据的，执行恢复的步骤如下： 若redis配置文件中aof为开启状态，则关闭aof：overrides = {‘appendonly’: ‘no’} 停止redis服务 删除当前的dbfile 根据当前的配置创建dbfile的目录（按实际需要） 从对象存储S3中拉取备份文件后，通过 decrypt | unzip到配置的dbfile 启动redis服务 若redis之前的配置中aof为关闭状态，则恢复完成，否则执行后续步骤； 通过redis-cli info persistence 获取redis的恢复状态：loading 0 重写aof文件：redis-cli BGREWRITEAOF 通过redis-cli info persistence 获取aof重写状态：aof_rewrite_in_progress 0 修改redis的配置文件，开启aof：overrides = {‘appendonly’: ‘yes’} 重启redis服务]]></content>
      <categories>
        <category>trove</category>
      </categories>
      <tags>
        <tag>trove</tag>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph块存储跨机房灾备调研]]></title>
    <url>%2F2017%2F02%2F22%2Frbd-data-replication%2F</url>
    <content type="text"><![CDATA[概述随着提供云服务的机房的增多，为了提供更高的可靠性，满足高可用客户的需求，跨机房灾备必须实现，本文档重点调研Ceph RBD的跨机房容灾方案。 当前状况当前应用中，每个机房的Ceph都是独立的cluster，彼此之间没有任何关系。 比如我们在机房A和机房B上部署了两套Openstack+Ceph的环境，底层的Ceph就是ClusterA和ClusterB。每个Cluster独立提供自己的块存储RBD服务。 方案方案1 - Snapshot* Ceph版本要求当前我们使用的Hammer 0.94.5版本即可 * 原理异步备份，基于RBD的snapshot机制 * 介绍Cluster A &amp; B仍然是独立的Ceph集群，通过RBD的snapshot机制，在Cluster A端，针对image定期通过rbd创建image的snap，然后通过rbd export-diff, rbd import-diff命令来完成image备份到Cluster B。 * 命令和步骤把 Cluster A 的 pool rbd 下面 image testimage 异步备份到 Cluster B 的 pool rbd 下的相同image上； 在Cluster A/B上创建rbd/testimagerbd create -p rbd --size 10240 testimage 在准备备份image前，暂停Cluster A端对testimage的IO操作，然后创建个snapshotrbd snap create &lt;snap-name&gt; 导出Cluster A端的testimage数据，不指定from-snaprbd export-diff &lt;image-name&gt; &lt;path&gt; copy上一步中导出的文件到Cluster B，并导入数据到testimagerbd import-diff &lt;path&gt; &lt;image-name&gt; 后续需周期性的暂停Cluster A端的testimage的IO，然后创建snapshot，通过 rbd export-diff &lt;image-name&gt; [--from-snap &lt;snap-name&gt;] &lt;path&gt;命令导出incremental diff，之后把差异数据文件copy到Cluster B上，然后通过命令rbd import-diff &lt;path&gt; &lt;image-name&gt;导入。 【注】：也可不暂停Cluster A端的IO，直接take snapshot；这样并不会引起image的数据不一致，只是有可能会使rbd export-diff时导出的数据在take snapshot之后 * 优缺点优点： 当前Ceph版本就支持rbd snapshot的功能 命令简介方便，通过定制执行脚本就能实现rbd块设备的跨区备份 缺点： 每次同步前都需要在源端take snapshot 持续的snapshots可能导致image的读写性能下降 还要考虑后续删除不用的snapshots snapshot只能保证IO的一致性，并不能保证使用rbd块设备上的系统一致性； 【可以每次暂停image的IO，sync IO数据来保证rbd块设备上的系统一致性，但需要虚拟机支持qemu-guest-agent】 * 参考资料https://ceph.com/dev-notes/incremental-snapshots-with-rbd/https://www.rapide.nl/blog/item/ceph_-_rbd_replication.htmlhttp://wiki.libvirt.org/page/Qemu_guest_agent 方案2 - One Ceph Cluster* Ceph版本要求当前我们使用的Hammer 0.94.5版本即可 * 原理同步备份，跨机房的Ceph集群，底层存储的跨机房容灾 * 介绍把两个机房的物理机搭建为一个Ceph Cluster，通过Ceph本身的写多份容灾 * 优缺点优点： 一个Ceph集群，能保证实时的跨机房容灾 不需要额外开发 缺点： 部署相对麻烦 跨机房的时延较大，影响Ceph集群性能 * 步骤和参考资料http://www.sebastien-han.fr/blog/2013/01/28/ceph-geo-replication-sort-of/ 方案3 - RBD Mirroring* Ceph版本要求Jewel版本，最好是最新的10.2.2 * 原理异步备份，Ceph新的rbd mirror功能 * 介绍Ceph新的rbd mirror功能支持配置两个Ceph Cluster之间的rbd同步 * 优缺点优点： Ceph新的功能，不需要额外开发 同步的粒度比较小，为一个块设备的transaction 保证了Crash consistency 可配置pool的备份，也可单独指定image备份 缺点： 需要升级线上Ceph到Jewel 10.2.2版本 不确定升级的影响和工作量 * 步骤和参考资料http://docs.ceph.com/docs/jewel/rbd/rbd-mirroring/https://www.sebastien-han.fr/blog/2016/03/28/ceph-jewel-preview-ceph-rbd-mirroring/http://www.sebastien-han.fr/blog/2016/04/27/OpenStack-Summit-Austin-protecting-the-galaxy-Multi-Region-Disaster-Recovery-with-OpenStack-and-Ceph/http://www.spinics.net/lists/ceph-devel/msg24169.htmlhttps://www.zybuluo.com/zphj1987/note/328708 推荐方案综合上面的描述和优缺点分析，方案3 - RBD Mirroring 会是一个比较好的选择。 这样我们不仅能跟随Ceph社区的节奏，也能很好的解决我们跨机房同步的需求，后续RBD Mirroring功能的升级，我们也能应用上。]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>rbd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对象存储OSS功能测试]]></title>
    <url>%2F2016%2F12%2F15%2Frgw-functions-tst%2F</url>
    <content type="text"><![CDATA[概述本文针对对象存储OSS的基本功能，提供能测试的方法和脚本一些基于boto3API的简单操作示例：boto3API.py 1234567891011121314151617181920212223242526272829303132333435363738394041424344#!/usr/bin/env python#-*- coding:utf-8 -*-import boto3# creating a clients3client = boto3.client('s3', aws_secret_access_key = 'ZSU2I***8aNgGyMHBFhqwWnRzKz1fO', aws_access_key_id = 'C6Y1E0C3EB***W8YIKEW', endpoint_url = 'http://10.1.0.29')# creating a bucketbucket_name = 'ictfox'response = s3client.create_bucket(Bucket = bucket_name)print "Creating bucket &#123;0&#125; returns =&gt; &#123;1&#125;\n".format(bucket_name, response)# listing owned bucketsresponse = s3client.list_buckets()for bucket in response['Buckets']: print "Listing owned buckets returns =&gt; &#123;0&#125; was created on &#123;1&#125;\n".format(bucket['Name'], bucket['CreationDate'])# creating an objectobject_key = 'hello.txt'response = s3client.put_object(Bucket = bucket_name, Key = object_key, Body = 'Hello World!')print "Creating object &#123;0&#125; returns =&gt; &#123;1&#125;\n".format(object_key, response)# Listing a bucket's contentresponse = s3client.list_objects(Bucket = bucket_name)for obj in response['Contents']: print "Listing a bucket's content returns =&gt; &#123;0&#125;\t&#123;1&#125;\t&#123;2&#125;\n".format(obj['Key'], obj['Size'], obj['LastModified'])# Changing an object's metadata(head object)metadata = &#123;'x-amz-meta-datastore': 'qr', 'x-amz-meta-datastore-version': '1.0.1'&#125;copySrc = '&#123;0&#125;/&#123;1&#125;'.format(bucket_name, object_key)response = s3client.copy_object(Bucket = bucket_name, CopySource = copySrc, Key = object_key, Metadata = metadata, MetadataDirective = 'REPLACE')print "Changing metadata of object &#123;0&#125; returns =&gt; &#123;1&#125;\n".format(object_key, response)# Deleting an objectresponse = s3client.delete_object(Bucket = bucket_name, Key = object_key)print "Deleting object &#123;0&#125; returns =&gt; &#123;1&#125;\n".format(object_key, response)# deleting a bucketresponse = s3client.delete_bucket(Bucket = bucket_name)print "Deleting bucket &#123;0&#125; returns =&gt; &#123;1&#125;\n".format(bucket_name, response) 用户管理用户的管理是通过Admin Ops API提供；这里我们通过之前创建好的管理账户来执行Admin Ops API，具体可以参考：http://docs.ceph.com/docs/master/radosgw/adminops/ 创建用户12345678910111213$ cat usercreate.sh#!/bin/bashtoken=5L65QDE4df8JJ8RM7MN5 ## USER_TOKENsecret=Y9HPiBCwLDeSMSaiQhmPT2h7NgNUndvLERhktnIZ ## USER_SECRETquery=$1name=$2echo $query, $namequery3="&amp;uid="query2=admin/userdate=$(for i in $(date "+%H") ; do date "+%a, %d %b %Y $(( 10#$i-8 )):%M:%S +0000" ; done)header="PUT\n\n\n$&#123;date&#125;\n/$&#123;query2&#125;"sig=$(echo -en $&#123;header&#125; | openssl sha1 -hmac $&#123;secret&#125; -binary | base64)curl -v -H "Date: $&#123;date&#125;" -H "Authorization: AWS $&#123;token&#125;:$&#123;sig&#125;" -L -X PUT "http://&lt;your-host-ip&gt;/$&#123;query2&#125;?format=json$&#123;query3&#125;$&#123;query&#125;&amp;display-name=$&#123;name&#125;" -H "Host: &lt;your-host-ip&gt;" 删除用户1234567891011$ cat userdelete.sh#!/bin/bashtoken=5L65QDE4238JJ8RM7MN5 ## USER_TOKENsecret=Y9HPiBCwLDeSMSaiQhmPT2h7NgNUnqVLERhktnIZ ## USER_SECRETquery=$1query3="&amp;uid="query2=admin/userdate=$(for i in $(date "+%H") ; do date "+%a, %d %b %Y $(( 10#$i-8 )):%M:%S +0000" ; done)header="DELETE\n\n\n$&#123;date&#125;\n/$&#123;query2&#125;"sig=$(echo -en $&#123;header&#125; | openssl sha1 -hmac $&#123;secret&#125; -binary | base64)curl -v -H "Date: $&#123;date&#125;" -H "Authorization: AWS $&#123;token&#125;:$&#123;sig&#125;" -L -X DELETE "http://&lt;your-host-ip&gt;/$&#123;query2&#125;?format=json$&#123;query3&#125;$&#123;query&#125;" -H "Host: &lt;your-host-ip&gt;" User keys管理User的keys也是通过admin Ops API操作的，同样需要通过管理账户来执行； 创建user key1234567891011121314$cat createKey.sh#!/bin/bashtoken=5L65QDE4238JJ8RM7MN5 ## USER_TOKENsecret=Y9HPiBCwLDeSMSaiQhmPT2h7NgNUnqVLERhktnIZ ## USER_SECREToperate="PUT"user=$1query="admin/user"date=$(for i in $(date "+%H") ; do date "+%a, %d %b %Y $(( 10#$i-8 )):%M:%S +0000" ; done)header="$&#123;operate&#125;\n\n\n$&#123;date&#125;\n/$&#123;query&#125;"sig=$(echo -en $&#123;header&#125; | openssl sha1 -hmac $&#123;secret&#125; -binary | base64)curl -v -L -X $&#123;operate&#125; "http://&lt;your-host-ip&gt;/$&#123;query&#125;?key&amp;format=json&amp;uid=$&#123;user&#125;&amp;generate-key=True" \ -H "Date: $&#123;date&#125;" \ -H "Authorization: AWS $&#123;token&#125;:$&#123;sig&#125;" \ -H "Host: &lt;your-host-ip&gt;" 删除user key1234567891011121314cat removeKey.sh#!/bin/bashtoken=5L65QDE4238JJ8RM7MN5 ## USER_TOKENsecret=Y9HPiBCwLDeSMSaiQhmPT2h7NgNUnqVLERhktnIZ ## USER_SECREToperate="DELETE"key=$1query="admin/user"date=$(for i in $(date "+%H") ; do date "+%a, %d %b %Y $(( 10#$i-8 )):%M:%S +0000" ; done)header="$&#123;operate&#125;\n\n\n$&#123;date&#125;\n/$&#123;query&#125;"sig=$(echo -en $&#123;header&#125; | openssl sha1 -hmac $&#123;secret&#125; -binary | base64)curl -v -L -X $&#123;operate&#125; "http://&lt;your-host-ip&gt;/$&#123;query&#125;?key&amp;format=json&amp;access-key=$&#123;key&#125;" \ -H "Date: $&#123;date&#125;" \ -H "Authorization: AWS $&#123;token&#125;:$&#123;sig&#125;" \ -H "Host: &lt;your-host-ip&gt;" 列出user keys1234567891011121314cat getUserInfo.sh#!/bin/bashtoken=5L65QDE4238JJ8RM7MN5 ## USER_TOKENsecret=Y9HPiBCwLDeSMSaiQhmPT2h7NgNUnqVLERhktnIZ ## USER_SECRETuser=$1query=admin/userdate=$(for i in $(date "+%H") ; do date "+%a, %d %b %Y $(( 10#$i-8 )):%M:%S +0000" ; done)header="GET\n\n\n$&#123;date&#125;\n/$&#123;query&#125;"sig=$(echo -en $&#123;header&#125; | openssl sha1 -hmac $&#123;secret&#125; -binary | base64)echo $sigcurl -v -L -X GET "http://&lt;your-host-ip&gt;/$&#123;query&#125;?format=json&amp;uid=$&#123;user&#125;" \ -H "Date: $&#123;date&#125;" \ -H "Authorization: AWS $&#123;token&#125;:$&#123;sig&#125;" \ -H "Host: &lt;your-host-ip&gt;" 权限管理开源的比较常用的s3的SDK有boto3，我们在RDS的备份中已经使用过参考：http://boto3.readthedocs.io/en/latest/reference/services/s3.html#s3 下面的操作都简要列出了调用的api和说明； 获取Bucket权限get_bucket_acl(**kwargs) Gets the access control policy for the bucket. 设置Bucket权限put_bucket_acl(**kwargs) Sets the permissions on a bucket using access control lists (ACL). Bucket操作同权限管理，下面的操作基于boto3，都简要列出了调用的api和说明； 列出Bucketslist_buckets() Returns a list of all buckets owned by the authenticated sender of the request. 创建Bucketcreate_bucket(**kwargs) Creates a new bucket. 删除Bucketdelete_bucket(**kwargs) Deletes the bucket. All objects (including all object versions and Delete Markers) in the bucket must be deleted before the bucket itself can be deleted. 列出Bucket Objects list_objects(**kwargs) Returns some or all (up to 1000) of the objects in a bucket. You can use the request parameters as selection criteria to return a subset of the objects in a bucket. list_objects_v2(**kwargs) Returns some or all (up to 1000) of the objects in a bucket. You can use the request parameters as selection criteria to return a subset of the objects in a bucket. Note: ListObjectsV2 is the revised List Objects API and we recommend you use this revised API for new application development. 获取Bucket ACLget_bucket_acl(**kwargs) Gets the access control policy for the bucket. 设置Bucket ACLput_bucket_acl(**kwargs) Sets the permissions on a bucket using access control lists (ACL). 获取Bucket Infohead_bucket(**kwargs) This operation is useful to determine if a bucket exists and you have permission to access it. 枚举Bucket分块上传list_multipart_uploads(**kwargs) This operation lists in-progress multipart uploads. Object操作上传Object put_object(**kwargs) Adds an object to a bucket. upload_file(Filename, Bucket, Key, ExtraArgs=None, Callback=None, Config=None) Upload a file to an S3 object. upload_fileobj(Fileobj, Bucket, Key, ExtraArgs=None, Callback=None, Config=None) Upload a file-like object to S3. The file-like object must be in binary mode. This is a managed transfer which will perform a multipart upload in multiple threads if necessary. 复制Objectcopy_object(**kwargs) Creates a copy of an object that is already stored in Amazon S3. 删除Object delete_object(**kwargs) Removes the null version (if there is one) of an object and inserts a delete marker, which becomes the latest version of the object. If there isn’t a null version, Amazon S3 does not remove any objects. delete_objects(**kwargs) This operation enables you to delete multiple objects from a bucket using a single HTTP request. You may specify up to 1000 keys. 下载Object get_object(**kwargs) Retrieves objects from Amazon S3. download_file(Bucket, Key, Filename, ExtraArgs=None, Callback=None, Config=None) Download an S3 object to a file. download_fileobj(Bucket, Key, Fileobj, ExtraArgs=None, Callback=None, Config=None) Download an object from S3 to a file-like object. The file-like object must be in binary mode. This is a managed transfer which will perform a multipart download in multiple threads if necessary. 获取Object ACLget_object_acl(**kwargs) Returns the access control list (ACL) of an object. 设置Object ACLput_object_acl(**kwargs) uses the acl subresource to set the access control list (ACL) permissions for an object that already exists in a bucket 获取Object Infohead_object(**kwargs) The HEAD operation retrieves metadata from an object without returning the object itself. This operation is useful if you’re only interested in an object’s metadata. To use HEAD, you must have READ access to the object. 支持Object Multipart create_multipart_upload(**kwargs) Initiates a multipart upload and returns an upload ID. complete_multipart_upload(**kwargs) Completes a multipart upload by assembling previously uploaded parts. abort_multipart_upload(**kwargs) Aborts a multipart upload. To verify that all parts have been removed, so you don’t get charged for the part storage, you should call the List Parts operation and ensure the parts list is empty. upload_part(**kwargs) Uploads a part in a multipart upload. 其他操作获取RGW User统计信息1234567891011121314$ cat getUsage.sh#!/bin/bashtoken=5L65QDE4238JJ8RM7MN5 ## USER_TOKENsecret=Y9HPiBCwLDeSMSaiQhmPT2h7NgNUnqVLERhktnIZ ## USER_SECREToperate="GET"user=$1query="admin/usage"date=$(for i in $(date "+%H") ; do date "+%a, %d %b %Y $(( 10#$i-8 )):%M:%S +0000" ; done)header="$&#123;operate&#125;\n\n\n$&#123;date&#125;\n/$&#123;query&#125;"sig=$(echo -en $&#123;header&#125; | openssl sha1 -hmac $&#123;secret&#125; -binary | base64)curl -v -L -X $&#123;operate&#125; "http://&lt;your-host-ip&gt;/$&#123;query&#125;?format=json&amp;uid=$&#123;user&#125;" \ -H "Date: $&#123;date&#125;" \ -H "Authorization: AWS $&#123;token&#125;:$&#123;sig&#125;" \ -H "Host: &lt;your-host-ip&gt;" 删除RGW User统计信息12345678910111213141516$ cat trimeUsage.sh#!/bin/bashtoken=5L65QDE4238JJ8RM7MN5 ## USER_TOKENsecret=Y9HPiBCwLDeSMSaiQhmPT2h7NgNUnqVLERhktnIZ ## USER_SECREToperate="DELETE"user=$1stime=$2 #start timeetime=$3 #end timequery="admin/usage"date=$(for i in $(date "+%H") ; do date "+%a, %d %b %Y $(( 10#$i-8 )):%M:%S +0000" ; done)header="$&#123;operate&#125;\n\n\n$&#123;date&#125;\n/$&#123;query&#125;"sig=$(echo -en $&#123;header&#125; | openssl sha1 -hmac $&#123;secret&#125; -binary | base64)curl -v -L -X $&#123;operate&#125; "http://&lt;your-host-ip&gt;/$&#123;query&#125;?format=json&amp;uid=$&#123;user&#125;&amp;start=$&#123;stime&#125;&amp;end=$&#123;etime&#125;" \ -H "Date: $&#123;date&#125;" \ -H "Authorization: AWS $&#123;token&#125;:$&#123;sig&#125;" \ -H "Host: &lt;your-host-ip&gt;"]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>radosgw</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RadosGW对象存储使用文档]]></title>
    <url>%2F2016%2F12%2F12%2Frgw-oss%2F</url>
    <content type="text"><![CDATA[概述OSS: Object Storage ServiceRadosGW兼容S3，我们需要依S3的方式提供OSS OSS功能列表Bucket相关 Buckets功能 REST API Ceph Operation &amp; Class List Buckets GET / HTTP/1.1Host: cname.company.cnAuthorization: AWS {access-key}:{hash-of-header-and-secret} RGW_OP_LIST_BUCKETSclass RGWListBuckets_ObjStore_S3 Put Bucket PUT /{bucket} HTTP/1.1Host: cname.company.cnx-amz-acl: public-read-writeAuthorization: AWS {access-key}:{hash-of-header-and-secret} RGW_OP_CREATE_BUCKETclass RGWCreateBucket_ObjStore_S3 Delete Bucket DELETE /{bucket} HTTP/1.1Host: cname.company.cnAuthorization: AWS {access-key}:{hash-of-header-and-secret} RGW_OP_DELETE_BUCKETclass RGWDeleteBucket_ObjStore_S3 List Bucket Objects GET /{bucket}?max-keys=25 HTTP/1.1Host: cname.company.cn RGW_OP_LIST_BUCKETclass RGWListBucket_ObjStore_S3 Get Bucket Location GET /{bucket}?location HTTP/1.1Host: cname.company.cnAuthorization: AWS {access-key}:{hash-of-header-and-secret} class RGWGetBucketLocation_ObjStore_S3 Get Bucket ACL GET /{bucket}?acl HTTP/1.1Host: cname.company.cnAuthorization: AWS {access-key}:{hash-of-header-and-secret} RGW_OP_GET_ACLSclass RGWGetACLs_ObjStore_S3 Put Bucket ACL PUT /{bucket}?acl HTTP/1.1 RGW_OP_PUT_ACLSclass RGWPutACLs_ObjStore_S3 List Bucket MultiPart Uploads GET /{bucket}?uploads HTTP/1.1 RGW_OP_LIST_BUCKET_MULTIPARTSclass RGWListBucketMultiparts_ObjStore_S3 Head Bucket HEAD / HTTP/1.1Host: cname.company.cnAuthorization: AWS {access-key}:{hash-of-header-and-secret} RGW_OP_STAT_BUCKETclass RGWStatBucket_ObjStore_S3 Object相关 Object功能 RESTful API Ceph Operation &amp; Class Put Object PUT /{bucket}/{object} HTTP/1.1 RGW_OP_PUT_OBJclass RGWPutObj_ObjStore_S3 Copy Object PUT /{dest-bucket}/{dest-object} HTTP/1.1x-amz-copy-source: {source-bucket}/{source-object} RGW_OP_COPY_OBJclass RGWCopyObj_ObjStore_S3 Remove Object DELETE /{bucket}/{object} HTTP/1.1 RGW_OP_DELETE_OBJ class RGWDeleteObj_ObjStore_S3 Get Object GET /{bucket}/{object} HTTP/1.1 RGW_OP_GET_OBJ class RGWGetObj_ObjStore_S3 Get Object Info HEAD /{bucket}/{object} HTTP/1.1 RGW_OP_GET_OBJ class RGWGetObj_ObjStore_S3 Get Object ACL GET /{bucket}/{object}?acl HTTP/1.1 RGW_OP_GET_ACLS class RGWGetACLs_ObjStore_S3 Set Object ACL PUT /{bucket}/{object}?acl RGW_OP_PUT_ACLS class RGWPutACLs_ObjStore_S3 Initiate MultiPart Upload POST /{bucket}/{object}?uploads RGW_OP_INIT_MULTIPART class RGWInitMultipart_ObjStore_S3 MultiPart Upload Part PUT /{bucket}/{object}?partNumber=&amp;uploadId= HTTP/1.1 RGW_OP_PUT_OBJ class RGWPutObj_ObjStore_S3 List MultiPart Upload Parts GET /{bucket}/{object}?uploadId=123 HTTP/1.1 RGW_OP_LIST_MULTIPART class RGWListMultipart_ObjStore_S3 Complete MultiPart Upload POST /{bucket}/{object}?uploadId= HTTP/1.1 RGW_OP_COMPLETE_MULTIPART class RGWCompleteMultipart_ObjStore_S3 Abort MultiPart Upload DELETE /{bucket}/{object}?uploadId= HTTP/1.1 RGW_OP_ABORT_MULTIPART class RGWAbortMultipart_ObjStore_S3 如何访问对象存储？REST APIService: GET Bucket: GET &lt;null&gt; logging location versioning acl cors uploads PUT &lt;null&gt; logging versioning acl cors DELETE &lt;null&gt; cors HEAD &lt;null&gt; acl uploads POST &lt;null&gt; delete OPTIONS &lt;null&gt; Object: GET &lt;null&gt; acl uploadId PUT acl copy_source DELETE &lt;null&gt; uploadId HEAD &lt;null&gt; acl uploadId POST uploadId uploads OPTIONS &lt;null&gt; CLI命令行工具s3cmd工具：apt-get install s3cmd s3cmd --configure s3cmd --help SDK包兼容AWS S3提供的SDK包，但有部分功能不支持。 GUI管理界面需要前端支持添加GUI管理界面。 RadosGW的用户帐号user类型There are two user types: User: The term ‘user’ reflects a user of the S3 interface. Subuser: The term ‘subuser’ reflects a user of the Swift interface. A subuser is associated to a user . user操作 CREATE A USER 1adosgw-admin user create --uid=&#123;username&#125; --display-name="&#123;display-name&#125;" \[--email=&#123;email&#125;\] GET USER INFO 1radosgw-admin user info --uid=johndoe MODIFY USER INFO 1radosgw-admin user modify --uid=johndoe --display-name="John E. Doe" USER ENABLE/SUSPEND 12radosgw-admin user suspend --uid=johndoeradosgw-admin user enable --uid=johndoe REMOVE A USER 1radosgw-admin user rm --uid=johndoe RadosGW兼容Keystone认证官网上指出RadosGW兼容Openstack KeyStone认证，http://docs.ceph.com/docs/hammer/radosgw/keystone/ 但搜索发现Mirantis分析测试了RGW with Keystone，并不推荐这么做。 RGW中S3的认证方式 Keystone-based （disable default） 如何配置： 12[client.radosgw.gateway] rgw s3 auth use keystone = true RADOS-based（internal） S3使用KeyStone认证的优缺点优点 所有认证存储在统一的Keystone 不需要配置额外的S3认证管理系统，可以用Horizon替代 缺点 需要提升Keystone的性能以支持S3的请求 因为Keystone认证方式优先于内部的RADOS认证，则打开Keystone认证会使所有的S3认证先走KeyStone认证方式，如果失败了再尝试RADOS认证。这样使得正常使用S3RADOS认证的请求时延增大，影响S3的性能。 S3频繁访问Keystone服务，可能影响其他的Openstack service 我们如何使用?个人推荐不使用Keystone认证S3的方式，而使用RadosGW内部的认证机制比较好。 但这样就引入了我们的注册用户如何使用S3的问题，结合阿里云，金山云，可以做如下实现： 类似阿里云，用户默认不能使用S3功能，需要点击“开通对象存储”按钮。 可以在开通对象存储过程中，给用户创建对应的S3 user和AccessKey/SecretKey对，与前端帐号信息绑定。 金山云用户注册后，登录对象存储跳转到单独的控制台界面，里面的帐号设置里就有两个AccessKey/SecretKey对。 这种方式也需要AccessKey/SecretKey对与前端帐号信息绑定。 参考资料：https://content.mirantis.com/rs/451-RBY-185/images/Mirantis-Technical-Bulletin-S3-API-Keystone-integration-in-Ceph-RADOS-Gateway.pdfhttp://dolphm.com/benchmarking-openstack-keystone-token-formats/ RadosGW提供OSS服务功能基于上面的分析，使用RadosGW我们可以提供如下OSS功能，对象存储OSSV1.0.0版本功能具体包括哪些？还需找再讨论确定。 分类 描述 操作 Amazon 金山云 我们公司 Service Operation 获取所有bucket信息 GET Service √ √ √ Bucket Operation Bucket基本操作 DELETE Bucket √ √ √ GET Bucket √ √ √ HEAD Bucket √ √ √ PUT Bucket √ √ √ Bucket cors相关操作 DELETE Bucket cors √ √ √ GET Bucket cors √ √ √ PUT Bucket cors √ √ √ Bucket lifecycle相关操作 DELETE Bucket lifecycle √ × × GET Bucket lifecycle √ PUT Bucket lifecycle √ Bucket policy相关操作 DELETE Bucket policy √ × × GET Bucket policy √ PUT Bucket policy √ Bucket tagging相关操作 DELETE Bucket tagging √ × × GET Bucket tagging √ PUT Bucket tagging √ Bucket website相关操作 DELETE Bucket website √ × hammer: × jewel: √ GET Bucket website √ PUT Bucket website √ Bucket logging相关操作 GET Bucket logging √ √ √ PUT Bucket logging √ √ √ Bucket notification相关操作 GET Bucket notification √ × × PUT Bucket notification √ Bucket versioning相关操作 GET Bucket versioning √ × √ GET Bucket Object versions √ hammer: × jewel: √ PUT Bucket versioning √ √ Bucket acl相关操作 PUT Bucket acl √ √ √ GET Bucket acl √ √ √ Bucket requestPayment相关操作 GET Bucket requestPayment √ × hammer: × jewel: √ PUT Bucket requestPayment √ 枚举该Bucket下的所有分块上传 List MultiPart Uploads √ √ √ Object Operation 删除Object DELETE Object √ √ √ 删除多个Object Delete Multiple Objects √ √ √ 下载Object GET Object √ √ √ 获取Object ACL GET Object ACL √ √ √ 获取Object BT 种子 GET Object torrent √ × × 获取Object 元信息 HEAD Object √ √ √ Object对HTML5浏览器跨域支持 OPTIONS Object √ × × 浏览器表单上传Object POST Object √ √ ×? Amazon Glacier存储恢复 POST Object restore √ × × 上传Object PUT Object √ √ √ 设置Object ACL PUT Object acl √ √ √ 复制Object PUT Object - Copy √ √ √ 分块上传相关操作 Initiate Multipart Upload √ √ √ Upload Part √ √ √ Upload Part - Copy √ × × Complete Multipart Upload √ √ √ Abort Multipart Upload √ √ √ List Parts √ √ √ Image Thumbnail × √ ×]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>radosgw</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RDS配置使用Thin Provision LVM]]></title>
    <url>%2F2016%2F11%2F11%2Ftrove-lvm-thin-provision%2F</url>
    <content type="text"><![CDATA[概述在RDS的存储中，我们使用LVM卷给Docker container使用，默认LVM是不使用thin的feature的；在LVM 的 2.02.89 版本后，引入了Thin Pool的概念，开始支持创建Thin Provision的volume； 我们线上的LVM版本为： 1234openstack@trove-server1:~$ sudo lvm version LVM version: 2.02.98(2) (2012-10-15) Library version: 1.02.77 (2012-10-15) Driver version: 4.33.0 所以我们这里调整LVM的配置，来支持创建的volume为Thin Provision的volume，提高设备的资源使用率 LVM创建Thin Provision Volume创建Thin Pool123456789root@trove-server1:/home/openstack# vgs VG #PV #LV #SN Attr VSize VFree volume-group-pcie 1 2 0 wz--n- 2.91t 2.61t volume-group-sata 2 1 0 wz--n- 7.28t 372.44g volume-group-ssd 3 1 0 wz--n- 1.85t 94.59groot@trove-server1:/home/openstack# lvcreate -L 7168G --thinpool vg-sata-thin-pool volume-group-sataroot@trove-server1:/home/openstack# lvs LV VG Attr LSize Pool Origin Data% Move Log Copy% Convert vg-sata-thin-pool volume-group-sata twi-a-tz- 7.00t 0.00 查看Thin Pool1234567891011121314151617181920212223root@trove-server1:/home/openstack# lvdisplay volume-group-sata/vg-sata-thin-pool --- Logical volume --- LV Name volume-group-sata-pool VG Name volume-group-sata LV UUID A3N8Bs-7mVk-VaAU-MwJp-J5RD-Dlfp-0pIHzK LV Write Access read/write LV Creation host, time trove-server1, 2016-11-01 14:11:12 +0800 LV Pool transaction ID 0 LV Pool metadata volume-group-sata-pool_tmeta LV Pool data volume-group-sata-pool_tdata LV Pool chunk size 4.00 MiB LV Zero new blocks yes LV Status available # open 0 LV Size 7.00 TiB Allocated pool data 0.00% Allocated metadata 0.32% Current LE 1812081 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 256 Block device 252:10 创建Thin Volume123456root@trove-server1:/home/openstack# lvcreate -V 5G --thin -n thin_vol_test volume-group-sata/vg-sata-thin-pool Logical volume "thin_vol_test" createdroot@trove-server1:/home/openstack# lvs LV VG Attr LSize Pool Origin Data% Move Log Copy% Convert thin_vol_test volume-group-sata Vwi-a-tz- 5.00g vg-sata-thin-pool 0.00 vg-sata-thin-pool volume-group-sata twi-a-tz- 7.00t 0.00 从上述的Data%中可以看出，创建5.00G的volume，vg-sata-thin-pool的Data%为0.00； 删除Thin Volume123root@trove-server1:/home/openstack# lvremove volume-group-sata/thin_vol_testDo you really want to remove and DISCARD active logical volume thin_vol_test? [y/n]: y Logical volume "thin_vol_test" successfully removed Thin Volume创建时间测试下Thin Volume的创建时间是否与volume size有关系 1234567891011121314151617root@BJ-BGP03-001-001:/home/openstack/mikeyang# time lvcreate -V 30G --thin -n volume-test volume-group-sata/volume-group-sata-pool Logical volume "volume-test" createdreal 0m0.378suser 0m0.008ssys 0m0.028s root@BJ-BGP03-001-001:/home/openstack/mikeyang# time lvcreate -V 300G --thin -n volume-test volume-group-sata/volume-group-sata-pool Logical volume "volume-test" createdreal 0m0.359suser 0m0.000ssys 0m0.036s root@BJ-BGP03-001-001:/home/openstack/mikeyang# time lvcreate -V 1000G --thin -n volume-test volume-group-sata/volume-group-sata-pool Logical volume "volume-test" createdreal 0m0.374suser 0m0.016ssys 0m0.024s 从上述测试中可以看出，Thin Volume的创建时间很短，不受volume size大小影响 扩容Thin Pool因为Thin Pool对应的其实也是一个LVM volume，所以通过lvextend命令即可扩容 12345678# lvextend -L 2.5t volume-group-pcie/volume-group-pcie-poolExtending logical volume volume-group-pcie-pool to 2.50 TiB Logical volume volume-group-pcie-pool successfully resized或者# lvextend -L +1t volume-group-pcie/volume-group-pcie-pool Extending logical volume volume-group-pcie-pool to 2.50 TiB Logical volume volume-group-pcie-pool successfully resized 参考https://linux.cn/article-4288-1.htmlhttp://mathslinux.org/?p=379 Cinder支持创建Thin Provision volumeCinder配置查询Cinder代码，发现cinder是支持thin的lvm格式的； 需要在cinder中添加如下配置： 12345678/opt/etc/cinder/cinder.conf[lvm-sata]volume_group = volume-group-satavolume_driver=cinder.volume.drivers.lvm.LVMISCSIDrivervolume_backend_name=LVM_SATAvolume_clear = nonelvm_type = thin... 如上述配置，只需要添加：lvm_type = thin 即可；LVM会自动创建名称为-pool的thin pool； 重启服务1openstack@trove-server1:~$ service cinder-api restart; service cinder-backup restart; service cinder-scheduler restart; service cinder-volume restart; 重启cinder服务后，测试创建RDS实例即可； 原有volumes迁移修改为支持Thin Provision Volumes后，对之前创建的volume需要迁移到Thin Provision Volume上，这里给出迁移方案 在RDS的两个docker container中，一端一端的做volume的迁移； Umount volume1234567891011openstack@trove-server1:~$ docker exec -it 583e869197cd bashroot@rds-vpie5bx2:/# dfFilesystem 1K-blocks Used Available Use% Mounted onnone 103081248 2906328 94915656 3% /tmpfs 198079304 0 198079304 0% /devtmpfs 198079304 0 198079304 0% /sys/fs/cgroup/dev/rbd0 51475068 54156 48783088 1% /var/log/mysql/dev/rbd1 103081248 2906328 94915656 3% /var/log/rds/9709c88c-d392-4a52-8301-4135cadcdea6shm 65536 0 65536 0% /dev/shm/dev/vdb 5029504 118360 4649000 3% /var/lib/mysqlroot@rds-vpie5bx2:/# umount /var/lib/mysql/ 停止Docker Container1openstack@trove-server1:~$ docker stop 583e869197cd rename原有volume1root@trove-server1:/home/openstack# lvrename volume-group-pcie/volume-ca880d25-c95f-4d19-993c-ee7ccfbf8d1c volume-group-pcie/backup-volume 创建新的Thin Provision Volume1root@trove-server1:/home/openstack# lvcreate -V 5G --thin -n volume-ca880d25-c95f-4d19-993c-ee7ccfbf8d1c volume-group-ssd/volume-group-ssd-pool 格式化新的Thin Provision Volume1root@trove-server1:/home/openstack# mkfs.ext3 /dev/volume-group-ssd/volume-ca880d25-c95f-4d19-993c-ee7ccfbf8d1c volume数据迁移123456root@trove-server1:/home/openstack# mount /dev/volume-group-ssd/volume-ca880d25-c95f-4d19-993c-ee7ccfbf8d1c /mnt/destination/root@trove-server1:/home/openstack# mount /dev/volume-group-pcie/backup-volume /mnt/source/root@trove-server1:/mnt/source# cp -r -p * /mnt/destination/root@trove-server1:/mnt/source# cd /home/openstack/root@trove-server1:/home/openstack# umount /mnt/source/root@trove-server1:/home/openstack# umount /mnt/destination/ 启动Docker Container1openstack@trove-server1:~$ nova reboot 9709c88c-d392-4a52-8301-4135cadcdea6 若报错，则先执行nova start ，然后再执行 nova reboot 检查trove实例状态12345678910openstack@trove-server1:~$ docker exec -it 583e869197cd bashroot@rds-vpie5bx2:/# dfFilesystem 1K-blocks Used Available Use% Mounted onnone 103081248 2906328 94915656 3% /tmpfs 198079304 0 198079304 0% /devtmpfs 198079304 0 198079304 0% /sys/fs/cgroup/dev/rbd0 51475068 54156 48783088 1% /var/log/mysql/dev/rbd1 103081248 2906328 94915656 3% /var/log/rds/9709c88c-d392-4a52-8301-4135cadcdea6shm 65536 0 65536 0% /dev/shm/dev/vdb 5029504 118360 4649000 3% /var/lib/mysql 若发现mysql没启动，可能是因为/var/lib/mysql/里的文件权限导致的，把/var/lib/mysql/的权限修改为mysql:mysql 即可； 注意事项 最好先迁移floating ip不在的trove实例上的volume；然后切换floating ip后，再迁移另外一个trove实例的volume； 最好在客户使用rds服务不频繁的时候做数据迁移； 迁移完一端的trove实例后，需要等待mysql为active-active状态后再迁移另外一端；]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
        <tag>trove</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ceph节点load很高问题的分析解决]]></title>
    <url>%2F2016%2F09%2F12%2Fceph-node-high-workload-issue%2F</url>
    <content type="text"><![CDATA[现象当ceph在做数据迁移的时候，有一个节点的系统load很高，通过top命令可以看到如下信息： 从上图中可以看出，ceph-osd占的cpu很高，这个比较奇怪。。。对比别的机器上的ceph配置，没找到差异信息。 分析在该机器上安装perf工具，通过perf看到如下数据：perf top -p &lt;ceph-osd-pid&gt; 从上面的输出中看出，ceph-osd很多的时间花费在内核内存分配函数：isolate_freepages_block搜索网上的信息，都是两年前的帖子，而我们用的ceph版本为: Hammer 0.94.5-1，不应该出这个问题。 查看该机器上的内存信息，发现free的内存很少，执行命令把内存释放掉： sync; echo 3 &gt; /proc/sys/vm/drop_caches 然后继续触发数据迁移，在该机器上监控系统load和ceph-osd的 perf top输出： 这时候系统的负载正常，ceph-osd也没有花费太多时间在内存的分配上。 问题后续的测试中发现，当物理机的free内存很小时(约2GB)，会导致ceph-osd花费较多时间在memory的分配上。 原因分析我们系统的物理内存有256G，当free命令显示只有2GB时，可能很多都是零散的小内存了，而ceph-osd可能在运行时分配大内存，这就会触发内存页的回收和合并，所以我们就会看到ceph-osd花费很多时间在内核内存分配函数isolate_freepages_block上。 解决办法登陆物理机，执行命令：sync; echo 3 &gt; /proc/sys/vm/drop_caches 当然每次都手动的执行这个命令并不明智，我们把ceph节点的内存监控添加到监控项里，在监控到物理机内存小于配置值时自动触发上述命令。 参考资料https://lkml.org/lkml/2012/6/27/545http://lists.opennebula.org/pipermail/ceph-users-ceph.com/2014-November/044679.html]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>perf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Radosgw如何统计用户使用信息?]]></title>
    <url>%2F2016%2F08%2F30%2Frgw-user-statistics%2F</url>
    <content type="text"><![CDATA[需求在我们的需求中，要统计radosgw中每个用户的使用信息，然后计算用户的花费 包括三部分： 存储空间费用 流量费用 请求费用 存储空间统计：可以通过radosgw-admin命令获取流量统计/请求次数统计：可以通过radosgw支持usage info的统计和获取，对应的也是radosgw-admin命令 Bucket statsradosgw-admin bucket stats1234567891011121314151617181920212223242526272829命令：radosgw-admin bucket stats --uid=[userid]$ radosgw-admin bucket stats --uid=S3User[ &#123; "bucket": "database_backups", "pool": ".rgw.buckets", "index_pool": ".rgw.buckets.index", "id": "default.553597.94", "marker": "default.553597.94", "owner": "S3User", "ver": "0#959", "master_ver": "0#0", "mtime": "2016-06-14 16:22:20.000000", "max_marker": "0#", "usage": &#123; "rgw.main": &#123; "size_kb": 4792, "size_kb_actual": 4828, "num_objects": 18 &#125; &#125;, "bucket_quota": &#123; "enabled": false, "max_size_kb": -1, "max_objects": -1 &#125; &#125;] 上面输出信息中，有bucket的objects个数和占用空间。 bucket stats对应的admin ops12GET /&#123;admin&#125;/bucket?format=json HTTP/1.1Host &#123;fqdn&#125; http://docs.ceph.com/docs/jewel/radosgw/adminops/#get-bucket-info 支持RGW usageenable rgw usage默认radosgw的usage是关闭的，需要修改ceph.conf文件打开，配置如下： 1234567[client.rgw.BJ-001-001] host = BJ-001-001 keyring = /etc/ceph/ceph.client.radosgw.keyring log file = /var/log/ceph/client.radosgw.gateway.log rgw socket path = /var/run/ceph/ceph.client.rgw.BJ-001-001.sock rgw enable usage log = true rgw_frontends = civetweb port=80 创建对应pool .usage创建usage需要的pools 1ceph osd pool create .usage 64 ## usage_log_pool 重启rgw服务重启radosgw服务：/etc/init.d/radosgw restart 检查rgw的usage配置123456$ ceph daemon /var/run/ceph/ceph-client/ceph-client.rgw.BJ-001-001.76550.asok config show | grep usage "rgw_usage_max_shards": "32", "rgw_usage_max_user_shards": "1", "rgw_enable_usage_log": "true", "rgw_usage_log_flush_threshold": "1024", "rgw_usage_log_tick_interval": "30", 上面的配置信息中有关于usage log更新的配置，所以默认刚开启配置后查看usage信息还是空的，等上一段时间再查看就有输出了。 查看RGW usage信息命令格式12345radosgw-admin -h... usage show show usage (by user, date range) usage trim trim usage (by user, date range)... radosgw-admin usage show命令可以查看指定user在某一时间段的统计信息。 12用法：radosgw-admin usage show [--uid=&#123;uid&#125;] [--start-date=&#123;date&#125;] [--end-date=&#123;date&#125;] [--categories=&lt;list&gt;] [--show-log-entries=&lt;flag&gt;] [--show-log-sum=&lt;flag&gt;]示例：radosgw-admin usage show --uid=demo --start-date="2016-09-01 08:00:00" --end-date="2016-09-02 08:00:00" --categories="put_obj,put_acls" 线上环境实际输出： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122$ radosgw-admin usage show --uid=S3User&#123; "entries": [ &#123; "owner": "S3User", "buckets": [ &#123; "bucket": "", "time": "2016-10-09 05:00:00.000000Z", "epoch": 1475989200, "categories": [ &#123; "category": "list_buckets", "bytes_sent": 698, "bytes_received": 0, "ops": 2, "successful_ops": 2 &#125; ] &#125;, &#123; "bucket": "database_backups", "time": "2016-10-09 05:00:00.000000Z", "epoch": 1475989200, "categories": [ &#123; "category": "get_acls", "bytes_sent": 474, "bytes_received": 0, "ops": 1, "successful_ops": 1 &#125;, &#123; "category": "get_cors", "bytes_sent": 75, "bytes_received": 0, "ops": 1, "successful_ops": 0 &#125;, &#123; "category": "get_obj", "bytes_sent": 555936, "bytes_received": 0, "ops": 3, "successful_ops": 3 &#125;, &#123; "category": "list_bucket", "bytes_sent": 37633, "bytes_received": 0, "ops": 6, "successful_ops": 6 &#125;, &#123; "category": "put_obj", "bytes_sent": 0, "bytes_received": 558144, "ops": 2, "successful_ops": 2 &#125; ] &#125; ] &#125; ], "summary": [ &#123; "user": "S3User", "categories": [ &#123; "category": "get_acls", "bytes_sent": 474, "bytes_received": 0, "ops": 1, "successful_ops": 1 &#125;, &#123; "category": "get_cors", "bytes_sent": 75, "bytes_received": 0, "ops": 1, "successful_ops": 0 &#125;, &#123; "category": "get_obj", "bytes_sent": 555936, "bytes_received": 0, "ops": 3, "successful_ops": 3 &#125;, &#123; "category": "list_bucket", "bytes_sent": 37633, "bytes_received": 0, "ops": 6, "successful_ops": 6 &#125;, &#123; "category": "list_buckets", "bytes_sent": 698, "bytes_received": 0, "ops": 2, "successful_ops": 2 &#125;, &#123; "category": "put_obj", "bytes_sent": 0, "bytes_received": 558144, "ops": 2, "successful_ops": 2 &#125; ], "total": &#123; "bytes_sent": 594816, "bytes_received": 558144, "ops": 15, "successful_ops": 14 &#125; &#125; ]&#125; 上面输出中，summery里的total信息就足够我们来计费使用了。 usage show对应的admin ops12GET /&#123;admin&#125;/usage?format=json HTTP/1.1Host: &#123;fqdn&#125; http://docs.ceph.com/docs/jewel/radosgw/adminops/#get-usage radosgw-admin usage trim命令修剪rgw的usage log，这是因为在长时间使用下，以往的usage log会占用ceph存储空间。在确定不需要的时间段后，通过该命令删除这部分记录。 用法示例： 123radosgw-admin usage trim --start-date=2010-01-01 --end-date=2010-12-31radosgw-admin usage trim --uid=johndoeradosgw-admin usage trim --uid=johndoe --end-date=2013-12-31 usage trim对应的admin ops12DELETE /&#123;admin&#125;/usage?format=json HTTP/1.1Host: &#123;fqdn&#125; http://docs.ceph.com/docs/jewel/radosgw/adminops/#trim-usage RGW log信息radosgw log功能可以记录每个ops的操作记录，方面查看历史操作和统计信息，使用方法如下。 enable rgw log123[client.rgw.BJ-001-001]... rgw enable ops log = true 或者命令： 12345678910ceph daemon /var/run/ceph/ceph-client/ceph-client.rgw.BJ-001-001.7455.asok config set rgw_enable_ops_log true``` ### 创建log对应的pools创建rgw log需要的pools```shceph osd pool create .log 64 ## log_poolceph osd pool create .intent-log 64 ## intent_log_pool rgw log命令可用通过radosgw-admin log命令操作 123456789101112131415161718192021222324252627282930313233343536373839$ radosgw-admin log list[ "2016-10-11-14-default.553597.94-database_backups"]$ radosgw-admin log show --bucket=database_backups --date=2016-10-11-14 --bucket-id=default.553597.94&#123; "bucket_id": "default.553597.94", "bucket_owner": "S3User", "bucket": "database_backups", "log_entries": [ &#123; "bucket": "database_backups", "time": "2016-10-11 06:02:20.595010Z", "time_local": "2016-10-11 14:02:20.595010", "remote_addr": "", "object_owner": "S3User", "user": "S3User", "operation": "GET", "uri": "\/database_backups\/a2edb41b-3515-4075-9b37-43dd7daf79dc.xbstream.gz.enc", "http_status": "200", "error_code": "", "bytes_sent": 279072, "bytes_received": 0, "object_size": 279072, "total_time": 0, "user_agent": "", "referrer": "" &#125; ], "log_sum": &#123; "bytes_sent": 279072, "bytes_received": 0, "total_time": 0, "total_entries": 1 &#125;&#125; $ radosgw-admin log rm --object=2016-10-11-14-default.553597.94-database_backups 问题测试发现，无论是部署了几个rgw，因为usage信息是存储在rados系统的.usage pool里的，不区分不同rgw过来的请求，所以通过radosgw-admin usage命令看到的是所有rgw操作的统计信息，这样我们没法区分内网和外网的rgw流量和ops个数。 参考青云，青云支持内网/外网流量和ops统计分开计费。https://docs.qingcloud.com/guide/qingstor.html#id15 若内网的rgw流量不计费，则可以通过配置rgw的参数实现，可配置的rgw参数为： 1234rgw_enable_usage_log：true/false ## 是否enable usage统计rgw_enable_ops_log：true/false ## 是否enable ops log统计rgw_ops_log_rados：true/false ## 是否记录ops log到rados里，对用pool为：.logrgw_ops_log_socket_path：&lt;string&gt; ## 配置ops log的socket path，ops log保存在内存，可以通过socket接口访问 每个rgw都可以配置独立的上述参数，所以针对内网的rgw，我们可以配置rgw_enable_usage_log = false，这样通过内网访问的操作都不会记在usage里。 参考资料http://docs.ceph.com/docs/jewel/man/8/radosgw/#usage-logginghttp://docs.ceph.com/docs/jewel/man/8/radosgw-admin/http://docs.ceph.com/docs/jewel/radosgw/admin/#usagehttp://docs.ceph.com/docs/jewel/radosgw/adminops/http://lyang.top/2016/01/04/Ceph-usage-%E7%9A%84%E6%9F%A5%E8%AF%A2/]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>radosgw</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何使用radosgw admin ops api?]]></title>
    <url>%2F2016%2F08%2F29%2Frgw-admin-ops-api%2F</url>
    <content type="text"><![CDATA[概述通过Radosgw的Admin ops api，可以执行radosgw-admin对应的很多管理操作。 创建管理用户要通过Restful请求管理Radosgw，必须先创建一个管理账户，user自己制定，可以命名为admin，例如： 1radosgw-admin user create --uid=admin --display-name=admin 此时admin还仅仅是普通的权限，需要通过–cap添加user的capabilities，例如： 12radosgw-admin caps add --uid=admin --caps="users=read, write"radosgw-admin caps add --uid=admin --caps="usage=read, write" GET USER INFOGet user information. If no user is specified returns the list of all users along with suspension information. caps: users=read SYNTAX12GET /&#123;admin&#125;/user?format=json HTTP/1.1Host: &#123;fqdn&#125; 比如上述的“GET USER INFO”的API，需要使用的{admin}用户有“users=read”的caps。 API示例创建user12345678910111213$ cat usercreate.sh#!/bin/bashtoken=5L65QDE4df8JJ8RM7MN5 ## USER_TOKENsecret=Y9HPiBCwLDeSMSaiQhmPT2h7NgNUndvLERhktnIZ ## USER_SECRETquery=$1name=$2echo $query, $namequery3="&amp;uid="query2=admin/userdate=$(for i in $(date "+%H") ; do date "+%a, %d %b %Y $(( 10#$i-8 )):%M:%S +0000" ; done)header="PUT\n\n\n$&#123;date&#125;\n/$&#123;query2&#125;"sig=$(echo -en $&#123;header&#125; | openssl sha1 -hmac $&#123;secret&#125; -binary | base64)curl -v -H "Date: $&#123;date&#125;" -H "Authorization: AWS $&#123;token&#125;:$&#123;sig&#125;" -L -X PUT "http://&lt;your-host-ip&gt;/$&#123;query2&#125;?format=json$&#123;query3&#125;$&#123;query&#125;&amp;display-name=$&#123;name&#125;" -H "Host: &lt;your-host-ip&gt;" 列出user info1234567891011$ cat userinfo.sh#!/bin/bashtoken=5L65QDE4238JJ8RM7MN5 ## USER_TOKENsecret=Y9HPiBCwLDeSMSaiQhmPT2h7NgNUnqVLERhktnIZ ## USER_SECRETquery=$1query3="&amp;uid="query2=admin/userdate=$(for i in $(date "+%H") ; do date "+%a, %d %b %Y $(( 10#$i-8 )):%M:%S +0000" ; done)header="GET\n\n\n$&#123;date&#125;\n/$&#123;query2&#125;"sig=$(echo -en $&#123;header&#125; | openssl sha1 -hmac $&#123;secret&#125; -binary | base64)curl -v -H "Date: $&#123;date&#125;" -H "Authorization: AWS $&#123;token&#125;:$&#123;sig&#125;" -L -X GET "http://&lt;your-host-ip&gt;/$&#123;query2&#125;?format=json$&#123;query3&#125;$&#123;query&#125;" -H "Host: &lt;your-host-ip&gt;" 删除user1234567891011$ cat userdelete.sh#!/bin/bashtoken=5L65QDE4238JJ8RM7MN5 ## USER_TOKENsecret=Y9HPiBCwLDeSMSaiQhmPT2h7NgNUnqVLERhktnIZ ## USER_SECRETquery=$1query3="&amp;uid="query2=admin/userdate=$(for i in $(date "+%H") ; do date "+%a, %d %b %Y $(( 10#$i-8 )):%M:%S +0000" ; done)header="DELETE\n\n\n$&#123;date&#125;\n/$&#123;query2&#125;"sig=$(echo -en $&#123;header&#125; | openssl sha1 -hmac $&#123;secret&#125; -binary | base64)curl -v -H "Date: $&#123;date&#125;" -H "Authorization: AWS $&#123;token&#125;:$&#123;sig&#125;" -L -X DELETE "http://&lt;your-host-ip&gt;/$&#123;query2&#125;?format=json$&#123;query3&#125;$&#123;query&#125;" -H "Host: &lt;your-host-ip&gt;" 获取usage info1234567891011$ cat usageinfo.sh#!/bin/bashtoken=5L65QDE4238JJ8RM7MN5 ## USER_TOKENsecret=Y9HPiBCwLDeSMSaiQhmPT2h7NgNUnqVLERhktnIZ ## USER_SECRETquery=$1query3="&amp;uid="query2=admin/usagedate=$(for i in $(date "+%H") ; do date "+%a, %d %b %Y $(( 10#$i-8 )):%M:%S +0000" ; done)header="GET\n\n\n$&#123;date&#125;\n/$&#123;query2&#125;"sig=$(echo -en $&#123;header&#125; | openssl sha1 -hmac $&#123;secret&#125; -binary | base64)curl -v -H "Date: $&#123;date&#125;" -H "Authorization: AWS $&#123;token&#125;:$&#123;sig&#125;" -L -X GET "http://&lt;your-host-ip&gt;/$&#123;query2&#125;?format=json$&#123;query3&#125;$&#123;query&#125;" -H "Host: &lt;your-host-ip&gt;" RadosGW Admin ops API还有很多其他的APIs详情见：http://docs.ceph.com/docs/master/radosgw/adminops/ 问题记录AccessDenied脚本报错：&lt; HTTP/1.1 403 Forbidden … {&quot;Code&quot;:&quot;AccessDenied&quot;}radosgw的log里报错：rgw/rgw_auth_s3.cc:188 NOTICE: failed to parse date for auth header 从上述log中看出是请求header中的date解析不出来，修改脚本中date如下：date=$(for i in $(date &quot;+%H&quot;) ; do date &quot;+%a, %d %b %Y $(( 10#$i-2 )):%M:%S +0000&quot; ; done) RequestTimeTooSkewed脚本报错：&lt; HTTP/1.1 403 Forbidden … {&quot;Code&quot;:&quot;RequestTimeTooSkewed&quot;}radosgw的log里报错：rgw/rgw_rest_s3.cc:2398 NOTICE: request time skew too big now=2016-08-29 15:09:40.000000 req_time=2016-08-29 21:09:40.000000 从上述log中看出是request的time跟服务器时间差别较大，修改脚本中date如下：date=$(for i in $(date &quot;+%H&quot;) ; do date &quot;+%a, %d %b %Y $(( 10#$i-8 )):%M:%S +0000&quot; ; done) 权限问题参考具体命令的caps需求，添加user的caps 参考资料http://docs.ceph.com/docs/master/radosgw/adminops/http://egonzalez.org/ceph-radosgw-admin-ops-how-to-use-it/]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>radosgw</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ceph在云英的实践]]></title>
    <url>%2F2016%2F08%2F12%2FCeph-used-in-cloudin%2F</url>
    <content type="text"><![CDATA[以下是在云英工作时候做的一个Ceph实践的分享，原文链接为：http://www.sdnlab.com/17584.html 概述大家好，我是云英负责存储的研发工程师，杨冠军，很高兴今天能在这里跟大家一起讨论分享下Ceph和Ceph在云英的实践。首先我先介绍下，Ceph是什么，我们为什么选择Ceph？ Ceph是最近开源系统中很火的一个项目，基于Sage Weil的一片博士论文发展而来的一个分布式文件系统，可提供PB级，动态可扩展，数据安全可靠的存储服务。Ceph提供分布式存储服务包括：块存储RBD，对象存储RADOSGW和CephFS三种，基本覆盖了绝大部分企业对存储的需求，所以越来越多企业加入到使用Ceph的行列。在国内也有越来越多的个人和企业参与到Ceph的研发中，贡献自己的力量。 Ceph架构下面我们来看下Ceph系统的整体架构： 如上图所示，有如下几部分： RADOS: Ceph的核心模块，提供固定大小的Object存储 LIBRADOS: RADOS的library，提供C, C++, Java, Python, Ruby, PHP的API访问RADOS RADOSGW: Rados GateWay，基于bucket策略，提供一个兼容S3和Swift的的REST gateway RBD: 提供可靠的分布式块存储服务，结合Openstack，应用非常之广 CEPH FS：提供POSIX协议的文件系统服务 从上面可以看出，RADOS是Ceph的核心，它主要由MDS + OSD组成，下图描述的即是一个个笑脸(object)如何存储到OSDs中： Client端会跟Monitors通信，获取Cluster Maps信息，然后通过固定的算法算出每个Object存储的OSD位置，直接与OSD通信，写入Object数据。 这里Ceph的一大优势很好的体现出来：无需元数据服务器节点，所有都是“无需查表，算算就好”！ Ceph主要特性前面我们提到了Ceph是一个动态可扩展，数据安全可靠的存储服务，现在我们逐一来讨论下Ceph作为一个分布式系统必须的三种特性： 高扩展性针对集群的扩容需求，Ceph支持OSD和Monitor集群的动态可扩容，并通过两层Map机制[(pool, object) -&gt; (pool, PG) -&gt; OSD set)来有效的隔离了集群扩容对上层client的影响，提供了很好的扩展性。这样我们可以利用大量的低配置设备轻松的搭建出PB甚至EB级的存储系统。 上图描述了Client段的一个File如何经过多层的映射，写入到OSDs中的，也正是这多层映射和CRUSH算法，保证了Ceph的高扩展性。在PG数不变的情况下，底层OSD的扩展对Client端是完全透明的。 高可靠性针对数据的安全可靠，Ceph会在集群中存储同一数据的多个副本（或者其他类型的冗余，例如erasure code），来保证在某些设备故障后，用户存入的数据还可用，针对用户不同的高可用需求，Ceph可以很方便的设置Pool的数据冗余规则，另外通过Ceph Crushmap，用户也可以方便的设置各个备份之间存储位置的逻辑关系，比如达到多个副本跨机房、跨机架、跨机器等目的，提高集群的数据可靠性。 另外Ceph能自动探测到OSD/Monitor/MDS的故障，并自行恢复，有效减少了单设备节点的稳定性对集群的影响。 下图是Ceph中一个写IO的流程，保证数据的强一致性： 从图中可以明显的看出，Ceph的写会由Client发给Primary OSD，由Primary OSD发送副本给Replica OSD上，而只有所有的副本都写完成后，写IO才算完成，保证了数据的一致性和高可靠性。 高性能Ceph中通过文件切分和CRUSH算法，保证数据chunk分布基本均衡，同时Ceph的无元数据信息的设计(CephFS除外)，保证了Client可以根据cluster map，通过固定算法确定数据的位置信息，避免了单个元数据节点的性能瓶颈，可以提供非常高的并行化IO性能。 如上图所示，Client端数据经过切分为Objects后，可以同时与多个OSDs交互，写入数据。 Ceph在云英的实践前面大致介绍了Ceph系统的原理和架构，那我们为什么选择Ceph呢？对比现有的一些其他的分布式存储系统，Ceph有如下优点： 完全的开源系统 能提供块存储，对象存储和文件系统存储的统一架构 设计理念先进，是个高扩展，高可用，高性能的分布式文件系统 与Openstack完美结合，社区支持好 Ceph社区活跃度很大 总之，作为一个比较完善的分布式存储系统，Ceph能满足绝大多数企业的存储需求，同时它也提供了足够多的配置选项，给用户根据需求定制化自己的存储系统。 上面大致介绍了Ceph的原理架构和设计理念，下面我们来介绍下Ceph在云英的具体实践，给大家一个真实的感受。 首先说一下云英，云英的全称是北京云英传奇技术有限公司。我们是一家专注于为创客和行业客户提供云计算服务的公司，我们提供的有公有云和私有云服务，包括IAAS和PAAS层产品，网址为 www.cloudin.cn 在云计算的技术上我们选择了openstack + ceph的架构，基于之上实现了我们自己的逻辑和特色功能，包括云监控，自动化运维，RDS等服务。针对这么多应用，绝大部分服务的数据都依靠Ceph系统提供可靠的存储服务，在应用实践中，针对我们自己的系统架构和机房部署，我们对Ceph也进行了部分调优和优化，达到了我们的应用需求。 下面我们来介绍下Ceph存储在云英的应用，大致可以分为如下几种： RBD块存储服务 CephFS提供服务器间数据共享 RADOSGW提供对象存储服务 Ceph的性能测试 Ceph的优化 Ceph的监控 首先我们先介绍下第一项: RBD块存储服务我们的块存储服务主要给Openstack组件使用，分为如下几类： Glance镜像存储 Nova instance数据存储 Cinder volume存储 Backup服务 应用中，把Openstack的Glance组件image和Nova instance数据一起存储到Ceph集群中，可以很好的避免Openstack创建虚拟机时的image复制，并且利用Ceph RBD的snapshot功能，基本可以实现秒级创建Nova instance。 同样利用RBD的snapshot功能，可以有效的减少Cinder Volume，Nova instance的备份创建时间和空间占用。 另外因为Ceph底层是一个共享存储，所以基于此可以便利的实现Nova instance的热迁移功能，缩短了虚拟机热迁移导致的服务停顿时间。 整个应用场景如下图所示，Ceph作为了一个统一存储，对Openstack各个组件提供服务： CephFS提供服务器间数据共享CephFS是基于Rados实现的PB级分布式文件系统，这里引入了一个新的组件MDS(Meta Data Server)，它主要为兼容POSIX文件系统提供元数据，如目录和文件元数据。同时，MDS会将元数据也存在RADOS(Ceph Cluster)中。元数据存储在RADOS中后，元数据本身也达到了并行化，大大加强了文件操作的速度。需要注意的是MDS并不会直接为Client提供文件数据，而只是为Client提供元数据的操作。 在我们的生产环境中，遇到过服务器间共享数据的需求。之前的思路可以通过NFS来实现，现在基于CephFS，可以轻松的满足需求。虽说我们用的版本Hammer中，Ceph官方没说CephFS完善到可用于生产环境，但也是经过大规模测试后的版本，据说在雅虎也有大规模使用的集群，另外我们共享的数据对可靠性没那么大要求，IO量也不是很大，所以CephFS已经能很好的满足我们的需求了。 最近Ceph发布的JEWEL版本是官方声称的第一个CephFS稳定版本，如果对CephFS有强烈需求的话，可以部署最新的JEWEL版本。另外部署中最好使用单MDS的方式，虽说Ceph支持MDS集群和很多很酷的特性，比如负载均衡，动态子树迁移，故障恢复等，但MDS集群还不是Ceph官方的推荐。 RADOSGW提供对象存储服务RadosGW是基于Librados之上实现的，它主要提供兼容S3、Swfit的RESTful接口。同时RadosGW提供了Bucket的命名空间(类似于文件夹)和账户支持，并且具备用于账单目的使用记录。相对的，它增加了Http协议的负载。RadosGW使得Ceph Cluster有了分布式对象存储的能力，如上面提到的Amazon S3和Swift等。企业也可以直接使用其作为数据存储或备份等用途。 RADOSGW在云英主要应用于以下两方面 1. RDS的数据备份存储RDS服务是云英提供的一项的MySQL服务，我们保证了MySQL的高可用和性能，用户只需创建自己的RDS服务即可使用，而不用麻烦的自己搭建MySQL服务并配置其高可用等特性。在RDS服务中，用户会有创建MySQL备份的需求，而这种备份是最适合对象存储的，我们自己实现了RDS的S3备份接口，把RDS的备份数据上传到兼容S3的RADOSGW中。这样使用统一的Ceph系统，我们就不需要再搭建一套Swift对象存储系统了，简化了公司的运维成本。 2. 对象存储服务Object Storage Service是很重要的一项存储服务，越来越多的应用都开始使用便利的对象存储来存储数据。Openstack源生的对象存储服务系统是Swift，对比Ceph，Swift可以便利的搭建部署，但它也有自己的劣势，我们也不想同时维护两套存储系统，所以我们就选择RADOSGW提供兼容S3和Swift的对象存储服务。 Ceph的性能测试为了做到心中有数，我们需要在现有硬件配置条件下，测试Ceph的性能，看是否满足我们的期望。结合网上的参考，Ceph性能测试可分为如下几类： RADOS性能测试 rados bench 命令 rados load-gen 命令 rbd块设备性能测试 rbd bench-write 命令 fio工具测试 fio + rbd ioengine 测试 fio + libaio 测试 在云英的应用中，Ceph主要提供的是rbd块设备，所以经过评估，我们选择了比较贴合实际应用的方式，使用fio + libaio的测试方法来测试虚拟机中云硬盘的性能。为此我们写了一系列的测试脚本来自动化测试和分析测试结果，结合Ceph的参数优化，给出实际的性能参考。 测试统计结果样本如下： 123456789101112131415161718device: /dev/rbd1ioengine: libaio size: 1000 runtime: 300models: randread randwritemixread: 50blocks: 4iodepth: 2numjobs: 1processes: 1 2 4rate: , ratemin:rate_iops: 100, rate_iops_min:host: 10.10.0.12 10.10.0.13 10.10.0.14 10.10.0.15hosts,model,mixread,bs,iodepth,numjobs,r-bw(KB/s),r-iops,r-avglat(msec),w-bw(KB/s),w-iops,w-avglat(msec)1,randread,/,4,2,1,1329.03,331,24.13,0,0,01,randwrite,/,4,2,1,0,0,0,1437.01,356,15.142,randread,/,4,2,1,3187.00,792,2.41,0,0,02,randwrite,/,4,2,1,0,0,0,3175.73,792,13.544,randread,/,4,2,1,6399.24,1591,1.72,0,0,04,randwrite,/,4,2,1,0,0,0,6344.70,1582,15.95 上述测试结果可以方便的导出到excel，制作成表格进行分析对比： 当然，测试并不是一帆风顺的，测试中的我们也会遇到一些问题，也会做一些调整，这里分享下常见的几个注意事项： 云硬盘需要先dd一遍后再测试 每轮测试前清空虚拟机的缓存数据 每轮测试前清空物理服务器的缓存数据 每轮测试中通过iostat命令搜集磁盘负载数据 测试获取顺序读写的bandwidth和随机读写的iops 独占系统，防止产生干扰 每轮测试后分析测试数据，找到系统评价和优化可能性 Ceph的优化我们前面说过，Ceph提供了很多的配置参数来允许用户订制自己的分布式存储系统，在赋予用户这个便利性的同时，也意味着如果用户想获取自己系统的最大性能时，必须自己进行分析调优。 Ceph是一个复杂的系统，官方的默认配置能保证系统基本运行，但是不能贴合用户实际需求，达到最大化用户物理系统性能的要求。虽说现在也已经有了一些朋友分享Ceph的配置参考和调优，但对每个用户来说都不是拿来主义。他们只是提供了一种优化的参考，具体的效果如何还需要用户贴合自己的实际测试结果来调整。 对于云英来说，我们的物理机配置是相当前卫的，用于Ceph系统的物理机硬件配置大致如下： 200G+内存 32核Intel Xeon处理器 1:3的SSD和SATA配比，SSD分区做Journal，SATA盘做OSD PCIE的存储卡提供超高性能存储Pool 万兆网卡提供Ceph的Cluster Network通信 千兆网卡提供Ceph的Public Network通信 参考网上朋友的Ceph配置和调优参数后，结合我们的经验和测试分析，我们做了适合自己的独特优化，对比各种调优项前后，很好的达到我们的要求。 依据我们的经验，可以在以下几个方面做Ceph的性能调优： BIOS设置： 开启CPU的Hyper-Threading 关闭CPU节能 关闭NUMA Linux参数调优 CPU设置为performance模式 调整内核的pid_max限制 调整SATA/SSD IO Scheduler 调整磁盘的read_ahead_kb大小 XFS相关 xfs mkfs options xfs mount options filestore调整 filestore fd cache size filestore omap header cache size filestore queue相关参数 filestore wbthrottle相关参数 object size journal 性能高的SSD分区做journal journal size &gt; 5G journal queue osd相关 osd上PG总数限制 osd op threads osd recovery threads crushmap优化 给osd划分合理的pools 故障域切分，降低数据丢失概率 Ceph的监控对于一个大型系统来说，完善的监控很重要，我们不可能时刻靠人工来发现系统的问题。针对Ceph系统，我们调研了很多种方案，主要有如下几种： Ceph官方的Calamari（已一年多没有提交） Intel的VSM Ceph-Dash Inkscope 定制化的Diamond + Grafana Ceph Collectd + Grafana 最后选择了适合我们的，方便我们扩展的一种。即：Diamond + Graphite + Grafana，下面介绍一下这些组件： Diamond是一个客户端性能收集工具，Python编写，易与扩展。 Graphite是一个Python编写的企业级开源监控工具，采用django框架。 Grafana是功能齐全的度量仪表盘和图形编辑器，支持Graphite，InfluxDB和OpenTSDB。 部署后，我们可以在Grafana的前端订制我们自己的监控项，类似下图： 另外，Ceph进程的监控，集群状态的监控，我们通过自己写的脚本，完美的集成到Zabbix系统，实现了Ceph系统有问题的实时通知。 我们的脚本监控主要有如下几个方面： Ceph状态和空间使用率的监控 OSD状态的监控和自动拉起 Monitor状态的监控和自动拉起 PG状态的监控和报警 Slow Requests的监控和报警 总结总之，Ceph是一个大型的完善的分布式系统，对它的研究和优化是一个持续的过程，在后续我们会继续深入研究Ceph系统，学习其精髓，优化其应用，也会继续分享我们的心得。 Q&amp;AQ1:有多大存储规模？ A1:我们有两个机房，每个机房是一个独立的Ceph集群。每个集群里有200多个OSD，裸容量约为1PB。 Q2：主要还是块服务？对象用于备份？ A2：我们的应用方式主要是结合Openstack提供云硬盘服务，所以主要是块存储服务。现在用于对象存储的有RDS的数据备份，还有马上准备推出的兼容S3的对象存储服务。 Q3：2个集群之间啥关系？互备？ A3：现在两个集群是独立的，没有打通作为互备。后续对于对象存储，我们有这个计划。 Q4：分享中提到调整磁盘read ahead大小和线程pid个数，只是告诉我们，去调整，但是我们不知道，调整多大？有什么参照关系？？比如是osd，两倍？ A4：应用中我们会调整SATA磁盘的read_ahead_kb到8K-16K，提高OSD的性能。PID的个数是linux 内核最大线程的个数，应用中我们会根据物理服务器上OSD的个数去调大这个值，避免因为PID个数的限制，导致服务OSD的线程数不够。当然如果每个服务器上的OSD个数较少，这个值可以不用调整的。 Q5：200个osd，也就是200块盘？ A5：200个OSD对应200多块盘，Ceph推荐的也是一块SATA盘对应一个OSD，SSD盘就不一定是这个对应关系了，这取决于SSD盘的性能。 Q6：i/o能达到多少M连续的文件实测完？最多支持多少块硬盘 支持混插不？支持异地双活不？ A6：Ceph系统IO的吞吐量跟系统的规模有关系，我们最后得到的性能约为所有OSD磁盘性能/备份个数后的40% — 60%左右。Ceph对磁盘个数没有限制，越多的磁盘就对应越大的Ceph集群，在提升系统容量的同时，也会带来一系列问题。现在据说最大的一个Ceph集群有3000多个OSDs。另外Ceph对OSD的磁盘没特殊要求，支持不同配置、不同容量的磁盘。但是针对商业使用，还是推荐相同的配置硬盘，方便做crushmap的划分。异地双活的方式，是需要上层技术支持的。Ceph本身没有异地双活。。但Ceph的对象存储功能 - RADOSGW，可以配置不同Region的备份，支持异地备份。 Q7：ceph稳定吗，云英有没有遇到过比较大的ceph故障？ A7：我们使用的是Ceph官方支持的Hammer版本，还是比较稳定的。相对的CephFS，推荐使用最新的Jewel版本，这个是第一个官方声明的CephFS稳定版本。在云英的使用中，我们会遇到一些小故障，但Ceph都能很好的处理这些小故障，因为它是自修复的，不会影响上层应用的访问。我们也通过一些监控及时发现这些故障，人为查看修复它们。Ceph是个分布式文件系统，每份数据都有多个备份，Monitors也是个主备的集群，正常不会出现大的故障，触发是维护人员的误操作等等。所以迄今为止我们还没有遇到过大的故障。 Q8：性能优化从哪着手？ A8：性能优化的问题，这个网上有些推荐，我们也是基于这些和自己对Ceph的理解，结合测试的结果和遇到的问题，进行分析后再做的具体优化调整。通过一些对比测试，可以分析出是SSD上的journal性能瓶颈？还是SATA盘的IO瓶颈？还是CPU处理能力的瓶颈？总之，Ceph的优化可以从client端发起IO到OSD写下数据这个path上分析后进行优化。 Q9：pid个数和osd有什么样关系？比如说我有两块osd，那么建议将pid设置成4。类似这样关系，是什么样规则？ A9：我们用的是ubuntu系统，内核默认的pid_max应该是32768。而每个OSD会占用很多线程来处理数据，为了防止这个值影响OSD的启动和性能，我们调整为一个很大的值。 Q10：分享中提到写完所有副本才算完成，假如中间有一个副本写入失败了，需要回退之前的吗？之前写入成功的吗？ A10：写的操作是client发送Primary OSD的，它再发送给Replica OSDs，如果有IO写失败，写操作会hang住。Ceph的写机制没有超时设置，Ceph也不会回退写，所以在对应IO写失败的OSD被mark为down状态前，写操作是不会返回给客户端的。而在OSD被mark为down以后，Ceph会启动恢复机制，数据副本会写入新的OSD里。同时Ceph也有scrub机制，能保证PG sets里的数据一致性。 Q11：有个问题，cephfs 本身有服务器共享功能，那openstack 的Manila 项目是不是感觉就多余了？你们现在有做cephfs与Manila 的对接嘛 A11：Manila提供的是云上的文件共享，通过driver的方式连接多种存储后端，而Cephfs是实现POSIX语义的分布式文件系统，它通过native driver给Manila对接使用，所以这两个项目是不重复的。我们还没做Manila与CephFS的对接，回头我们会考虑把这个提上日程。]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>openstack</tag>
        <tag>grafana</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ceph修改OSD和Monitor的网络]]></title>
    <url>%2F2016%2F08%2F11%2FCeph-modify-osd-monitor-network%2F</url>
    <content type="text"><![CDATA[引言随着Ceph应用的不断深入，不少企业在部署完Ceph集群并运行一段时间后，会遇到机房网络变动或集群网络升级的情况，这时我们都期望能用最简便高效的方法解决Ceph网络变化带来的问题。那么，如何处理才能快速解除Ceph网络变动危机呢？ 在Ceph底层的RADOS集群中，有两种节点，一种是为数众多的、负责完成数据存储和维护功能的OSD（Object Storage Device），另一种则是若干个负责完成系统状态检测和维护的monitor。OSD和monitor之间相互传输节点状态信息，共同得出系统的总体工作状态，并形成一个全局系统状态记录数据结构。所以，只需要修改这两种节点的网络，就可以解决因机房网络变化或集群网络升级而引起的诸多问题。 本文基于Ceph的官方文档和作者的实践，为大家讲述正确修改Ceph OSD和Monitor网络的方法。希望能够帮助大家在尽量减少对现有Ceph集群影响的情况下，通过修改Ceph的OSD和Monitor网络实现迁移。 12本文使用的Ceph版本：Hammer 0.94.5 测试系统环境：Ubuntu 14.04 修改OSD网络由于Ceph OSD的网络配置是启动时读取ceph.conf配置动态加载的，所以修改OSD的网络比较简单，步骤如下： 修改ceph.conf中pubulic/cluster network信息 12public network =172.16.1.0/24cluster network =172.16.1.0/24 把ceph.conf更新到所有OSD节点测配合 1ceph-deploy --overwrite-conf config push &lt;node1&gt; &lt;node2&gt; &lt;node3&gt; 重启所有的osd daemon 1restart ceph-all 检查OSD使用的网络 12345# netstat -nap | grep ceph-osdtcp 0 0 172.16.1.16:844 0.0.0.0:* LISTEN 23412/ceph-osdtcp 0 0 172.16.1.16:812 0.0.0.0:* LISTEN 19423/ceph-osdtcp 0 0 172.16.1.16:845 0.0.0.0:* LISTEN 23412/ceph-osdtcp 0 0 172.16.1.16:813 0.0.0.0:* LISTEN 19529/ceph-osd 修改Monitor的网络因为Ceph的Monitor是集群中非常重要的模块，它们需要维护一组合理有效的Monitor节点信息，这些节点之间彼此能发现，通过选举达成一致的状态，来保证整个Ceph系统处于一个可用的一致状态。 不同于别的Ceph Daemon通过ceph.conf文件中的配置来与Monitor通信，Monitors之间则通过独立的monitor map来彼此发现，在monitor map中有monitor的ip信息，所以单独通过修改ceph.conf文件的方法来修改Monitor的网络是行不通的。 下面是修改monitor网络的两种方法： 方法1依次添加新的monitor node(使用新的网络)，再删除旧的monitor node注: 新的monitor网络跟旧的monitor的网络必须是互通的，否则该方法失效。 添加新的monitor节点步骤如下： 在新的monitor节点创建默认文件夹 12ssh &#123;new-mon-host&#125;sudo mkdir /var/lib/ceph/mon/ceph-&#123;mon-id&#125; 获取monitors的keyring文件，若内部集群没配置认证，该步会出错，忽略即可 1ceph auth get mon. -o &#123;tmp&#125;/&#123;key-filename&#125; 获取monitor map文件 1ceph mon getmap -o &#123;tmp&#125;/&#123;map-filename&#125; 在新的monitor节点生产monitor所需数据信息 1sudo ceph-mon -i &#123;mon-id&#125; --mkfs --monmap &#123;tmp&#125;/&#123;map-filename&#125; --keyring &#123;tmp&#125;/&#123;key-filename&#125; 添加新的monitor到monitor集群，绑定新的ip地址和port 1ceph-mon -i &#123;mon-id&#125; --public-addr &#123;ip:port&#125; 删除旧的monitor节点步骤如下： 在旧的monitor节点停止该monitor 1service ceph -a stop mon.&#123;mon-id&#125; 把该monitor从集群中移除 1ceph mon remove &#123;mon-id&#125; 方法2修改monitor map里的信息，更新后重启所有monitors注：因为该方法一般是通过整体修改monitor map里的网络信息，然后重启monitors，所以会短暂影响Ceph集群的服务 步骤如下： 获取现在的monitor map 1ceph mon getmap -o &#123;tmp&#125;/&#123;filename&#125; 查看当前的monitor map信息 12345678910$ monmaptool--print&#123;tmp&#125;/&#123;filename&#125; monmaptool: monmap file &#123;tmp&#125;/&#123;filename&#125; epoch 1 fsid 224e376d-c5fe-4504-96bb-ea6332a19e61 last_changed 2012-12-17 02:46:41.591248 created 2012-12-17 02:46:41.591248 0:10.0.0.1:6789/0 mon.a 1:10.0.0.2:6789/0 mon.b 2:10.0.0.3:6789/0 mon.c 删除现有的monitors信息 1234567$ monmaptool --rm a --rm b --rm c &#123;tmp&#125;/&#123;filename&#125; monmaptool: monmap file&#123;tmp&#125;/&#123;filename&#125; monmaptool: removing a monmaptool: removing b monmaptool: removing c monmaptool: writing epoch 1 to &#123;tmp&#125;/&#123;filename&#125; (0 monitors) 添加新的monitors信息 1234$ monmaptool --add a 10.1.0.1:6789--add b 10.1.0.2:6789--add c 10.1.0.3:6789&#123;tmp&#125;/&#123;filename&#125;monmaptool: monmap file&#123;tmp&#125;/&#123;filename&#125;monmaptool: writing epoch 1to &#123;tmp&#125;/&#123;filename&#125; (3 monitors) 检查新的monitors信息 123456789$ monmaptool --print&#123;tmp&#125;/&#123;filename&#125;monmaptool: monmap file&#123;tmp&#125;/&#123;filename&#125;epoch 1 fsid 224e376d-c5fe-4504-96bb-ea6332a19e61last_changed 2012-12-17 02:46:41.591248created 2012-12-17 02:46:41.5912480: 10.1.0.1:6789/0mon.a1: 10.1.0.2:6789/0mon.b2: 10.1.0.3:6789/0mon.c copy修改后的{tmp}/{filename}文件到所有monitor节点 1scp &#123;tmp&#125;/&#123;filename&#125; monitors:~/ 停止所有的monitor 1stop ceph-mon-all 注入新的monitor map信息 1ceph-mon -i &#123;mon-id&#125; --inject-monmap &#123;tmp&#125;/&#123;filename&#125; 启动所有的monitor 1start ceph-mon-all 结语在Ceph的应用中，网络一直是很重要的一块，整个Ceph集群的性能多半情况下会受到网络性能的影响，所以在部署Ceph前，我们需要规划好Ceph使用的网络，包括public和cluster网络。 一般OSD之间数据传输的cluster网络，我们会使用万兆网络，提升数据传输带宽，降低IO延迟；而Monitor，Client使用的public网络，因为数据量比较小，则可以使用普通的千兆网络。]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>osd</tag>
        <tag>monitor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ceph RBD设备分区迁移方案]]></title>
    <url>%2F2016%2F08%2F03%2Fceph-rbd-migration%2F</url>
    <content type="text"><![CDATA[需求在公有云多个IDC的情况下，有时候需要把底层的volume从一个区迁移到另一个区，由于我们底层都使用的是ceph系统，所以这里的迁移方案是基于ceph RBD的。 操作source区操作 获取cinder volume的id 12$ cinder list --all | grep tst-vol| 8097de58-343e-4070-8e83-f7c5541bd26b | 3c9fd84b60694c40bcfeaf1fb4ed3c13 | available | tst-vol2 | 10 | sata | false | cinder volume id: 8097de58-343e-4070-8e83-f7c5541bd26b cinder volume size: 10G cinder volume type: sata 根据volume的类型，获取其对应的ceph pool 普通型：volumes 高性能：volumes_hp 我们系统对volume做了分类，对应不同的Ceph pool，底层使用不同性能的磁盘 通过命令可以看到ceph里存的volume rbd ls -p &lt;volumes/volumes_hp&gt; | grep &lt;volume id&gt; 例如： 12$ rbd ls -p volumes | grep 8097de58-343e-4070-8e83-f7c5541bd26bvolume-8097de58-343e-4070-8e83-f7c5541bd26b 通过rbd export 命令导出数据 rbd export -p &lt;volumes/volumes_hp&gt; &lt;image-name&gt; &lt;path file&gt; 例如： 1rbd export -p volumes volume-8097de58-343e-4070-8e83-f7c5541bd26b backup.volume-8097de58-343e-4070-8e83-f7c5541bd26b 即把volume数据保存到文件：backup.volume-8097de58-343e-4070-8e83-f7c5541bd26b 这里命令：ls -l 查看备份文件的大小为10G但是实际文件size没有10G大小，真实size与volume写入的数据多少有关系，通过命令：du -sh &lt;volume file&gt; 可看到文件真实size，一般比volume真实size多3-4GB存储volume数据文件的fs类型对文件大小也有影响 通过linux的tar命令还可以对数据进行压缩，但这个时间比较长，10G的真实数据，压缩时间约为10分钟，压缩后的size约为 3.2G。 不同压缩算法用的时间和压缩比都不一样，根据实际情况挑选 destination区操作 destination区创建云硬盘 在新的服务区，主机恢复后，创建一个新的同类型的云硬盘，用上面的1，2，3步骤找到对应的image-name 比如对应的image-name为：volume-8c4cc416-b667-4cf2-8a7c-9ff29a23666b 通过rbd命令删除这个image rbd rm -p &lt;volume pool&gt; &lt;volume id&gt; 例如： 1rbd rm -p volumes volume-8c4cc416-b667-4cf2-8a7c-9ff29a23666b 不删除的话，没法执行下一步的数据导入 然后通过命令导入数据 rbd import -p &lt;volumes/volumes_hp&gt; &lt;path file&gt; &lt;image-name&gt; 例如： 1rbd import -p volumes backup.volume-8097de58-343e-4070-8e83-f7c5541bd26b volume-8c4cc416-b667-4cf2-8a7c-9ff29a23666b 正常attach新的云硬盘到主机即可]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>rbd</tag>
        <tag>cinder</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[配置RDS使用本地LVM卷]]></title>
    <url>%2F2016%2F05%2F16%2Frds-with-lvm-configuration%2F</url>
    <content type="text"><![CDATA[为什么RDS使用LVM针对RDS服务，我们提供LVM卷给Docker使用，这样的考虑基于如下几点： RDS做了双主，本身提供了高可用，底层存储不再需要再做高可用 使用PCIE存储卡提供超高性能的存储设备，支撑高性能mysql的需求 减少跨物理机访问存储的网络开销 服务器上部署LVM卷的详情一般我们RDS服务在每个区都有单独的两台物理机提供服务，这是因为： 我们使用Docker而非普通nova instance运行trove instance，nova-docker和普通nova-compute在同一台物理机上都支持有困难（nova，neutron的限制） 两台物理机正好把RDS的双主分配到不同物理机，提高可靠性 在每台物理机上，我们支持三种类型的存储： 普通型 - SATA盘，使用2块4T的SATA盘（为了加快速度，会使用SSD分区做SATA盘的flashcache） 高性能型 - SSD盘，使用3个SSD盘的分区 超高性能型 - PCIE存储卡，使用1个宝存的3.2T的PCIE存储卡 如Server1上： 12345678root@Server1:/home/openstack# pvs PV VG Fmt Attr PSize PFree /dev/dfa volume-group-pcie lvm2 a-- 2.91t 633.44g /dev/sdc4 volume-group-ssd lvm2 a-- 605.18g 144.67g /dev/sdd4 volume-group-ssd lvm2 a-- 605.18g 0 /dev/sde4 volume-group-ssd lvm2 a-- 685.18g 0 /dev/sdl volume-group-sata lvm2 a-- 3.64t 386.48g /dev/sdm volume-group-sata lvm2 a-- 3.64t 0 配置LVM步骤选择合适的磁盘，规划好每个磁盘的使用创建不同类型的pv和vg因为SATA盘的性能限制，所以这里先给SATA盘建立flashcache： 12flashcache_create -p back lvm-sata-cache1 /dev/sdj3 /dev/sdlflashcache_create -p back lvm-sata-cache2 /dev/sdk3 /dev/sdm 创建SATA物理卷组： 123pvcreate /dev/mapper/lvm-sata-cache1pvcreate /dev/mapper/lvm-sata-cache2vgcreate volume-group-sata /dev/mapper/lvm-sata-cache1 /dev/mapper/lvm-sata-cache2 创建SSD物理卷组： 1234pvcreate /dev/sdc4pvcreate /dev/sdd4pvcreate /dev/sde4vgcreate volume-group-ssd /dev/sdc4 /dev/sdd4 /dev/sde4 创建PCIE物理卷组： 12pvcreate /dev/dfavgcreate volume-group-pcie /dev/dfa 创建好的vgs如下： 12345root@Server1:/home/openstack# vgs VG #PV #LV #SN Attr VSize VFree volume-group-pcie 1 5 0 wz--n- 2.91t 633.44g volume-group-sata 2 7 0 wz--n- 7.28t 386.48g volume-group-ssd 3 6 0 wz--n- 1.85t 144.67g cinder配置cinder.conf做如下修改，然后重启服务 123456789101112131415161718# Define the names of the groups for multiple volume backendsenabled_backends = lvm-sata,lvm-ssd,lvm-pcie# Define the groups as above[lvm-sata]volume_group = volume-group-satavolume_driver=cinder.volume.drivers.lvm.LVMISCSIDrivervolume_backend_name=LVM_SATAvolume_clear = none[lvm-ssd]volume_group = volume-group-ssdvolume_driver=cinder.volume.drivers.lvm.LVMISCSIDrivervolume_backend_name=LVM_SSDvolume_clear = none[lvm-pcie]volume_group = volume-group-pcievolume_driver=cinder.volume.drivers.lvm.LVMISCSIDrivervolume_backend_name=LVM_PCIEvolume_clear = none 可能遇到的问题问题：pvcreate命令返回失败 12# pvcreate /dev/dfa Device /dev/dfa not found (or ignored by filtering). 解决办法： 1234567891011121314151617181920212223# pvcreate -vvvv /dev/dfa 2&gt;&amp;1 | less能看到包含如下信息：...#filters/filter.c:109 /dev/dfa: Skipping: Unrecognised LVM device type 251...# cat /proc/devices...Block devices:...251 shannon...# vim /etc/lvm/lvm.conf修改types对应行如下：types = [ "shannon", 251 ]然后可以确认如下：# lvm dumpconfig | grep types types=["shannon", 251]# pvcreate /dev/dfa Physical volume "/dev/dfa" successfully created]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
        <tag>LVM</tag>
        <tag>RDS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph daemon core dump方法]]></title>
    <url>%2F2016%2F05%2F04%2Fceph-daemon-core-dump%2F</url>
    <content type="text"><![CDATA[习惯使用gdb分析的朋友，在研究ceph后，都希望同样能通过gdb分析下ceph dameon，哪如何操作呢？下面介绍下步骤； 设置linux core dump123456789101112131415161718192021222324# echo "ulimit -c 1024" &gt;&gt; /etc/profile然后退出重新登录# ulimit -acore file size (blocks, -c) 1024data seg size (kbytes, -d) unlimitedscheduling priority (-e) 0file size (blocks, -f) unlimitedpending signals (-i) 128434max locked memory (kbytes, -l) 64max memory size (kbytes, -m) unlimitedopen files (-n) 65535pipe size (512 bytes, -p) 8POSIX message queues (bytes, -q) 819200real-time priority (-r) 0stack size (kbytes, -s) 8192cpu time (seconds, -t) unlimitedmax user processes (-u) 128434virtual memory (kbytes, -v) unlimitedfile locks (-x) unlimited# ulimit -c unlimited# echo 1 &gt; /proc/sys/kernel/core_uses_pid# echo "/home/openstack/ceph/core-%e-%p-%t" &gt; /proc/sys/kernel/core_pattern 通过上述方法设置了core dump的限制和规则后，就可以生成linux程序的core dump了 通过ceph-run运行ceph命令12345ceph-run /usr/bin/ceph-osd --cluster=ceph -i 0 -f &amp;# ps ax | grep -w ceph... 94899 ? S 0:00 /bin/sh /usr/bin/ceph-run /usr/bin/ceph-osd --cluster=ceph -i 0 -f 96306 ? Sl 0:01 /usr/bin/ceph-osd --cluster=ceph -i 0 -f 注意：ceph安装包最好自己编译，指定--with-debug 通过kill触发ceph daemon的core dump1kill -3 96306 / kill -4 96306 然后在之前配置的core dump目录下就能看到生成的core dump file比如：/home/openstack/ceph/core-ceph-osd-96306-1450097306 通过gdb来分析core dump1gdb /usr/bin/ceph-osd /home/openstack/ceph/core-ceph-osd-96306-1450097306]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>linux</tag>
        <tag>gdb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Trove源码分析]]></title>
    <url>%2F2016%2F04%2F26%2Ftrove-sourcecode-analyse%2F</url>
    <content type="text"><![CDATA[Trove代码版本：Kilo Trove数据库代码Trove相关数据库表格的代码在：trove/db/sqlalchemy/migrate_repo/versions/目录下： 12345001_base_schema.py002_service_images.py...032_clusters.py033_datastore_parameters.py 创建表格函数：create_tables(...) Trove Createtrove/instance/service.py 123456789class InstanceController(wsgi.Controller): def create(self, req, body, tenant_id): instance = models.Instance.create(context, name, flavor_id, image_id, databases, users, datastore, datastore_version, volume_size, backup_id, availability_zone, nics, configuration, slave_of_id, replica_count=replica_count) trove/instance/models.py 1234567891011121314151617181920212223242526272829class Instance(BuiltInstance): def create(cls, context, name, flavor_id, image_id, databases, users, datastore, datastore_version, volume_size, backup_id, availability_zone=None, nics=None, configuration_id=None, slave_of_id=None, cluster_config=None, replica_count=None): client = create_nova_client(context) ... def _create_resources(): for instance_index in range(0, instance_count): db_info = DBInstance.create(name=name, flavor_id=flavor_id, tenant_id=context.tenant, volume_size=volume_size, datastore_version_id= datastore_version.id, task_status=InstanceTasks.BUILDING, configuration_id=configuration_id, slave_of_id=slave_of_id, cluster_id=cluster_id, shard_id=shard_id, type=instance_type) ... task_api.API(context).create_instance( instance_id, instance_name, flavor, image_id, databases, users, datastore_version.manager, datastore_version.packages, volume_size, backup_id, availability_zone, root_password, nics, overrides, slave_of_id, cluster_config) return SimpleInstance(context, db_info, service_status, root_password) trove/taskmanager/api.py 123456789101112131415161718192021222324class API(object): def create_instance(self, instance_id, name, flavor, image_id, databases, users, datastore_manager, packages, volume_size, backup_id=None, availability_zone=None, root_password=None, nics=None, overrides=None, slave_of_id=None, cluster_config=None): cctxt = self.client.prepare(version=self.version_cap) cctxt.cast(self.context, "create_instance", instance_id=instance_id, name=name, flavor=self._transform_obj(flavor), image_id=image_id, databases=databases, users=users, datastore_manager=datastore_manager, packages=packages, volume_size=volume_size, backup_id=backup_id, availability_zone=availability_zone, root_password=root_password, nics=nics, overrides=overrides, slave_of_id=slave_of_id, cluster_config=cluster_config) trove/taskmanager/manager.py 123456789101112131415161718192021222324252627class Manager(periodic_task.PeriodicTasks): def create_instance(self, context, instance_id, name, flavor, image_id, databases, users, datastore_manager, packages, volume_size, backup_id, availability_zone, root_password, nics, overrides, slave_of_id, cluster_config): if slave_of_id: self._create_replication_slave(context, instance_id, name, flavor, image_id, databases, users, datastore_manager, packages, volume_size, availability_zone, root_password, nics, overrides, slave_of_id, backup_id) else: if type(instance_id) in [list]: raise AttributeError(_( "Cannot create multiple non-replica instances.")) instance_tasks = FreshInstanceTasks.load(context, instance_id) instance_tasks.create_instance(flavor, image_id, databases, users, datastore_manager, packages, volume_size, backup_id, availability_zone, root_password, nics, overrides, cluster_config) timeout = (CONF.restore_usage_timeout if backup_id else CONF.usage_timeout) instance_tasks.wait_for_instance(timeout, flavor) trove/taskmanager/models.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293class FreshInstanceTasks(FreshInstance, NotifyMixin, ConfigurationMixin): def create_instance(self, flavor, image_id, databases, users, datastore_manager, packages, volume_size, backup_id, availability_zone, root_password, nics, overrides, cluster_config, snapshot=None): ... if use_heat: volume_info = self._create_server_volume_heat( flavor, image_id, datastore_manager, volume_size, availability_zone, nics, files) elif use_nova_server_volume: volume_info = self._create_server_volume( flavor['id'], image_id, security_groups, datastore_manager, volume_size, availability_zone, nics, files) else: volume_info = self._create_server_volume_individually( flavor['id'], image_id, security_groups, datastore_manager, volume_size, availability_zone, nics, files) self._guest_prepare(flavor['ram'], volume_info, packages, databases, users, backup_info, config.config_contents, root_password, config_overrides.config_contents, cluster_config, snapshot) def _create_server_volume_individually(self, flavor_id, image_id, security_groups, datastore_manager, volume_size, availability_zone, nics, files): volume_info = self._build_volume_info(datastore_manager, volume_size=volume_size) block_device_mapping = volume_info['block_device'] try: server = self._create_server(flavor_id, image_id, security_groups, datastore_manager, block_device_mapping, availability_zone, nics, files) server_id = server.id # Save server ID. self.update_db(compute_instance_id=server_id) ... return volume_info def _build_volume_info(self, datastore_manager, volume_size=None): if volume_support: try: volume_info = self._create_volume( volume_size, datastore_manager) ... return volume_info def _create_server(self, flavor_id, image_id, security_groups, datastore_manager, block_device_mapping, availability_zone, nics, files=&#123;&#125;): server = self.nova_client.servers.create( name, image_id, flavor_id, files=files, userdata=userdata, security_groups=security_groups, block_device_mapping=bdmap, availability_zone=availability_zone, nics=nics, config_drive=config_drive) return server def _guest_prepare(self, flavor_ram, volume_info, packages, databases, users, backup_info=None, config_contents=None, root_password=None, overrides=None, cluster_config=None, snapshot=None): LOG.debug("Entering guest_prepare") # Now wait for the response from the create to do additional work self.guest.prepare(flavor_ram, packages, databases, users, device_path=volume_info['device_path'], mount_point=volume_info['mount_point'], backup_info=backup_info, config_contents=config_contents, root_password=root_password, overrides=overrides, cluster_config=cluster_config, snapshot=snapshot) Trove Deletetrove/instance/service.py 123class InstanceController(wsgi.Controller): def delete(self, req, tenant_id, id): instance.delete() trove/instance/models.py 1234class BaseInstance(SimpleInstance): def delete(self): def _delete_resources(): task_api.API(self.context).delete_instance(self.id) trove/taskmanager/api.py 1234class API(object): def delete_instance(self, instance_id): cctxt = self.client.prepare(version=self.version_cap) cctxt.cast(self.context, "delete_instance", instance_id=instance_id) trove/taskmanager/manager.py 12345678910class Manager(periodic_task.PeriodicTasks): def delete_instance(self, context, instance_id): try: instance_tasks = models.BuiltInstanceTasks.load(context, instance_id) instance_tasks.delete_async() except exception.UnprocessableEntity: instance_tasks = models.FreshInstanceTasks.load(context, instance_id) instance_tasks.delete_async() trove/instance/models.py 123456789101112class BaseInstance(SimpleInstance): def delete_async(self): deleted_at = datetime.utcnow() self._delete_resources(deleted_at) LOG.debug("Setting instance %s to be deleted.", self.id) self.update_db(deleted=True, deleted_at=deleted_at, task_status=InstanceTasks.NONE) self.set_servicestatus_deleted() # Delete associated security group if CONF.trove_security_groups_support: SecurityGroup.delete_for_instance(self.db_info.id, self.context) trove/taskmanager/models.py 12345678910111213141516171819202122232425class BuiltInstanceTasks(BuiltInstance, NotifyMixin, ConfigurationMixin): def _delete_resources(self, deleted_at): server_id = self.db_info.compute_instance_id old_server = self.nova_client.servers.get(server_id try: self.guest.stop_db() ... try: if use_heat: # Delete the server via heat heatclient = create_heat_client(self.context) name = 'trove-%s' % self.id heatclient.stacks.delete(name) else: self.server.delete() ... # If volume has been resized it must be manually removed in cinder try: if self.volume_id: volume_client = create_cinder_client(self.context) volume = volume_client.volumes.get(self.volume_id) if volume.status == "available": LOG.info(_("Deleting volume %(v)s for instance: %(i)s.") % &#123;'v': self.volume_id, 'i': self.id&#125;) volume.delete() Trove resize volumetrove/instance/service.py 1234567891011121314151617181920class InstanceController(wsgi.Controller): def action(self, req, body, tenant_id, id): _actions = &#123; 'restart': self._action_restart, 'resize': self._action_resize, 'reset_password': self._action_reset_password, 'promote_to_replica_source': self._action_promote_to_replica_source, 'eject_replica_source': self._action_eject_replica_source, &#125; def _action_resize(self, instance, body): options = &#123; 'volume': self._action_resize_volume, 'flavorRef': self._action_resize_flavor &#125; def _action_resize_volume(self, instance, volume): instance.resize_volume(volume['size']) return wsgi.Result(None, 202) trove/instance/models.py 12345678910111213141516class Instance(BuiltInstance): def resize_volume(self, new_size): def _resize_resources(): self.validate_can_perform_action() ... self.update_db(task_status=InstanceTasks.RESIZING) task_api.API(self.context).resize_volume(new_size, self.id) if not self.volume_size: raise exception.BadRequest(_("Instance %s has no volume.") % self.id) new_size_l = long(new_size) validate_volume_size(new_size_l) return run_with_quotas(self.tenant_id, &#123;'volumes': new_size_l - self.volume_size&#125;, _resize_resources) trove/taskmanager/api.py 123456class API(object): def resize_volume(self, new_size, instance_id): cctxt = self.client.prepare(version=self.version_cap) cctxt.cast(self.context, "resize_volume", new_size=new_size, instance_id=instance_id) trove/taskmanager/manager.py 1234class Manager(periodic_task.PeriodicTasks): def resize_volume(self, context, instance_id, new_size): instance_tasks = models.BuiltInstanceTasks.load(context, instance_id) instance_tasks.resize_volume(new_size) trove/taskmanager/models.py 1234class BuiltInstanceTasks(BuiltInstance, NotifyMixin, ConfigurationMixin): def resize_volume(self, new_size): action = ResizeVolumeAction(self, self.volume_size, new_size) action.execute() trove/taskmanager/models.py 12345678910111213141516171819202122232425262728293031class ResizeVolumeAction(object): def execute(self): if self.instance.server.status == InstanceStatus.ACTIVE: self._resize_active_volume() self.instance.reset_task_status() # send usage event for size reported by cinder volume = self.instance.volume_client.volumes.get( self.instance.volume_id) launched_time = timeutils.isotime(self.instance.updated) modified_time = timeutils.isotime(self.instance.updated) self.instance.send_usage_event('modify_volume', old_volume_size=self.old_size, launched_at=launched_time, modify_at=modified_time, volume_size=volume.size) def _resize_active_volume(self): LOG.debug("Begin _resize_active_volume for id: %(id)s" % &#123; 'id': self.instance.id&#125;) self._stop_db() self._unmount_volume(recover_func=self._recover_restart) self._detach_volume(recover_func=self._recover_mount_restart) self._extend(recover_func=self._recover_full) self._verify_extend() # if anything fails after this point, recovery is futile self._attach_volume(recover_func=self._fail) self._resize_fs(recover_func=self._fail) self._mount_volume(recover_func=self._fail) self.instance.restart() LOG.debug("End _resize_active_volume for id: %(id)s" % &#123; 'id': self.instance.id&#125;) Trove resize flavortrove/instance/service.py 123456789101112131415161718192021class InstanceController(wsgi.Controller): def action(self, req, body, tenant_id, id): _actions = &#123; 'restart': self._action_restart, 'resize': self._action_resize, 'reset_password': self._action_reset_password, 'promote_to_replica_source': self._action_promote_to_replica_source, 'eject_replica_source': self._action_eject_replica_source, &#125; def _action_resize(self, instance, body): options = &#123; 'volume': self._action_resize_volume, 'flavorRef': self._action_resize_flavor &#125; def _action_resize_flavor(self, instance, flavorRef): new_flavor_id = utils.get_id_from_href(flavorRef) instance.resize_flavor(new_flavor_id) return wsgi.Result(None, 202) trove/instance/models.py 123456789class Instance(BuiltInstance): def resize_flavor(self, new_flavor_id): self.validate_can_perform_action() ... # Set the task to RESIZING and begin the async call before returning. self.update_db(task_status=InstanceTasks.RESIZING) LOG.debug("Instance %s set to RESIZING.", self.id) task_api.API(self.context).resize_flavor(self.id, old_flavor, new_flavor) trove/taskmanager/api.py 1234567class API(object): def resize_flavor(self, instance_id, old_flavor, new_flavor): cctxt = self.client.prepare(version=self.version_cap) cctxt.cast(self.context, "resize_flavor", instance_id=instance_id, old_flavor=self._transform_obj(old_flavor), new_flavor=self._transform_obj(new_flavor)) trove/taskmanager/manager.py 1234class Manager(periodic_task.PeriodicTasks): def resize_flavor(self, context, instance_id, old_flavor, new_flavor): instance_tasks = models.BuiltInstanceTasks.load(context, instance_id) instance_tasks.resize_flavor(old_flavor, new_flavor) trove/taskmanager/models.py 1234class BuiltInstanceTasks(BuiltInstance, NotifyMixin, ConfigurationMixin): def resize_flavor(self, old_flavor, new_flavor): action = ResizeAction(self, old_flavor, new_flavor) action.execute() trove/taskmanager/models.py 123456789101112131415161718192021222324252627282930313233343536373839class ResizeAction(ResizeActionBase): def _initiate_nova_action(self): self.instance.server.resize(self.new_flavor_id)class ResizeActionBase(object): def _assert_processes_are_ok(self): """Checks the procs; if anything is wrong, reverts the operation.""" # Tell the guest to turn back on, and make sure it can start. self._assert_guest_is_ok() LOG.debug("Nova guest is ok.") self._assert_datastore_is_ok() LOG.debug("Datastore is ok.") def _confirm_nova_action(self): LOG.debug("Instance %s calling Compute confirm resize..." % self.instance.id) self.instance.server.confirm_resize() def execute(self): try: self._assert_datastore_is_offline() self._perform_nova_action() def _perform_nova_action(self): try: LOG.debug("Initiating nova action") self._initiate_nova_action() LOG.debug("Waiting for nova action") self._wait_for_nova_action() LOG.debug("Asserting nova status is ok") self._assert_nova_status_is_ok() need_to_revert = True LOG.debug("* * * REVERT BARRIER PASSED * * *") LOG.debug("Asserting nova action success") self._assert_nova_action_was_successful() LOG.debug("Asserting processes are OK") self._assert_processes_are_ok() LOG.debug("Confirming nova action") self._confirm_nova_action() Trove Backuptrove/backup/models.py 1234class Backup(object): def create(cls, context, instance, name, description=None, parent_id=None): def _create_resources(): api.API(context).create_backup(backup_info, instance_id) trove/guestagent/api.py 123class API(object): def create_backup(self, backup_info): self._cast("create_backup", self.version_cap, backup_info=backup_info) trove/taskmanager/api.py 123456class API(object): def create_backup(self, backup_info, instance_id): cctxt = self.client.prepare(version=self.version_cap) cctxt.cast(self.context, "create_backup", backup_info=backup_info, instance_id=instance_id) trove/taskmanager/manager.py 1234class Manager(periodic_task.PeriodicTasks): def create_backup(self, context, backup_info, instance_id): instance_tasks = models.BuiltInstanceTasks.load(context, instance_id) instance_tasks.create_backup(backup_info) trove/taskmanager/models.py 1234class BuiltInstanceTasks(BuiltInstance, NotifyMixin, ConfigurationMixin): def create_backup(self, backup_info): LOG.info(_("Initiating backup for instance %s.") % self.id) self.guest.create_backup(backup_info) trove/guestagent/datastore/mysql/manager.py 123class Manager(periodic_task.PeriodicTasks): def create_backup(self, context, backup_info): backup.backup(context, backup_info) trove/guestagent/backup/init.py 12def backup(context, backup_info): return AGENT.execute_backup(context, backup_info) trove/guestagent/backup/backupagent.py 12345678910class BackupAgent(object): def execute_backup(self, context, backup_info, runner=RUNNER, extra_opts=EXTRA_OPTS, incremental_runner=INCREMENTAL_RUNNER): storage = get_storage_strategy( CONF.storage_strategy, CONF.storage_namespace)(context) ... self.stream_backup_to_storage(backup_info, runner, storage, parent_metadata, extra_opts) 12345678mysql.backup_incremental_strategy = &#123;'InnoBackupEx': 'InnoBackupExIncremental'&#125;mysql.backup_namespace = trove.guestagent.strategies.backup.mysql_implmysql.backup_strategy = InnoBackupExmysql.device_path = /dev/vdbmysql.mount_point = /var/lib/mysqlmysql.replication_namespace = trove.guestagent.strategies.replication.mysql_gtidmysql.replication_strategy = MysqlGTIDReplicationmysql.restore_namespace = trove.guestagent.strategies.restore.mysql_impl]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
        <tag>trove</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux fstab配置]]></title>
    <url>%2F2016%2F04%2F18%2Flinux-fstab-uuid%2F</url>
    <content type="text"><![CDATA[在应用中我们经常需要在linux系统启动后自动mount文件系统，这就要用到fstab。 fstab文件中包含了各种各样的文件系统描述信息，fstab中每一个文件系统描述占一行，每一行是TAB或空格分隔。 123456789# cat /etc/fstab# /etc/fstab: static file system information.## Use 'blkid' to print the universally unique identifier for a# device; this may be used with UUID= as a more robust way to name devices# that works even if disks are added and removed. See fstab(5).## &lt;file system&gt; &lt;mount point&gt; &lt;type&gt; &lt;options&gt; &lt;dump&gt; &lt;pass&gt;/dev/mapper/lvm-lv_root / ext4 errors=remount-ro 0 1 因为在linux中有磁盘漂浮的问题，所以为了防止盘符漂移，建议使用UUID代替盘符来配置fstab。步骤如下： 获取磁盘的UUID执行blkid | grep vdb1获取磁盘对应的UUID： 1/dev/vdb1: UUID="fac80332-85bf-4899-80fd-42beb49cf23b" TYPE="ext3" fstab写入fs信息1echo 'UUID=fac80332-85bf-4899-80fd-42beb49cf23b /mnt/vdb ext3 defaults 0 0' &gt;&gt; /etc/fstab 执行cat /etc/fstab查看是否写入成功。 监测fstab中的信息执行mount -a检查并mount /etc/fstab中配置的所有分区。这样能避免写错fstab后导致linux系统启动失败。 重启系统验证重启linux系统，待系统启动后通过mount命令查看是否配置fs自动mount成功。 fstab中各个参数的含义fstab中每个fs后面的几个参数的含义需要搞清楚，特别是后面的两个参数，配置错误可能导致系统启动非常慢或者失败的。 &lt;file system&gt; &lt;dir&gt; &lt;type&gt; &lt;options&gt; &lt;dump&gt; &lt;pass&gt; 其中&lt;dump&gt;，&lt;pass&gt;参数解释如下： &lt;dump&gt; dump工具通过它决定何时作备份。 dump会检查其内容，并用数字来决定是否对这个文件系统进行备份。 允许的数字是0/1。0表示忽略，1则进行备份。 大部分的用户是没有安装dump的，则&lt;dump&gt;应设为0。 &lt;pass&gt; fsck读取&lt;pass&gt;的数值来决定需要检查的文件系统的检查顺序。 允许的数字是0/1/2。 根目录应当获得最高的优先权1，其它所有需要被检查的设备设置为2。0表示设备不会被fsck检查。 【建议配置为0，大容量硬盘开机fsck检查会比较慢】]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>fstab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[创建trove使用的glance image]]></title>
    <url>%2F2016%2F04%2F06%2Ftrove-glance-image-creation%2F</url>
    <content type="text"><![CDATA[使用Trove提供database服务，首先我们需要制作一个Trove对应database服务的glance image，这样才能基于该image启动虚拟机提供database服务； 本文档以创建mysql 5.6为例，描述下制作Trove对应服务的glance image的步骤： 选择base系统，启动虚拟机 比如选择基本的Ubuntu 14.04镜像，启动一个nova instance 配置apt-get源 登陆nova instance上，设置合适的apt-get源 安装相应软件包 比如安装mysql-server-5.6，安装mysql备份插件xtrabackup，安装其他软件：比如keepalived 创建调试用户 默认openstack创建nova instance的root账号是随机密码，所以这里添加一个ubuntu的user，设置密码和super权限，方便之后登陆nova instance查看调试 配置trove运行环境 copy trove部署机器上的 .venv/ 目录到虚拟机 /home/ubuntu/ 目录下，提供python的运行环境 获取trove源码 copy trove部署机器上的 /opt/openstace/trove 目录到虚拟机 /home/ubuntu/ 目录下； 若使用trove源码，需要执行trove代码的安装： 12345cd /opt/openstack/trovegit checkout stable/kilopython setup.py egg_infopip install -r trove.egg-info/requires.txtpython setup.py develop 查看配置trove的开机自启动 12345678$ cat /etc/init/trove-guestagent.confdescription "Trove Guest Agent"author "Thomas Goirand &lt;zigo@debian.org&gt;"start on runlevel [2345]stop on runlevel [!2345]respawn exec /home/ubuntu/trove/contrib/trove-guestagent --config-dir=/etc/trove/conf.d 配置ceilometer监控代码 copy ceilometer的相关代码到 /home/ubuntu/ 目录下 copy ceilometer的agent程序、配置文件、启动脚本（根据业务自己写）到指定目录 注：ceilometer的监控看自己项目需要 配置ceilometer的开机自启动 12345678910111213141516$ cat /etc/rc.local#!/bin/sh -e## rc.local## This script is executed at the end of each multiuser runlevel.# Make sure that the script will "exit 0" on success or any other# value on error.## In order to enable or disable this script just change the execution# bits.## By default this script does nothing. bash /home/ubuntu/ceilometer/start-ceilometer.shexit 0 配置nova instance启动自动更新trove代码 在调试的情况下，可以每次在nova instance启动时候更新trove代码，这样不用每次都重做glance image； 步骤如下： 把nova instance的id_rsa.pub文件的内容添加到放置trove代码的服务器的authorized_keys里； 在nova instance的rc.local文件中添加：scp -o StrictHostKeyChecking=no -r root@&lt;serverip&gt;:/opt/openstack/trove/trove /home/ubuntu/trove/ 创建glance镜像 退出nova instance，通过nova show 查找instance所在物理机，然后通过qemu-img命令导出instance为一个镜像； 1root@compute01:~# qemu-img convert -O qcow2 /var/lib/nova/instances/a61216f6-eacc-45b3-a6a7-9d1829f6de99/disk ~/new.image 上传镜像到glance中 1234567891011121314151617181920212223root@compute01:~# glance --debug image-create --name "mysql-ubuntu" --file ~/new.image --disk-format qcow2 --container-format bare --is-public True --progress...+------------------+--------------------------------------+| Property | Value |+------------------+--------------------------------------+| checksum | 31b45bf9d9e8c2e9a6215f2912c22113 || container_format | bare || created_at | 2016-02-27T07:00:26.000000 || deleted | False || deleted_at | None || disk_format | qcow2 || id | 296c7baf-5041-4fd5-8640-1b3bc03e5236 || is_public | True || min_disk | 0 || min_ram | 0 || name | mysql-ubuntu || owner | None || protected | False || size | 2938568704 || status | active || updated_at | 2016-02-27T07:02:56.000000 || virtual_size | None |+------------------+--------------------------------------+ 更新trove的mysql的datastore后即可使用该镜像创建Trove实例了 注：trove是支持使用cinder volume的，所以这里要配置好cinder volume type与nova instance运行机器的对应关系；]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
        <tag>trove</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack DBBS Trove部署]]></title>
    <url>%2F2016%2F03%2F28%2Ftrove-deploy%2F</url>
    <content type="text"><![CDATA[本文是基于Trove Kilo版本部署的 配置环境变量source /opt/osdeploy/admin_openrc.sh 下载源码12git clone https://github.com/openstack/trove.git /opt/openstack/trovegit clone https://github.com/openstack/python-troveclient.git /opt/openstack/python-troveclient 安装 troveclient12345cd /opt/openstack/python-troveclientgit checkout stable/kilopython setup.py egg_infopip install -r python_troveclient.egg-info/requires.txtpython setup.py develop 安装 trove kilo版本12345cd /opt/openstack/trovegit checkout stable/kilopython setup.py egg_infopip install -r trove.egg-info/requires.txtpython setup.py develop 问题 IOError: [Errno 2] No such file or directory: ‘/opt/openstack/.venv/build/oslo.serialization/oslo.serialization-2.2.0.dist-info/METADATA’ 解决 把build下面的所有东西都删除就行了。rm -rf /opt/openstack/.venv/build/* 创建用户创建trove用户，trove用户添加admin角色，创建trove服务，给 trove 服务添加 api 访问地址 1234openstack user create --password 22116ccf32a troveopenstack role add --project service --user trove adminopenstack service create --name trove --description "OpenStack Trove" databaseopenstack endpoint create --publicurl 'http://10.10.1.1:8779/v1.0/$(tenant_id)s' --internalurl 'http://10.10.1.1:8779/v1.0/$(tenant_id)s' --adminurl 'http://10.10.1.1:8779/v1.0/$(tenant_id)s' --region RegionOne database 创建trove相关目录，并修改权限123mkdir /opt/etc/trove; chown openstack:openstack /opt/etc/trovemkdir /var/log/cloud/openstack/trove; chown openstack:openstack /var/log/cloud/openstack/trovemkdir /var/lib/trove; chown openstack:openstack /var/lib/trove 使用mysql作为trove后端db在mysql的机器上执行：（查看/opt/etc/nova/nova.conf找到mysql服务所在机器） 123mysql -uroot -p&gt; create database trove default character set utf8;&gt; grant all on trove.* to 'trove'@'%' identified by 'trove'; 注：需要输入mysql的密码 拷贝trove相关配置到/opt/etc/trove目录12345cp /opt/openstack/trove/etc/trove/api-paste.ini /opt/etc/trove/api-paste.inicp /opt/openstack/trove/etc/trove/trove.conf.sample /opt/etc/trove/trove.confcp /opt/openstack/trove/etc/trove/trove-taskmanager.conf.sample /opt/etc/trove/trove-taskmanager.confcp /opt/openstack/trove/etc/trove/trove-conductor.conf.sample /opt/etc/trove/trove-conductor.confcp -r /opt/openstack/trove/etc/trove/cloudinit/ /opt/etc/trove/ 修改/opt/etc/trove目录里的conf文件 trove.conf 1234567891011121314151617181920212223rabbit_hosts = 127.0.0.1:5672rabbit_userid=openstackrabbit_password=iafdhewak2trove_auth_url = http://127.0.0.1:5000/v2.0nova_compute_url = http://127.0.0.1:8774/v2cinder_url = http://127.0.0.1:8776/v2neutron_url = http://127.0.0.1:9696/log_dir = /var/log/openstack/trovelog_file = trove-api.logconnection = mysql://trove:trove@127.0.0.1/trove[keystone_authtoken]auth_uri = http://127.0.0.1:5000auth_url = http://127.0.0.1:35357auth_plugin = passwordproject_domain_id = defaultuser_domain_id = defaultproject_name = serviceusername = trovepassword = 221***6d42d8ca trove-taskmanager.conf 12345678910111213141516171819rabbit_host=127.0.0.1rabbit_userid=openstackrabbit_password=iafdhewak2trove_auth_url = http://127.0.0.1:5000/v2.0nova_compute_url = http://127.0.0.1:8774/v2cinder_url = http://127.0.0.1:8776/v2swift_url = http://127.0.0.1:8080/v1/AUTH_neutron_url = http://127.0.0.1:9696/log_dir = /var/log/openstack/trovelog_file = trove-taskmanager.log# ================= Guestagent related ========================guest_config = /opt/etc/trove/trove-guestagent.confinjected_config_location = /etc/trove/conf.dcloudinit_location = /opt/etc/trove/cloudinitconnection = mysql://trove:trove@127.0.0.1/trove trove-conductor.conf 12345trove_auth_url = http://127.0.0.1:5000/v2.0rabbit_host = 127.0.0.1rabbit_userid = openstackrabbit_password = iafdhewak2connection = mysql://trove:trove@127.0.0.1/trove trove-guestagent.conf 12345678910111213141516171819202122232425rabbit_hosts = 192.168.1.3:5672rabbit_userid = openstackrabbit_password = iafdhewak2swift_url = http://192.168.1.3:8080/v1/AUTH_os_region_name = RegionOneswift_service_type = object-storedatastore_registry_ext = mysql:trove.guestagent.datastore.mysql.manager.Manager, percona:trove.guestagent.datastore.mysql.manager.Managerignore_users = os_adminignore_dbs = lost+found, mysql, information_schema, performance_schemaroot_grant = ALLroot_grant_option = Truestorage_strategy = SwiftStoragestorage_namespace = trove.guestagent.strategies.storage.swiftbackup_swift_container = database_backups[keystone_authtoken]auth_uri = http://192.168.1.3:5000auth_url = http://192.168.1.3:35357auth_plugin = passwordproject_name = serviceproject_domain_id = defaultuser_domain_id = defaultusername = trovepassword = 221***6d42d8ca 添加包含mysql的ubuntu镜像到glance里12345678glance --debug image-create --name "mysql-ubuntu" --file ~/mysql.qcow2 --disk-format qcow2 --container-format bare --is-public True --progress(.venv)openstack@cloud01:~$ glance image-list+--------------------------------------+---------------------+-------------+------------------+-----------+--------+| ID | Name | Disk Format | Container Format | Size | Status |+--------------------------------------+---------------------+-------------+------------------+-----------+--------+| 0ad5a403-09fa-4be3-81f8-74caac155be0 | mysql-ubuntu | qcow2 | bare | 533397504 | active |+--------------------------------------+---------------------+-------------+------------------+-----------+--------+ 初始化trove database因为mysql外键的问题，可能需要执行括号内的额外步骤： 123[mysql -uroot -ppwd -e"use trove;SET GLOBAL foreign_key_checks=0;"]trove-manage --config-file=/opt/etc/trove/trove.conf db_sync[mysql -uroot -ppwd -e"use trove;SET GLOBAL foreign_key_checks=1;"] 初始化Trove Datastores12345678export DATASTORE_TYPE="mysql"export DATASTORE_VERSION="5.6"export PACKAGES="mysql-server-5.6"export IMAGEID="0ad5a403-09fa-4be3-81f8-74caac155be0"trove-manage --config-file /opt/etc/trove/trove.conf datastore_update $&#123;DATASTORE_TYPE&#125; ""trove-manage --config-file /opt/etc/trove/trove.conf datastore_version_update $&#123;DATASTORE_TYPE&#125; $&#123;DATASTORE_VERSION&#125; $&#123;DATASTORE_TYPE&#125; $&#123;IMAGEID&#125; $&#123;PACKAGES&#125; 1trove-manage --config-file /opt/etc/trove/trove.conf datastore_update $&#123;DATASTORE_TYPE&#125; $&#123;DATASTORE_VERSION&#125;trove datastore-version-show $&#123;DATASTORE_VERSION&#125; --datastore $&#123;DATASTORE_TYPE&#125; 重启trove service1service trove-api restart; service trove-taskmanager restart; service trove-conductor restart; 配置datastore的默认值修改trove.conf，在[default]字段添加： 1default_datastore = mysql 查看datastore mysql的default version值： 1234567891011$ trove datastore-show mysql...| default_version | 9b8987aa-282b-46b9-af19-900aadfa8d9b |...$ trove datastore-version-list mysql+--------------------------------------+------+| ID | Name |+--------------------------------------+------+| 1832c3bd-3233-45ef-b6fe-c00fc6b4750e | 5.7 || 9b8987aa-282b-46b9-af19-900aadfa8d9b | 5.6 |+--------------------------------------+------+ 配置mysql, 5.6版本的config parameters123456789101112131415161718192021222324252627282930313233343536trove-manage --config-file /opt/etc/trove/trove.conf db_load_datastore_config_parameters mysql 5.6 /opt/openstack/trove/trove/templates/mysql/validation-rules.json$ openstack endpoint show trove+--------------+------------------------------------------+| Field | Value |+--------------+------------------------------------------+| adminurl | http://127.0.0.1:8779/v1.0/$(tenant_id)s || enabled | True || id | 677e119b1309446590e174f1f2fb2d80 || internalurl | http://127.0.0.1:8779/v1.0/$(tenant_id)s || publicurl | http://127.0.0.1:8779/v1.0/$(tenant_id)s || region | RegionOne || service_id | 4f456705ce124c62918f29b4b70b5ed1 || service_name | trove || service_type | database |+--------------+------------------------------------------+(.venv)openstack@cloud01:~$ trove datastore-list+--------------------------------------+-------+| ID | Name |+--------------------------------------+-------+| 2f2c8eee-cac5-4a52-9f89-9c39a140f611 | mysql |+--------------------------------------+-------+(.venv)openstack@cloud01:~$ trove create myFirstDB 1 --size 2 --databases trovedb --users ictfox:ictfox --datastore_version 5.6 --datastore mysql+-------------------+--------------------------------------+| Property | Value |+-------------------+--------------------------------------+| created | 2016-01-22T11:41:48 || datastore | mysql || datastore_version | 5.6 || flavor | 2 || id | 31dbea3e-1117-407e-82f0-cdf7ecd3ac3b || name | myFirstDB || status | BUILD || updated | 2016-01-22T11:41:48 || volume | 2 |+-------------------+--------------------------------------+ 参考资料http://docs.openstack.org/kilo/config-reference/content/ch_configuring-trove.htmlhttp://docs.openstack.org/developer/trove/dev/manual_install.htmlhttps://github.com/openstack/trove/blob/master/doc/source/dev/manual_install.rsthttps://www.youtube.com/watch?v=zk42zzc_38ohttp://blog.csdn.net/myproudcodelife/article/details/39839891]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
        <tag>trove</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack DBBS Trove Introduction]]></title>
    <url>%2F2016%2F03%2F18%2Ftrove-introduction%2F</url>
    <content type="text"><![CDATA[Trove简介Openstack Trove项目是Openstack开源代码中提供Database As A Service的项目，它的目的是提供一个Database的框架，集成现有常见的关系型数据库、非关系型数据库。 现在Trove项目支持的数据库有：Mysql，Redis，MoogoDB，Cassandra，Couchbase，Couchdb， DB2，Postgresql，Vertica； 上述数据库中，Mysql的支持是最完善和最稳定的，别的数据库多半还在试验阶段，有些仅能提供一些数据库操作的基本功能； Trove Docs：https://docs.openstack.org/developer/trove/Trove Wiki：https://wiki.openstack.org/wiki/TroveTrove源码：https://github.com/openstack/trove Openstack Trove基于Openstack的别的模块来提供Dbaas的服务，使用到的模块有：Glance，Nova，Cinder，Neutron，Keystone，Swift； 注释：本文对应的Trove版本为Kilo Trove架构Openstack Trove的架构如下： Trove Instance：指的是通过trove命令创建的一个实例，如上图所示，它实际上是一个nova instance，在里面运行了指定的database服务，同时它也会跟glance、cinder、neutron、swift打交道。 Trove有四个基本的模块，简介如下： Trove-APITrove的API模块，接受Trove的命令来操作数据库，然后异步交给Trove-Taskmanager执行，中间会保存Trove Instance状态到database中； Trove-TaskmanagerTrove的任务管理模块，主要接受Trove-API发来的请求，Trove执行的主要逻辑都在这里实现，中间会更新Trove Instance状态到database中； Trove-ConductorTrove的信息接收模块，它接收Trove Instance的心跳信息，把Trove Instance的状态信息保存到trove使用的database中； Trove-GuestagentTrove的客户端代理模块，它运行在每个Trove Instance上，接收Trove-Taskmanger的命令并执行，上报Trove Instance的状态信息到Trove-Conductor； Trove的四个模块之间通信也是使用Rabbitmq，可以使用openstack其他模块使用的Rabbitmq，也可以单独部署Rabbitmq使用； Trove命令Trove服务都是通过CLI命令或者Restful请求执行的，支持的命令有如下几类： 1. Instance操作* create：创建一个trove实例 * delete：删除一个trove实例 * resize-instance：resize trove实例到新的flavor * resize-volume：resize trove实例的volume到更大值 * restart：重启一个trove实例 * show：显示一个trove实例的detail信息 * update：更新一个trove实例的信息 2. datastore操作datastore的创建和更新是通过 trove-manager 命令生成的； * datastore-list：显示有哪些datastore * datastore-show：显示一个datastore的detail信息 * datastore-version-list：显示一个datastore的version list * datastore-version-show：显示一个datastore的一个version的detail信息 3. backup操作trove提供备份instance到swift的操作，支持全量备份和增量备份，看具体database的实现； * backup-create：创建一个instance的backup * backup-delete：删除指定ID的backup * backup-list：列出可用的所有backups * backup-list-instance：列出指定instance的可用的所有backups * backup-show：显示指定ID的backu的detail信息 * backup-copy：从一个backup copy生成一个新的backup 4. cluster操作针对有些database才有cluster的概念，比如MoogoDB； * cluster-create：创建一个新的cluster * cluster-delete：删除一个cluster * cluster-instances：列出一个cluster的所有instances * cluster-list：列出所有的clusters * cluster-show：显示指定ID的cluster的detail信息 5. configuration group操作trove提出了配置组的概念，这是为了是用户可以定制不同instance的配置参数，针对不同的database，支持的配置参数也不相同，详细请参考代码； 比如mysql支持的配置参数定义在：trove/templates/mysql/validation-rules.json trove限制每个instance只能配置一个configuration group * configuration-create：创建一个新的configuration group * configuration-delete：删除一个configuration group * configuration-attach：attach一个configuration group到一个trove instance上 * configuration-detach：detach一个trove instance上的configuration group * configuration-default：显示一个trove instance的默认configuration group * configuration-instances：显示绑定到一个configuration group上的所有trove instances * configuration-list：显示所有的configuration group * configuration-show：显示一个configuration group的detail信息 * configuration-parameter-list：列出指定version的datastore支持的configuration group配置参数 * configuration-parameter-show：显示指定version的datastore支持的configuration group的某一项配置的详细信息 * configuration-patch：把新的&lt;values&gt; patch到一个configuration group * configuration-update：更新一个configuration group的信息 6. replica操作为了支持database的高可用，trove提供命令来创建一个trove instance的replica instance，并根据不同的database做不同的配置； 通过create命令创建一个instance的replica instance trove create [--replica_of &lt;source_instance&gt;] [--replica_count &lt;count&gt;] 7. user操作trove支持创建一个instance的user，并支持赋予/收回 user访问databases的权限； * user-create：创建一个instance的user * user-delete：删除一个instance的user * user-grant-access：赋予user访问database（可以同时指定多个）的权限 * user-revoke-access：收回user访问database的权限 * user-list：list一个instance的所有users * user-show：显示一个instance指定user的detail信息 * user-show-access：显示一个instance指定user访问database的权限信息 8. database操作trove支持在一个trove instance上有多个database； * database-create：在一个instance上创建database * database-delete：删除一个instance上的database * database-list：list一个instance上的所有databases 9. metadata操作这部分应该是给trove instance打些特定标签使用的，我们使用中并没用到； Trove代码结构下载完trove代码后进入trove/目录，看到如下的目录结构： 基本可以通过目录的命名就知道每个目录中代码的作用了，下面列举几个常用目录的代码部分： cluster trove cluster实例的相关代码； conductor trove conductor模块的相关代码； taskmanager trove taskmanager模块的相关代码，这部分代码比较重要； configuration trove configuration group逻辑的相关代码； guestagent trove guestagent模块的相关代码，不同数据库的不同实现都在该目录下，目录结构如下： templates trove支持的各个database的模板文件，包括配置模板和configuration group模板； backup trove backup逻辑的相关代码； datastore trove datastore逻辑的相关代码； instance trove instance的相关代码； 创建Trove实例的过程这里通过trove create一个instance的过程，来描述trove是如何工作的； 通过trove命令触发create操作； 123456789101112131415usage: trove create &lt;name&gt; &lt;flavor_id&gt; [--size &lt;size&gt;] [--volume_type &lt;type&gt;] [--databases &lt;databases&gt; [&lt;databases&gt; ...]] [--users &lt;users&gt; [&lt;users&gt; ...]] [--backup &lt;backup&gt;] [--availability_zone &lt;availability_zone&gt;] [--datastore &lt;datastore&gt;] [--datastore_version &lt;datastore_version&gt;] [--nic &lt;net-id=net-uuid,v4-fixed-ip=ip-addr,port-id=port-uuid&gt;] [--configuration &lt;configuration&gt;] [--replica_of &lt;source_instance&gt;] [--replica_count &lt;count&gt;] [--virtual_ip &lt;vip&gt;] [--vrid &lt;vrid&gt;] [--sec_group_id &lt;sec_group_id&gt;] [--root_password &lt;root_password&gt;]error: too few argumentsTry 'trove help create' for more information. trove api收到create一个instance的请求，创建一个trove instance的实例，并把实例状态置为：building，写入database； 12345678910111213 def _create_resources():... db_info = DBInstance.create(name=instance_name, flavor_id=flavor_id, tenant_id=context.tenant, volume_size=volume_size, datastore_version_id=datastore_version.id, task_status=InstanceTasks.BUILDING, configuration_id=configuration_id, slave_of_id=slave_of_id, type=volume_type, virtual_ip_vrid=virtual_ip_vrid)... trove api发送异步请求给trove taskmanager，然后返回给用户，此时用户通过trove list命令可以看到trove instance的状态为building； 12345678910111213141516171819202122232425def create_instance(self, instance_id, name, flavor, image_id, databases, users, datastore_manager, packages, volume_size, backup_id=None, availability_zone=None, root_password=None, nics=None, overrides=None, slave_of_id=None, cluster_config=None, volume_type=None): LOG.debug("Making async call to create instance %s " % instance_id) cctxt = self.client.prepare(version=self.version_cap) cctxt.cast(self.context, "create_instance", instance_id=instance_id, name=name, flavor=self._transform_obj(flavor), image_id=image_id, databases=databases, users=users, datastore_manager=datastore_manager, packages=packages, volume_size=volume_size, backup_id=backup_id, availability_zone=availability_zone, root_password=root_password, nics=nics, overrides=overrides, slave_of_id=slave_of_id, cluster_config=cluster_config, volume_type=volume_type) trove taskmanager根据参数调用cinder接口创建 –size 指定的volume，并等待返回；若出错则置实例状态为特定error； 1234567891011 def _build_volume_info(self, datastore_manager, volume_size=None, volume_type=None, master_vol_host=None):... if volume_support: try: volume_info = self._create_volume(volume_size, datastore_manager, volume_type, master_vol_host) except Exception as e:... trove taskmanager根据参数调用nova接口创建nova instance；会传入 cinder volume信息，glance image信息，网络信息等等；并等待nova instance创建成功返回；若出错则置实例状态为特定error； 1234567891011121314151617def _create_server(self, flavor, image_id, security_groups, datastore_manager, block_device_mapping, availability_zone, nics, files=&#123;&#125;): userdata = self._prepare_userdata(datastore_manager, flavor) name = self.hostname or self.name bdmap = block_device_mapping config_drive = CONF.use_nova_server_config_drive server = self.nova_client.servers.create( name, image_id, flavor['id'], files=files, userdata=userdata, security_groups=security_groups, block_device_mapping=bdmap, availability_zone=availability_zone, nics=nics, config_drive=config_drive) LOG.debug("Created new compute instance %(server_id)s " "for instance %(id)s" % &#123;'server_id': server.id, 'id': self.id&#125;) return server trove taskmanager调用_guest_prepare发请求给trove instance里面的guestagent进程； 123456789101112131415def _guest_prepare(self, flavor_ram, volume_info, packages, databases, users, backup_info=None, config_contents=None, root_password=None, overrides=None, cluster_config=None, snapshot=None): LOG.debug("Entering guest_prepare") # Now wait for the response from the create to do additional work self.guest.prepare(flavor_ram, packages, databases, users, device_path=volume_info['device_path'], mount_point=volume_info['mount_point'], backup_info=backup_info, config_contents=config_contents, root_password=root_password, overrides=overrides, cluster_config=cluster_config, snapshot=snapshot) trove guestagent收到请求后，会根据不同database中guestagent的实现做不同操作 以mysql为例，mysql的guestagent会做如下操作： 123456789101112* stop database* cinder volume格式化，并mount到指定目录* 把之前database目录下的东西迁移到cinder volume mount的目录* start database* 根据传入的参数决定是否做：create database，create user，enable replica的操作* 发送消息给trove conductor修改trove instance状态为runningtrove/guestagent/datastore/mysql/manager.py文件： def prepare(self, context, packages, databases, memory_mb, users, device_path=None, mount_point=None, backup_info=None, config_contents=None, root_password=None, overrides=None, cluster_config=None, snapshot=None): trove创建instance的过程完成，此时用户通过trove list命令可以看到trove instance的状态为active； 参考资料https://www.openstack.org/summit/openstack-summit-atlanta-2014/session-videos/presentation/introduction-to-openstack-trove-a-multi-database-deployment-with-mongodb-and-mysqlhttp://www.slideshare.net/ToruMakabe/openstack-trovehttp://tech.it168.com/a2016/0407/2587/000002587322.shtmlhttp://blog.csdn.net/myproudcodelife/article/details/39839891http://www.odbms.org/2016/02/10-things-you-should-know-about-openstack-trove-the-open-source-database-as-a-service/]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
        <tag>trove</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ceph配置rbd client端的log file]]></title>
    <url>%2F2015%2F12%2F10%2Flibrbd-log-file-configure%2F</url>
    <content type="text"><![CDATA[概述openstack cinder通过librbd访问ceph rbd块设备，有时候需要查看rbd client端的log信息，这时候就需要在rbd client端配置ceph.conf文件。 因为我们openstack里的nova是通过libvirt来启动虚拟机的，所以这里需要保证libvirt对配置的log file目录有写权限。 ceph配置12345678910111213141516171819202122232425262728配置rbd client的cache，debug配置和log file[client] rbd cache = true rbd cache size = 67108864 rbd cache max dirty = 33554432 rbd cache max dirty age = 5 rbd cache writethrough until flush = true rbd_default_order = 25 rbd_default_stripe_unit = 4194304 rbd_default_stripe_count = 8 admin socket = /var/run/ceph/ceph-client/$cluster-$type.$id.$pid.asok debug rbd = 20 debug client = 20 debug objectcacher = 20 log file = /var/run/ceph/ceph-client/$cluster-$name.log配置各个rbd client的keyring和log file[client.images] keyring = /etc/ceph/images.keyring log file = /var/run/ceph/ceph-client/images.log[client.volumes] keyring = /etc/ceph/volumes.keyring log file = /var/run/ceph/ceph-client/volumes.log[client.cinder] keyring = /etc/ceph/cinder.keyring log file = /var/run/ceph/ceph-client/cinder.log 代码分析默认情况下client端的log file等设置是为空的。]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>rbd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack cinder configuration with multi ceph pools]]></title>
    <url>%2F2015%2F11%2F17%2Fcinder-with-multi-ceph-pools%2F</url>
    <content type="text"><![CDATA[添加rbd相关配置文件：/opt/etc/cinder/cinder.conf 删除之前cinder中rbd的配置，然后添加如下配置： 1234567891011121314151617# Define the names of the groups for multiple volume backendsenabled_backends = rbd-sata,rbd-ssd# Define the groups as above[rbd-sata]volume_driver = cinder.volume.drivers.rbd.RBDDriverrbd_user = cinderrbd_pool = volumesvolume_backend_name = RBD_SATArbd_ceph_conf = /etc/ceph/ceph.conf[rbd-ssd]volume_driver = cinder.volume.drivers.rbd.RBDDriverrbd_user = cinderrbd_pool = volumes_hpvolume_backend_name = RBD_SSDrbd_ceph_conf = /etc/ceph/ceph.conf rbd的其他如下配置可以根据需要添加到不同的backend设置中。 12345678rbd_flatten_volume_from_snapshot = falserbd_max_clone_depth = 5rbd_store_chunk_size = 4rados_connect_timeout = -1# if cephX is enablerbd_user=cinderrbd_secret_uuid=&lt;None&gt; 同一backend用多个pools若要支持同一个backend用多个pools，可以类似如下配置： 1234567891011121314enabled_backends = rbd-sata,rbd-ssd-1,rbd-ssd-2[rbd-ssd-1]volume_driver = cinder.volume.drivers.rbd.RBDDriverrbd_user = cinderrbd_pool = volumes_hp-1volume_backend_name = RBD_SSDrbd_ceph_conf = /etc/ceph/ceph.conf[rbd-ssd-2]volume_driver = cinder.volume.drivers.rbd.RBDDriverrbd_user = cinderrbd_pool = volumes_hp-2volume_backend_name = RBD_SSDrbd_ceph_conf = /etc/ceph/ceph.conf 添加LVM支持cinder添加LVM支持，类似如下配置： 因为LVM是固定在每个host上的，每个host的cinder.conf文件也只需要配置添加本地的LVM信息即可。 123456789101112# Define the names of the groups for multiple volume backendsenabled_backends = rbd-sata,rbd-ssd,lvm-1,lvm-2 ...[lvm-1]volume_group = volume-group1volume_driver=cinder.volume.drivers.lvm.LVMISCSIDrivervolume_backend_name=LVM_iSCSI[lvm-2]volume_group = volume-group2volume_driver=cinder.volume.drivers.lvm.LVMISCSIDrivervolume_backend_name=LVM_iSCSI 添加cinder对应lvm的类型和backend： 123cinder type-create lvmcinder type-key lvm set volume_backend_name=LVM_iSCSIcinder extra-specs-list 然后重启cinder所有服务 1234service cinder-api restartservice cinder-scheduler restartservice cinder-volume restartservice cinder-backup restart 若还不行，尝试执行下面的命令：安装 cinder 相关包 1# apt-get install -y lvm2 tgt 问题有一个错误跟cinder-rootwrap有关系，代码中会调用 sudo cinder-rootwrap命令：若不是用root用户部署的，就需要创建如下的一个软连接： 1ln -s /opt/openstack/.venv/bin/cinder-rootwrap /usr/bin/cinder-rootwrap]]></content>
      <categories>
        <category>openstack</category>
      </categories>
      <tags>
        <tag>openstack</tag>
        <tag>cinder</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ceph RBD image map rules]]></title>
    <url>%2F2015%2F11%2F12%2Frbd-image-map-rules%2F</url>
    <content type="text"><![CDATA[代码定义Ceph版本：Hammer 0.94.5 12345678/* New-style rbd image 'foo' consists of objects * rbd_id.foo - id of image * rbd_header.&lt;id&gt; - image metadata * rbd_object_map.&lt;id&gt; - optional image object map * rbd_data.&lt;id&gt;.00000000 * rbd_data.&lt;id&gt;.00000001 * ... - data */ 从中可以看出一个rbd image会对应至少4个文件： rbd_id.foo - foo为image的name rbd_header.&lt;id&gt; rbd_object_map.&lt;id&gt; rbd_data.&lt;id&gt;.00000000 选择一个image12# rbd ls volumesvolume-5ba95c5d-287b-429f-aadb-b72e990122ad 查看rbd_id文件获取image的id12345678910# ceph osd map volumes rbd_id.volume-5ba95c5d-287b-429f-aadb-b72e990122adosdmap e3001 pool 'volumes' (2) object 'rbd_id.volume-5ba95c5d-287b-429f-aadb-b72e990122ad' -&gt; pg 2.49c908b (2.8b) -&gt; up ([13,15], p13) acting ([13,15], p13)在osd节点上找到image对应的文件：# find . -name "*volume-5ba95c5d-287b-429f-aadb-b72e990122ad*"./ceph-15/current/2.8b_head/rbd\uid.volume-5ba95c5d-287b-429f-aadb-b72e990122ad__head_049C908B__2# cat ./ceph-15/current/2.8b_head/rbd\\uid.volume-5ba95c5d-287b-429f-aadb-b72e990122ad__head_049C908B__213cd1213ee70 上面13cd1213ee70即为image的id 查看rbd_id文件的attr123# rados -p volumes listxattr rbd_id.volume-5ba95c5d-287b-429f-aadb-b72e990122ad# rados -p volumes getxattr rbd_id.volume-5ba95c5d-287b-429f-aadb-b72e990122ad attrerror getting xattr volumes/rbd_id.volume-5ba95c5d-287b-429f-aadb-b72e990122ad/attr: (61) No data available rbd_id文件并没有设置attr 查看rbd_header文件12345678# ceph osd map volumes rbd_header.13cd1213ee70osdmap e3001 pool 'volumes' (2) object 'rbd_header.13cd1213ee70' -&gt; pg 2.57c27b74 (2.374) -&gt; up ([5,11], p5) acting ([5,11], p5)# find . -name "*header.13cd1213ee70*"./ceph-5/current/2.374_head/rbd\uheader.13cd1213ee70__head_57C27B74__2# ll ./ceph-5/current/2.374_head/rbd\\uheader.13cd1213ee70__head_57C27B74__2-rw-r--r-- 1 root root 0 Oct 29 17:13 ./ceph-5/current/2.374_head/rbd\uheader.13cd1213ee70__head_57C27B74__2 创建rbd的时候，image-format的解释如下： 123--image-format &lt;format-number&gt; format to use when creating an image format 1 is the original format (default) format 2 supports cloning old_format格式 调用顺序为：create_v1() --&gt; init_rbd_header()，里面会把image的metadata写入rbd_headerobject里。 new_format格式 rbd_header object的size为0，而image的metadata存在omap里。 貌似file的xattr只在有snap的时候才会设置，clone后会清除，需要验证。xattr主要用来给CephFS提供XATTR数据存放 查看rbd_header的attr123# rados -p volumes listxattr rbd_header.13cd1213ee70# rados -p volumes getxattr rbd_header.13cd1213ee70 attrerror getting xattr volumes/rbd_header.13cd1213ee70/attr: (61) No data available rbd_header文件并没有设置attr 查看rbd_header的omapvalue123456789101112131415161718192021# rados -p volumes listomapvals rbd_header.13cd1213ee70featuresvalue: (8 bytes) :0000 : 03 00 00 00 00 00 00 00 : ........object_prefixvalue: (25 bytes) :0000 : 15 00 00 00 72 62 64 5f 64 61 74 61 2e 31 33 63 : ....rbd_data.13c0010 : 64 31 32 31 33 65 65 37 30 : d1213ee70ordervalue: (1 bytes) :0000 : 19 : .sizevalue: (8 bytes) :0000 : 00 00 00 00 32 00 00 00 : ....2...snap_seqvalue: (8 bytes) :0000 : 00 00 00 00 00 00 00 00 : ........ 123456789# rados -p volumes listomapkeys rbd_header.13cd1213ee70featuresobject_prefixordersizesnap_seq# rados -p volumes getomapheader rbd_header.13cd1213ee70header (0 bytes) : 从rbd_directory中查看信息rbd_directory里包含了pool里所有image的信息 1234567891011121314151617# rados -p volumes listomapvals rbd_directoryid_13cd1213ee70value: (47 bytes) :0000 : 2b 00 00 00 76 6f 6c 75 6d 65 2d 35 62 61 39 35 : +...volume-5ba950010 : 63 35 64 2d 32 38 37 62 2d 34 32 39 66 2d 61 61 : c5d-287b-429f-aa0020 : 64 62 2d 62 37 32 65 39 39 30 31 32 32 61 64 : db-b72e990122adname_volume-5ba95c5d-287b-429f-aadb-b72e990122advalue: (16 bytes) :0000 : 0c 00 00 00 31 33 63 64 31 32 31 33 65 65 37 30 : ....13cd1213ee70# rados -p volumes listomapkeys rbd_directoryid_13cd1213ee70name_volume-5ba95c5d-287b-429f-aadb-b72e990122ad# rados -p volumes getomapheader rbd_directoryheader (0 bytes) : 查看rbd_data文件查看该image的其中一个object的存放信息： 12345678获取映射关系# ceph osd map volumes rbd_header.13cd1213ee70.0000000000000ac3osdmap e3001 pool 'volumes' (2) object 'rbd_data.13cd1213ee70.0000000000000ac3' -&gt; pg 2.bc79817 (2.17) -&gt; up ([6,12], p6) acting ([6,12], p6)在对应osd上找到上面的数据objects，命名规则介绍如下：# find . -name "*13cd1213ee70.0000000000000ac3*"./ceph-12/current/2.17_head/rbd\udata.13cd1213ee70.0000000000000ac3__head_0BC79817__2 rbd_data &lt;id&gt; &lt;slice num&gt; head &lt;HASH&gt; &lt;pool id&gt;]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>rbd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ceph cpu profile]]></title>
    <url>%2F2015%2F11%2F08%2Fceph-cpu-profile%2F</url>
    <content type="text"><![CDATA[oprofileceph官网推荐推荐使用oprofile，参考：http://docs.ceph.com/docs/master/dev/cpu-profiler/ 重新编译ceph： 12345678910apt-get install oprofile oprofile-guimake distcleanexport CFLAGS=“-fno-omit-frame-pointer -O2 -g"dpkg-buildpackage或者：./autogen.sh./configuremake 使用oprofile：http://docs.ceph.com/docs/master/rados/troubleshooting/cpu-profiling/ perf官网介绍：https://perf.wiki.kernel.org/index.php/Main_Page 安装：perf在ubuntu 14.04.1上需要安装linux-tools-3a.16.0-30-generic, linux-cloud-tools-3.16.0-30-generic 安装后perf不在PATH里指定的目录下，需要创建如下软连接： 1ln -s /usr/lib/linux-lts-utopic-tools-3.16.0-30/perf /usr/local/bin/perf 常用操作 1234perf statperf topperf recordperf report 指定process 1234perf stat -p 1436446 sleep 10perf record -p 1436446perf top -p 1436446perf report enable call-graph 123[-g enables call-graph recording]perf record -a -g perf report -g graph]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>oprofile</tag>
        <tag>perf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Google perftools usage with ceph]]></title>
    <url>%2F2015%2F10%2F30%2Fgoogle-perftools-usage-with-ceph%2F</url>
    <content type="text"><![CDATA[Install123456789apt-get install libtoolgit clone https://github.com/gperftools/gperftools.gitcd gperftools./autogen.sh./configuremakemake checkmake install Run12345ceph tell osd.1 heap start_profilerceph tell osd.1 heap statsceph tell osd.1 heap dumpceph tell osd.1 heap stop_profilerceph tell osd.1 heap release Then you can find files in dir ／var/log/ceph such as osd.1.profile.0001.heap Text display12345678910111213141516# pprof --text /usr/bin/ceph-osd /var/log/ceph/osd.1.profile.0001.heapUsing local file /usr/bin/ceph-osd.Using local file /var/log/ceph/osd.1.profile.0001.heap.Total: 1.7 MB 0.8 45.6% 45.6% 0.8 45.6% ceph::log::Log::create_entry 0.6 38.0% 83.6% 0.6 38.0% std::string::_Rep::_S_create 0.2 10.3% 93.9% 0.2 10.3% leveldb::Arena::AllocateNewBlock 0.0 1.8% 95.7% 0.0 1.8% intrusive_ptr_release@7c61d0 0.0 1.0% 96.7% 0.0 1.2% SimpleMessenger::add_accept_pipe 0.0 0.8% 97.5% 0.7 43.6% TrackedOp::mark_event...* The first column contains the direct memory use in MB.* The fourth column contains memory use by the procedure and all of its callees.* The second and fifth columns are just percentage representations of the numbers in the first and fourth columns.* The third column is a cumulative sum of the second column (i.e., the kth entry in the third column is the sum of the first k entries in the second column.) Comparing Profiles12345678910# pprof --text /usr/bin/ceph-osd --base=osd.1.profile.0002.heap osd.1.profile.0001.heapUsing local file /usr/bin/ceph-osd.Using local file osd.1.profile.0001.heap.Total: 1.5 MB 0.7 47.2% 47.2% 0.7 47.2% ceph::log::Log::create_entry 0.6 38.6% 85.8% 0.6 38.6% std::string::_Rep::_S_create 0.2 10.6% 96.4% 0.2 10.6% leveldb::Arena::AllocateNewBlock 0.0 1.8% 98.2% 0.0 1.8% intrusive_ptr_release@7c61d0 0.0 0.6% 98.8% 0.7 44.2% TrackedOp::mark_event 0.0 0.3% 99.1% 0.0 0.6% decode_message Referenceshttp://docs.ceph.com/docs/master/rados/troubleshooting/memory-profiling/http://google-perftools.googlecode.com/svn/trunk/doc/heapprofile.htmlhttps://gperftools.googlecode.com/svn/trunk/doc/cpuprofile.html]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>perftools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph mutex lock monitor]]></title>
    <url>%2F2015%2F10%2F25%2Fceph-mutex-lock-monitor%2F</url>
    <content type="text"><![CDATA[在分析ceph性能的时候，发现OSD处理线程等待PG lock的时间比较长，当时就想能不能像ceph perf counter一样，统计下PG lock上的等待时间？（后来分析代码知道了PG lock互斥的原因：Ceph PGLock and RWState) 虽然最后证明这样的办法不可靠，不方便分析，但本次尝试也给出了在ceph代码中添加tracepoint的方法和步骤，记录如下； ceph perf counter现在ceph系统支持perf counter统计mutex花费的时间 123456789101112131415161718OPTION(mutex_perf_counter, OPT_BOOL, false)ceph daemon /var/run/ceph/ceph-osd.0.asok config set mutex_perf_counter 1ceph daemon /var/run/ceph/ceph-osd.0.asok config set mutex_perf_counter 0# ceph daemon /var/run/ceph/ceph-osd.0.asok perf dump | grep mutex "mutex-FileJournal::completions_lock": &#123; "mutex-FileJournal::finisher_lock": &#123; "mutex-FileJournal::write_lock": &#123; "mutex-FileJournal::writeq_lock": &#123; "mutex-JOS::ApplyManager::apply_lock": &#123; "mutex-JOS::ApplyManager::com_lock": &#123; "mutex-JOS::SubmitManager::lock": &#123; "mutex-WBThrottle::lock": &#123; "wait": &#123; "avgcount": 0, "sum": 0.000000000 &#125; &#125;, 这个能统计出一段时间内所有mutex lock花费的时间，但是不能显示具体某一个mutex lock的花费时间。 mutex添加tracepoint添加tracepoint到mutex实现中，通过tracepoint log来分析具体某一个mutex lock的花费时间。 src/tracing目录添加对应的mutex.tp文件123456789101112131415161718192021#include "include/int_types.h"TRACEPOINT_EVENT(mutex, lock_enter, TP_ARGS( const void *, addr, const char *, name), TP_FIELDS( ctf_integer_hex(unsigned long, addr, addr) ctf_string(name, name) ))TRACEPOINT_EVENT(mutex, lock_exit, TP_ARGS( const void *, addr, const char *, name), TP_FIELDS( ctf_integer_hex(unsigned long, addr, addr) ctf_string(name, name) )) 修改src/common/Mutex.cc文件添加： 12345#ifdef WITH_LTTNG#include “tracing/mutex.h"#else#define tracepoint(...)#endif 修改src/tracing/Makefile.am文件123456789101112131415161718192021222314d13&lt; mutex.tp \23,24d21&lt; mutex.c \&lt; mutex.h \63,73d59&lt; nodist_libmutex_tp_la_SOURCES = \&lt; mutex.c \&lt; mutex.h&lt; endif&lt; libmutex_tp_la_LIBADD = -llttng-ust -ldl&lt; libmutex_tp_la_CPPFLAGS = -DTRACEPOINT_PROBE_DYNAMIC_LINKAGE&lt; libmutex_tp_la_CFLAGS = -I$(top_srcdir)/src $(AM_CFLAGS)&lt; libmutex_tp_la_LDFLAGS =&lt;&lt;&lt; if WITH_LTTNG78d63&lt; libmutex_tp.la \87d71&lt; mutex.h \95d78&lt; $(nodist_libmutex_tp_la_SOURCES) \ 修改src/Makefile-env.am文件12添加：LIBMUTEX_TP = tracing/libmutex_tp.la 修改src/common/Makefile.am文件添加： 123if WITH_LTTNGlibcommon_la_LIBADD += $(LIBMUTEX_TP)endif 修改src/test/Makefile.am文件添加： 123if WITH_LTTNGUNITTEST_LDADD += $(LIBMUTEX_TP)endif 修改PG lock添加PG lock的perf counter统计。修改： 12345678PG::PG(OSDService *o, OSDMapRef curmap, const PGPool &amp;_pool, spg_t p) : osd(o), cct(o-&gt;cct),... map_lock("PG::map_lock"), osdmap_ref(curmap), last_persisted_osdmap_ref(curmap), pool(_pool), _lock((std::string("PG::_lock") + _pool.name.c_str()).c_str(), false, true, false, o-&gt;cct), 编译测试最终编译出来的安装包中，rbd命令不能执行，报以下错误： 12345# rbdLTTng-UST: Error (-17) while registering tracepoint probe. Duplicate registration of tracepoint probes having the same name is not allowed.Aborted (core dumped) https://www.mail-archive.com/ceph-users@lists.ceph.com/msg23627.htmlhttp://comments.gmane.org/gmane.comp.file-systems.ceph.devel/20353 上述问题是PG lock的命名重复导致的，可以先忽略，ceph是能正常工作的，也能通过lttng搜集出来mutex log的trace； 但是因为mutex太多地方用到，这个log太多了，没办法分析，需要寻求别的方法追踪PG lock。]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>tracepoint</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph client rbd cahce配置]]></title>
    <url>%2F2015%2F10%2F15%2Fceph-client-rbd-cache-configuration%2F</url>
    <content type="text"><![CDATA[修改ceph.conf文件添加如下内容： 123456789vim /etc/ceph/ceph.conf...[client] rbd cache = true rbd cache size = 268435456 rbd cache max dirty = 134217728 rbd cache max dirty age = 5 rbd cache writethrough until flush = true admin socket = /var/run/ceph/ceph-client/$cluster-$type.$id.$pid.asok 修改libvirt-qemu文件添加如下内容： 1234567vim /etc/apparmor.d/abstractions/libvirt-qemu...# for rbd/etc/ceph/ceph.conf r,capability mknod,/var/log/ceph/* rw,/&#123;,var/&#125;run/ceph/** rw, 重启相关服务重启libvirt-bin和nova-compute服务 12service libvirt-bin restartservice nova-compute restart 重启nova instances重启Host上的所有nova instances 1nova reboot [--hard] [--poll] &lt;server&gt; [&lt;server&gt; ...] 验证查看asok文件是否存在，并执行perf dump和config show命令 在 /var/run/ceph/ceph-client/ 目录下会出现类似如下的文件：ceph-client.volumes.824997.asok 12ceph --admin-daemon /var/run/ceph/ceph-client.volumes.824997.asok perf dumpceph --admin-daemon /var/run/ceph/ceph-client.volumes.824997.asok config show [注]： /var/run/ceph的目录是root权限，需要在其下创建ceph-client目录，然后执行 chmod 777 ceph-client，使得libvirt-qemu用户有创建asok的权限。]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>rbd</tag>
        <tag>nova</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ceph osd write op tracker analysis]]></title>
    <url>%2F2015%2F08%2F06%2Fceph-osd-write-op-tracker-analysis%2F</url>
    <content type="text"><![CDATA[简介Ceph OSD op tracker是osd的一个调试手段，通过op tracker可以分析osd的op在各个操作端的时间开销，进而分析出系统的性能瓶颈和需要调优的参数。 我们基于Hammer版本的op tracker添加了几个events，能更细粒度的切分op的各个阶段，下面是我们通过op tracker的log来分析OSD端性能的实例。 tracker eventsPrimary write这里把op的处理过程分为5个阶段，后面会对每一阶段进行分析； op: osd_op 12345678910111213141516171819202122232425262728293031 tracker -- seq: 329476, time: 2015-08-06 13:42:32.301230, event: header_read - OpRequest::OpRequest tracker -- seq: 329476, time: 2015-08-06 13:42:32.301232, event: throttled - OpRequest::OpRequest tracker -- seq: 329476, time: 2015-08-06 13:42:32.301726, event: all_read - OpRequest::OpRequest tracker -- seq: 329476, time: 0.000000, event: dispatched - OpRequest::OpRequest tracker -- seq: 329476, time: 2015-08-06 13:42:33.429736, event: reached_pg - OSD::dequeue_op - OSD::ShardedOpWQ::_process1. 上面这部分延时是osd op queue的处理时延 tracker -- seq: 329476, time: 2015-08-06 13:42:33.430201, event: started - ReplicatedPG::do_pg_op tracker -- seq: 329476, time: 2015-08-06 13:42:33.430800, event: waiting for subops from 6,47 - ReplicatedBackend::issue_op -&gt; ReplicatedPG::send_message_osd_cluster tracker -- seq: 329476, time: 2015-08-06 13:42:33.510099, event: commit_queued_for_journal_write - FileJournal::submit_entry2. 上面这部分延时主要是reserve filestore throttle等待的时间，往replica osd发送请求应该比较快。 tracker -- seq: 329476, time: 2015-08-06 13:42:33.510596, event: sub_op_commit_rec - ReplicatedBackend::sub_op_modify_reply tracker -- seq: 329476, time: 2015-08-06 13:42:33.510983, event: write_thread_in_journal_buffer - FileJournal::prepare_single_write [-&gt; journalq.push_back()] &lt;- FileJournal::prepare_multi_write &lt;- FileJournal::write_thread_entry tracker -- seq: 329476, time: 2015-08-06 13:42:35.071590, event: journaled_completion_queued - FileJournal::queue_completions_thru &lt;- FileJournal::committed_thru &lt;- JournalingObjectStore::ApplyManager::commit_finish3. 上面这部分延时是写journal的花费时间 tracker -- seq: 329476, time: 2015-08-06 13:42:35.074449, event: sub_op_commit_rec - ReplicatedBackend::sub_op_modify_reply tracker -- seq: 329476, time: 2015-08-06 13:42:35.906469, event: op_applied - ReplicatedBackend::op_applied &lt;- C_OSD_OnOpApplied.finish()4. 上面这部分延时是filestore数据罗盘的花费时间 tracker -- seq: 329476, time: 2015-08-06 13:42:35.947938, event: op_commit - ReplicatedBackend::op_commit tracker -- seq: 329476, time: 2015-08-06 13:42:35.947989, event: commit_sent - ReplicatedPG::eval_repop tracker -- seq: 329476, time: 2015-08-06 13:42:35.948057, event: done5. 上面这部分是发送reply给client的时延 Secondary writeop: osd_repop 1234567891011121314151617tracker -- seq: 329473, time: 2015-08-06 13:42:32.292964, event: header_read - OpRequest::OpRequesttracker -- seq: 329473, time: 2015-08-06 13:42:32.292965, event: throttled - OpRequest::OpRequesttracker -- seq: 329473, time: 2015-08-06 13:42:32.293070, event: all_read - OpRequest::OpRequesttracker -- seq: 329473, time: 0.000000, event: dispatched - OpRequest::OpRequesttracker -- seq: 329473, time: 2015-08-06 13:42:33.427436, event: reached_pg - OSD::dequeue_op - OSD::ShardedOpWQ::_processtracker -- seq: 329473, time: 2015-08-06 13:42:33.427459, event: started - ReplicatedPG::do_pg_optracker -- seq: 329473, time: 2015-08-06 13:42:33.428036, event: startedtracker -- seq: 329473, time: 2015-08-06 13:42:33.428083, event: commit_queued_for_journal_write - FileJournal::submit_entrytracker -- seq: 329473, time: 2015-08-06 13:42:33.507348, event: write_thread_in_journal_buffer - FileJournal::prepare_single_write &lt;- FileJournal::prepare_multi_write &lt;- FileJournal::write_thread_entrytracker -- seq: 329473, time: 2015-08-06 13:42:35.071399, event: journaled_completion_queued - FileJournal::queue_completions_thru &lt;- FileJournal::committed_thru &lt;- JournalingObjectStore::ApplyManager::commit_finishtracker -- seq: 329473, time: 2015-08-06 13:42:35.724058, event: sub_op_applied - ReplicatedBackend::sub_op_modify_appliedtracker -- seq: 329473, time: 2015-08-06 13:42:35.878877, event: commit_sent - ReplicatedBackend::sub_op_modify_committracker -- seq: 329473, time: 2015-08-06 13:42:35.878919, event: done 分析针对上述primary osd上的op tracker events，我们对前4个阶段的分析如下： 第1阶段：osd op queue的处理时延 处理osd op的workqueue和threadpool为op_shardedwq, osd_op_tp; 1234op_shardedwq(cct-&gt;_conf-&gt;osd_op_num_shards, this, cct-&gt;_conf-&gt;osd_op_thread_timeout, &amp;osd_op_tp),osd_op_tp(cct, "OSD::osd_op_tp", cct-&gt;_conf-&gt;osd_op_num_threads_per_shard * cct-&gt;_conf-&gt;osd_op_num_shards), 可以调大如下值来提高osd的处理能力，减少在这个阶段的时延 12OPTION(osd_op_num_threads_per_shard, OPT_INT, 2)OPTION(osd_op_num_shards, OPT_INT, 5) 第2阶段：reserve filestore throttle等待的时间 所耗时间包括往replica osd发送message的时间 + reserve filestore queue throttle wait时间 + filejournal throttle wait时间 123456789101112 void ReplicatedBackend::submit_transaction()|-- void ReplicatedBackend::issue_op()| |-- void ReplicatedPG::send_message_osd_cluster() // send write data to replica osds| |-- void OSDService::send_message_osd_cluster(int peer, Message *m, epoch_t from_epoch) |-- parent-&gt;queue_transaction(op_t, op.op);| |-- int FileStore::queue_transactions() // 没有journal或需要同时写journal和disk时，会调用 queue_op(osr, o);| |-- void JournalingObjectStore::_op_journal_transactions()| |-- void FileJournal::submit_entry() //添加到 deque&lt;write_item&gt; writeq;FileStore::queue_transactions()里会调用：（获取filestore和filejournal的throttle） op_queue_reserve_throttle(o, handle); journal-&gt;throttle(); 调整这些参数应该可以减少这部分的处理时间 12345OPTION(filestore_queue_max_ops, OPT_INT, 50)OPTION(filestore_queue_max_bytes, OPT_INT, 100 &lt;&lt; 20)OPTION(journal_queue_max_ops, OPT_INT, 300)OPTION(journal_queue_max_bytes, OPT_INT, 32 &lt;&lt; 20) 第3阶段：写journal的时间 journal写数据： 1FileJournal.Writer.entry() -&gt; FileJournal::write_thread_entry() -&gt; FileJournal::do_write() -&gt; FileJournal::write_bl() 使用高性能ssd作为journal应该能减少这部分时间。 第4阶段：写filestore的时间 所耗时间为所有osd写filestore的时间，这里测试发现数据写filestore比写journal更快，可能因为journal带的盘数较多的缘故。 filestore的处理线程配置参数： 1OPTION(filestore_op_threads, OPT_INT, 2)]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>osd</tag>
        <tag>optracker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ceph osd request分析]]></title>
    <url>%2F2015%2F06%2F18%2Fceph-osd-requst-analysis%2F</url>
    <content type="text"><![CDATA[OSD Message示例分析12345678910111213141516171819202122232425262728293031323334353637osd_op(client.4813.0:510 default.4161.780__shadow__99999.txt_0 [write 2097152~524288] 5.ba5249f6 ack+ondisk+write+known_if_redirected e130) v4class MOSDOp : public Message &#123;... void print(ostream&amp; out) const &#123; out &lt;&lt; "osd_op(" &lt;&lt; get_reqid(); client.4813.0:510 out &lt;&lt; " "; if (!oloc.nspace.empty()) out &lt;&lt; oloc.nspace &lt;&lt; "/"; out &lt;&lt; oid; default.4161.780__shadow__99999.txt_0#if 0 out &lt;&lt; " "; if (may_read()) out &lt;&lt; "r"; if (may_write()) out &lt;&lt; "w";#endif if (snapid != CEPH_NOSNAP) out &lt;&lt; "@" &lt;&lt; snapid; if (oloc.key.size()) out &lt;&lt; " " &lt;&lt; oloc; out &lt;&lt; " " &lt;&lt; ops; [write 2097152~524288] out &lt;&lt; " " &lt;&lt; pgid; 5.ba5249f6 if (is_retry_attempt()) out &lt;&lt; " RETRY=" &lt;&lt; get_retry_attempt(); if (reassert_version != eversion_t()) out &lt;&lt; " reassert_version=" &lt;&lt; reassert_version; if (get_snap_seq()) out &lt;&lt; " snapc " &lt;&lt; get_snap_seq() &lt;&lt; "=" &lt;&lt; snaps; out &lt;&lt; " " &lt;&lt; ceph_osd_flag_string(get_flags()); ack+ondisk+write+known_if_redirected out &lt;&lt; " e" &lt;&lt; osdmap_epoch; e130 out &lt;&lt; ")"; &#125;&#125; OSD Message处理流程Message在Primary OSD的处理流程12345678910111213141516171819202122bool OSD::dispatch_op_fast(OpRequestRef&amp; op, OSDMapRef&amp; osdmap)void OSD::handle_op(OpRequestRef&amp; op, OSDMapRef&amp; osdmap)void OSD::enqueue_op(PG *pg, OpRequestRef&amp; op)void PG::queue_op(OpRequestRef&amp; op) -- &#123; osd-&gt;op_wq.queue(make_pair(PGRef(this), op));&#125; void OSD::ShardedOpWQ::_process(uint32_t thread_index, heartbeat_handle_d *hb) void OSD::dequeue_op(PGRef pg, OpRequestRef op, ThreadPool::TPHandle &amp;handle)void ReplicatedPG::do_request(OpRequestRef&amp; op, ThreadPool::TPHandle &amp;handle)void ReplicatedPG::do_op(OpRequestRef&amp; op) // case CEPH_MSG_OSD_OPvoid ReplicatedPG::execute_ctx(OpContext *ctx)|-- int ReplicatedPG::prepare_transaction(OpContext *ctx)| int ReplicatedPG::do_osd_ops(OpContext *ctx, vector&lt;OSDOp&gt;&amp; ops)|-- void ReplicatedPG::issue_repop(RepGather *repop, utime_t now)| void ReplicatedBackend::submit_transaction()| |-- void ReplicatedBackend::issue_op()| | |-- void ReplicatedPG::send_message_osd_cluster() // send write data to replica osds| | |-- void OSDService::send_message_osd_cluster(int peer, Message *m, epoch_t from_epoch) | |-- parent-&gt;queue_transaction(op_t, op.op);| | |-- int FileStore::queue_transactions() // 没有journal或需要同时写journal和disk时，会调用 queue_op(osr, o);| | |-- void JournalingObjectStore::_op_journal_transactions()| | |-- void FileJournal::submit_entry() //添加到 deque&lt;write_item&gt; writeq;|-- void ReplicatedPG::eval_repop(RepGather *repop) 异步journal写数据1FileJournal.Writer.entry() -&gt; FileJournal::write_thread_entry() -&gt; FileJournal::do_write() -&gt; FileJournal::write_bl() Message在Replica OSD的处理流程123456789101112bool OSD::dispatch_op_fast(OpRequestRef&amp; op, OSDMapRef&amp; osdmap)void OSD::handle_op(OpRequestRef&amp; op, OSDMapRef&amp; osdmap)void OSD::enqueue_op(PG *pg, OpRequestRef&amp; op)void PG::queue_op(OpRequestRef&amp; op) -- &#123; osd-&gt;op_wq.queue(make_pair(PGRef(this), op));&#125;void OSD::ShardedOpWQ::_process(uint32_t thread_index, heartbeat_handle_d *hb) void OSD::dequeue_op(PGRef pg, OpRequestRef op, ThreadPool::TPHandle &amp;handle)void ReplicatedPG::do_request(OpRequestRef&amp; op, ThreadPool::TPHandle &amp;handle)bool ReplicatedBackend::handle_message()void ReplicatedBackend::sub_op_modify(OpRequestRef op)|-- parent-&gt;queue_transaction(op_t, op.op);| |-- int FileStore::queue_transactions() 回调函数在把transaction加入队列前，会初始化transaction的几个operation回调函数： 1234class ObjectStore: list&lt;Context *&gt; on_applied; list&lt;Context *&gt; on_commit; list&lt;Context *&gt; on_applied_sync; Primary OSD123456789101112131415161718— 发往replica的operation，处理replica reply是否全部返回Context *on_all_commit = new C_OSD_RepopCommit(this, repop);Context *on_all_applied = new C_OSD_RepopApplied(this, repop);Context *onapplied_sync = new C_OSD_OndiskWriteUnlock( repop-&gt;obc, repop-&gt;ctx-&gt;clone_obc, unlock_snapset_obc ? repop-&gt;ctx-&gt;snapset_obc : ObjectContextRef());— 发送到本OSD的operation，处理写本OSD的reply是否返回op_t-&gt;register_on_applied_sync(on_local_applied_sync);op_t-&gt;register_on_applied( parent-&gt;bless_context( new C_OSD_OnOpApplied(this, &amp;op)));op_t-&gt;register_on_applied( new ObjectStore::C_DeleteTransaction(op_t));op_t-&gt;register_on_commit( parent-&gt;bless_context( new C_OSD_OnOpCommit(this, &amp;op))); Replica OSD1234567— 发送到本OSD的operation，处理写本OSD的reply是否返回rm-&gt;localt.register_on_commit( parent-&gt;bless_context( new C_OSD_RepModifyCommit(this, rm)));rm-&gt;localt.register_on_applied( parent-&gt;bless_context( new C_OSD_RepModifyApply(this, rm))); Finisher类Finisher类是在src/common中定义的一个专门查看操作是否结束的一个类。 在这个类里面拥有一个线程finisher_thread和一个类型为Context指针的队列finisher_queue。当一个操作线程完成自己的操作后，会将Context类型对象送入队列。此时finisher_thread线程循环监视着自己的finisher_queue队列，当发现了有新进入的Context时，会调用这个Context::complete函数，这个函数则会调用到Context子类自己实现的finish函数，来处理操作完成后的后续工作。 1234567891011121314151617181920212223242526272829303132333435class Finisher &#123; CephContext *cct;... vector&lt;Context*&gt; finisher_queue;... void *finisher_thread_entry(); struct FinisherThread : public Thread &#123; Finisher *fin; FinisherThread(Finisher *f) : fin(f) &#123;&#125; void* entry() &#123; return (void*)fin-&gt;finisher_thread_entry(); &#125; &#125; finisher_thread;...&#125;void *Finisher::finisher_thread_entry()&#123;... while (!finisher_stop) &#123; while (!finisher_queue.empty()) &#123; vector&lt;Context*&gt; ls; ls.swap(finisher_queue);... for (vector&lt;Context*&gt;::iterator p = ls.begin(); p != ls.end(); ++p) &#123; if (*p) &#123; (*p)-&gt;complete(0); // 调用Context子类实现的finish函数 &#125;... &#125;... &#125;...&#125; IO相关finisher跟IO相关的finisher有三个，journal有一个，filestore有两个： 12345678910111213class JournalingObjectStore : public ObjectStore &#123;protected: Journal *journal; Finisher finisher; // 负责journal写完后的通知处理...&#125;class FileStore : public JournalingObjectStore, public md_config_obs_t&#123; Finisher ondisk_finisher; // 负责data写到journal后commit的通知处理 Finisher op_finisher; // 负责data写到filestore后apply的通知处理&#125; 写数据到FileStore和IO回调Primary OSD12345678910111213141516171819202122int FileStore::queue_transactions(Sequencer *posr, list&lt;Transaction*&gt; &amp;tls, TrackedOpRef osd_op, ThreadPool::TPHandle *handle)&#123; _op_journal_transactions(o-&gt;tls, o-&gt;op, new C_JournaledAhead(this, osr, o, ondisk), osd_op);...&#125;struct C_JournaledAhead : public Context &#123; FileStore *fs; FileStore::OpSequencer *osr; FileStore::Op *o; Context *ondisk; C_JournaledAhead(FileStore *f, FileStore::OpSequencer *os, FileStore::Op *o, Context *ondisk): fs(f), osr(os), o(o), ondisk(ondisk) &#123; &#125; void finish(int r) &#123; fs-&gt;_journaled_ahead(osr, o, ondisk); &#125;&#125;; 在journal commit后会通过JournalingObjectStore类的 finisher 来通知调用到C_JournaledAhead.finish()函数。 在_journal_ahead函数里会把op添加到filestore的写队列里和FileStore类的ondisk_finisher队列里。 123void FileStore::_journaled_ahead(OpSequencer *osr, Op *o, Context *ondisk)|-- queue_op(osr, o); — void FileStore::queue_op(OpSequencer *osr, Op *o) &#123;op_wq.queue(osr);&#125;，把op添加到FileStore的op_wq队列中。|-- ondisk_finisher.queue(to_queue); — 这里通过ondisk_finisher来通知上层IO已经commit到journal，具体会调用到C_OSD_OnOpCommit.finish()函数。 FileStore workqueue处理过程： 123456789101112FileStore.OpWQ._process &#123;store-&gt;_do_op(osr, handle);&#125;void FileStore::_do_op(OpSequencer *osr, ThreadPool::TPHandle &amp;handle)&#123; ... apply_manager.op_apply_start(o-&gt;op); int r = _do_transactions(o-&gt;tls, o-&gt;op, &amp;handle); apply_manager.op_apply_finish(o-&gt;op); ...&#125;FileStore::_do_transactions() &#123;r = _do_transaction(**p, op_seq, trans_num, handle);&#125;unsigned FileStore::_do_transaction() &#123;r = _write(cid, oid, off, len, bl, replica);// case write&#125; C_OSD_OnOpCommit.finish()函数调用到： 1234567891011121314void ReplicatedBackend::op_commit(InProgressOp *op)&#123;... op-&gt;waiting_for_commit.erase(get_parent()-&gt;whoami_shard()); // 把waiting_for_commit set里对应的信息清除 if (op-&gt;waiting_for_commit.empty()) &#123; // 为空是表明所有OSD写请求都已经返回 op-&gt;on_commit-&gt;complete(0); // 会调用到C_OSD_RepOpCommit.finish() op-&gt;on_commit = 0; &#125; if (op-&gt;done()) &#123; assert(!op-&gt;on_commit &amp;&amp; !op-&gt;on_applied); in_progress_ops.erase(op-&gt;tid); &#125;&#125; Replica OSD从OSD写请求返回后处理流程： 1234567891011121314151617181920212223void ReplicatedBackend::sub_op_modify_reply(OpRequestRef op)&#123;... if (in_progress_ops.count(rep_tid)) &#123; ip_op.waiting_for_applied.erase(from);... if (ip_op.waiting_for_applied.empty() &amp;&amp; ip_op.on_applied) &#123; ip_op.on_applied-&gt;complete(0); // C_OSD_RepOpApplied.finish() ip_op.on_applied = 0; &#125; if (ip_op.waiting_for_commit.empty() &amp;&amp; ip_op.on_commit) &#123; ip_op.on_commit-&gt;complete(0); // C_OSD_RepOpCommit.finish() ip_op.on_commit= 0; &#125; if (ip_op.done()) &#123; assert(!ip_op.on_commit &amp;&amp; !ip_op.on_applied); in_progress_ops.erase(iter); &#125; &#125;...&#125; C_OSD_RepOpCommit.finish()函数调用到： 12345678910void ReplicatedPG::repop_all_committed(RepGather *repop)&#123;... repop-&gt;all_committed = true; if (!repop-&gt;rep_aborted) &#123;... eval_repop(repop); // 返回reply给client端 &#125;&#125; sync数据到diskfilestore数据sync到disk(调底层fs的sync接口)： 12345678910111213class FileStore : public JournalingObjectStore, public md_config_obs_t&#123; struct SyncThread : public Thread &#123; FileStore *fs; SyncThread(FileStore *f) : fs(f) &#123;&#125; void *entry() &#123; fs-&gt;sync_entry(); return 0; &#125; &#125; sync_thread;...&#125; 在FileStore::mount方法中，会创建sync线程sync_thread.create(), 该线程的入口函数为： 123456789101112131415161718void FileStore::sync_entry()&#123; utime_t max_interval; max_interval.set_from_double(m_filestore_max_sync_interval); // 参数可配 utime_t min_interval; min_interval.set_from_double(m_filestore_min_sync_interval); // 参数可配... sync_cond.WaitInterval(g_ceph_context, lock, max_interval); // 等待interval时间 ... if (apply_manager.commit_start()) &#123; ... apply_manager.commit_started(); int err = backend-&gt;syncfs(); err = write_op_seq(op_fd, cp); err = ::fsync(op_fd); apply_manager.commit_finish();...&#125; 主要通过sync函数，将FileStore打开的文件进行数据的flush磁盘操作。 12345678910int GenericFileStoreBackend::syncfs()&#123;... if (m_filestore_fsync_flushes_journal_data) &#123; ret = ::fsync(get_op_fd()); &#125; else &#123; ret = sync_filesystem(get_current_fd()); &#125; return ret;&#125; commit_finish会把commit_waiters加入finisher队列，由finisher函数通知commit完成： 123456789101112131415void JournalingObjectStore::ApplyManager::commit_finish()&#123;... if (journal) journal-&gt;committed_thru(committing_seq); committed_seq = committing_seq; map&lt;version_t, vector&lt;Context*&gt; &gt;::iterator p = commit_waiters.begin(); while (p != commit_waiters.end() &amp;&amp; p-&gt;first &lt;= committing_seq) &#123; finisher.queue(p-&gt;second); commit_waiters.erase(p++); &#125;&#125; 何时唤醒sync线程： 123456789— sync_cond.Signal();void FileStore::_start_sync() &lt;- unsigned FileStore::_do_transaction() &#123;case Transaction::OP_STARTSYNC:&#125;&#123; if (!journal) &#123; sync_cond.Signal(); &#125;&#125;void FileStore::do_force_sync() &lt;- int FileStore::umount()void FileStore::start_sync(Context *onsafe) &lt;- void FileStore::sync() FileJournal的do_sync_cond也指向FileStore的sync_cond： 1234567891011121314151617int FileStore::open_journal()&#123;... journal = new FileJournal(fsid, &amp;finisher, &amp;sync_cond, journalpath.c_str(), m_journal_dio, m_journal_aio, m_journal_force_aio);...&#125;FileJournal.Writer.entry()-&gt; FileJournal::write_thread_entry()-&gt; FileJournal::prepare_multi_write()-&gt; FileJournal::prepare_single_write()-&gt; FileJournal::check_for_full()&#123; // passing half full mark, triggering commit do_sync_cond-&gt;SloppySignal(); // initiate a real commit so we can trim&#125;]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>osd</tag>
        <tag>filestore</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RGW performance with MemStore and CacheTiering]]></title>
    <url>%2F2015%2F06%2F12%2Frgw-perf-with-memstore-cachetiering%2F</url>
    <content type="text"><![CDATA[MemStoreMemStore配置简单，应用中会占用host内存来存储objects数据，OSD在mkfs的时候会在对应目录下创建一个collections的文件，用于存放OSD对应内存中的objects数据。 参考文章：How to configure ceph use memstore? 配置在ceph.conf里添加相应配置: 如果要所有OSD都启动MemStore，在[global]或[osd]里添加即可： 1osd objectstore = MemStore 如果要制定某些OSD启动MemStore，在对应OSD的选项里制定即可： 1234[osd.0] host = 10.20.6.54 devs = /dev/sdb osd objectstore = memstore 性能测试测试环境：5个Hosts，每个host上3个OSD。 启动MemStore后，运行rest-bench测试工具，write IO性能约有所提升，但提升幅度在两次测试中不一致。 配置所有OSD启动MemStore 12MemStore: Bandwidth (MB/sec) - 211.208 FileStore: Bandwidth (MB/sec) - 177.186 配置部分OSD启动MemStore，余下的OSD启动FileStore 每个Host上配置1个OSD启动MemStore，剩余2个启动FileStore 12MemStore: Bandwidth (MB/sec) - 123.848FileStore: Bandwidth (MB/sec) - 222.263 MemStore OSDs较少，rest-bench压力测试的话，MemStore性能受限。 详细参考文章：RGW performance with FileStore and MemStore 问题分析 MemStore何时刷数据到disk不确定？ 看代码是在OSD::umount里调用_save()函数，但是在实际测试中，stop一个OSD并没在log文件中找到对应log。 MemStore会一直占用host的内存，测试中发现host的物理内存会一直减少到200多M后，swap分区开始减少，当swap分区也没的时候，客户端写会报错。 但比较奇怪的是过阵子Host端内存被回收，这时OSD一切正常。。。不确定系统会不会直接把OSD占用的内存直接回收，导致OSD丢失数据。所以MemStore不适合做大数据的直接存储，可以结合Cache Tiering做缓存。 如果某个PG所在的OSD同时重启，会导致该PG数据丢失。测试中我还同时把5个Hosts上的ceph osd都重启，结果整个ceph系统数据混乱且无法恢复。 Cache TieringCache Tiering是结合CRUSH map, pool, CRUSH ruleset来实现某个pool作为其他pool的cache。 详细链接： http://docs.ceph.com/docs/master/rados/operations/cache-tiering/ 配置5个Hosts，每个host上3个OSD, 每个Host上配置1个OSD启动MemStore，剩余2个启动FileStore。测试中把5个MemStore的OSD配置为一个rule，剩余的FileStore的OSD配置位另一个rule。 修改并设置CRUSH map 这里配置了两个rule，rule 0 和rule 1，其中rule 0只使用FileStore的OSD，rule 1只是用MemStore的OSD。 1234ceph osd getcrushmap -o crushmap crushtool -d crushmap -o crush.map crushtool -c crush.map -o crushmapceph osd setcrushmap -i crushmap 创建cachepool，并制定其crush_ruleset 123ceph osd pool create cachepool 512 512 replicated ceph osd pool set cachepool crush_ruleset 1ceph osd pool set .rgw.buckets crush_ruleset 0 指定某个pool为其他pool的cache tier，设置cache mode 123ceph osd tier add .rgw.buckets cachepoolceph osd tier cache-mode cachepool writeback ceph osd tier set-overlay .rgw.buckets cachepool 性能测试 如上配置的Cache Tiering Bandwidth (MB/sec): 169.912 MemStore – 取消pool的cache设置，并指定.rgw.buckets的crush_ruleset为1 Bandwidth (MB/sec): 222.263 FileStore – 取消pool的cache设置，并指定.rgw.buckets的crush_ruleset为0 Bandwidth (MB/sec): 123.848 问题分析Cache Tiering能提升慢设备的吞吐量，但是其配置参数很多，需要根据具体应用场景测试配置参数。拿MemStore作为cache能一定程度提升client的IO write性能（前提是设置cache-mode为writeback）。 可配置参数Cache Tiering可以设置cache的刷新参数，包括以下几个： 123456789ceph osd pool set &#123;cachepool&#125; hit_set_type bloom – 使用bloom算法来管理hit_set集合ceph osd pool set &#123;cachepool&#125; hit_set_count 1 – 多少hit set 可以存储ceph osd pool set &#123;cachepool&#125; hit_set_period 3600 – 每个hit set可以保留多少时间ceph osd pool set &#123;cachepool&#125; target_max_bytes 1000000000000 – cache pool占用空间的刷盘阀值ceph osd pool set &#123;cachepool&#125; cache_target_dirty_ratio &#123;0.0..1.0&#125; – dirty数据占用率刷盘的阀值ceph osd pool set &#123;cachepool&#125; cache_target_full_ratio &#123;0.0..1.0&#125; – cache pool空间占用率达到多少时刷盘 ceph osd pool set &#123;cachepool&#125; target_max_objects &#123;#objects&#125; – cache pool中object数据的刷盘阀值ceph osd pool set &#123;cachepool&#125; cache_min_flush_age &#123;#seconds&#125; – object在cache中最少停留时间ceph osd pool set &#123;cachepool&#125; cache_min_evict_age &#123;#seconds&#125; – object会在多少时间后被逐出cache]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>memstore</tag>
        <tag>radosgw</tag>
        <tag>cache tiering</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RGW performance with FileStore and MemStore]]></title>
    <url>%2F2015%2F06%2F05%2Frgw-perf-with-filestore-memstore%2F</url>
    <content type="text"><![CDATA[概述针对我们的应用，ceph使用Filestore和Memstore的性能有多大差距呢？我们通过ceph的rest-bench工具对之进行了性能对比测试。 FileStore分两次的测试结果如下： 12345678910111213141516171819202122232425262728293031323334353637# rest-bench --proxy=localhost --proxy-port=36666 --bucket=bug_fix --api-host=localhost --access-key=LC5O0HVGBZ1RJXBW4FZ4 --secret=2MsVUZ+h4lTrSAUx9ZCY4D4a874X+MPULzlUc0RB write...2015-05-21 15:08:16.741796min lat: 0.158293 max lat: 1.01379 avg lat: 0.359322 sec Cur ops started finished avg MB/s cur MB/s last lat avg lat 60 16 2681 2665 177.642 168 0.31313 0.359322 Total time run: 60.523804Total writes made: 2681Write size: 4194304Bandwidth (MB/sec): 177.186Stddev Bandwidth: 30.3802Max bandwidth (MB/sec): 208Min bandwidth (MB/sec): 0Average Latency: 0.360339Stddev Latency: 0.131017Max latency: 1.01379Min latency: 0.158293In sync_write(), oid: benchmark_last_metadata# rest-bench --proxy=localhost --proxy-port=36666 --bucket=bug_fix --api-host=localhost --access-key=LC5O0HVGBZ1RJXBW4FZ4 --secret=2MsVUZ+h4lTrSAUx9ZCY4D4a874X+MPULzlUc0RB write...2015-05-21 15:18:32.523700min lat: 0.169844 max lat: 1.1016 avg lat: 0.338614 sec Cur ops started finished avg MB/s cur MB/s last lat avg lat 60 16 2842 2826 188.376 188 0.197176 0.338614 Total time run: 60.473186Total writes made: 2842Write size: 4194304Bandwidth (MB/sec): 187.984Stddev Bandwidth: 27.4199Max bandwidth (MB/sec): 208Min bandwidth (MB/sec): 0Average Latency: 0.339906Stddev Latency: 0.131774Max latency: 1.1016Min latency: 0.169844In sync_write(), oid: benchmark_last_metadata MemStore参考文章配置memstore：How to configure ceph use memstore? 分两次的测试结果如下： 12345678910111213141516171819202122232425262728293031323334353637# rest-bench --proxy=localhost --proxy-port=36666 --bucket=bug_fix --api-host=localhost --access-key=GR73W0UWWQ6U43DUHVCC --secret= GzgWGEp0hO0JdfdbrxLuopCv+ufHyHYgFSgjvU0U write...2015-05-21 16:03:21.806960min lat: 0.069719 max lat: 1.45503 avg lat: 0.302808 sec Cur ops started finished avg MB/s cur MB/s last lat avg lat 60 16 3177 3161 210.701 224 0.193723 0.302808 Total time run: 60.187152Total writes made: 3178Write size: 4194304Bandwidth (MB/sec): 211.208Stddev Bandwidth: 34.5447Max bandwidth (MB/sec): 244Min bandwidth (MB/sec): 0Average Latency: 0.302945Stddev Latency: 0.101357Max latency: 1.45503Min latency: 0.069719In sync_write(), oid: benchmark_last_metadata# rest-bench --proxy=localhost --proxy-port=36666 --bucket=bug_fix --api-host=localhost --access-key=GR73W0UWWQ6U43DUHVCC --secret= GzgWGEp0hO0JdfdbrxLuopCv+ufHyHYgFSgjvU0U write...2015-05-21 16:04:45.235222min lat: 0.080655 max lat: 1.23022 avg lat: 0.287523 sec Cur ops started finished avg MB/s cur MB/s last lat avg lat 60 16 3344 3328 221.834 204 0.295268 0.287523 Total time run: 60.175814Total writes made: 3345Write size: 4194304Bandwidth (MB/sec): 222.348Stddev Bandwidth: 30.089Max bandwidth (MB/sec): 240Min bandwidth (MB/sec): 0Average Latency: 0.287704Stddev Latency: 0.0919689Max latency: 1.23022Min latency: 0.080655In sync_write(), oid: benchmark_last_metadata 测试结果总结上述各两次的测试结果如下： type filestore memstore Bandwidth (MB/s) 177.186 211.208 187.984 222.348 Average Latency 0.360339 0.339906 0.302945 0.287704 从上述表格中看出，使用Memstore能提升约20%的Bandwidth，能提升约10%的Latency，相比于内存 vs 磁盘的性能对比，Memstore vs Filestore的提升不是很明显。]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>memstore</tag>
        <tag>rest-bench</tag>
        <tag>filestore</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[How to configure Ceph use memstore?]]></title>
    <url>%2F2015%2F05%2F25%2Fceph-use-memstore%2F</url>
    <content type="text"><![CDATA[Configuration of memstore123456789101112131415161718192021222324252627282930313233— — in file: ceph_osd.ccint main(int argc, const char **argv)&#123;... // the store ObjectStore *store = ObjectStore::create(g_ceph_context, g_conf-&gt;osd_objectstore, g_conf-&gt;osd_data, g_conf-&gt;osd_journal);...&#125;ObjectStore *ObjectStore::create(CephContext *cct, const string&amp; type, const string&amp; data, const string&amp; journal, osflagbits_t flags)&#123; if (type == "filestore") &#123; return new FileStore(data, journal, flags); &#125; if (type == "memstore") &#123; return new MemStore(cct, data); &#125; if (type == "keyvaluestore-dev") &#123; return new KeyValueStore(data); &#125; return NULL;&#125;— — in file: config_opts.hOPTION(osd_objectstore, OPT_STR, "filestore") // ObjectStore backend type So we can config OSD use memstore in file ceph.conf: 12[osd] osd objectstore = memstore For Memstore, we could use Normal disk as storage of OSD, it would only use the host memory to store object data.Only when OSD unmount, the data in memory would flush to disk. create a memory virtual disk supported by linuxThere are three type memory virtual disk supported in linux: ramdisk 首先查看一下可用的RamDisk，使用 ls /dev/ram* 然后对/dev/ram0 创建文件系统，运行 mkfs.xfs /dev/ram0 最后挂载 /dev/ram0，运行 mount /dev/ram /mnt/test ramfsRamfs顾名思义是内存文件系统，它处于虚拟文件系统（VFS）层，而不像ramdisk那样基于虚拟在内存中的其他文件系统(ex2fs)。因而，它无需格式化，可以创建多个，只要内存足够，在创建时可以指定其最大能使用的内存大小。在编译内核时须将File systems –&gt;&gt; pseudo filesystems –&gt;&gt; Virtual memory file system support支持选上。 1# mount -t ramfs none /testRAM 缺省情况下，Ramfs被限制最多可使用内存大小的一半。可以通过maxsize（以kbyte为单位）选项来改变。 12创建了一个限定最大使用内存为2M的ramdisk# mount -t ramfs none /testRAM -o maxsize=2000 tmpfsTmpfs是一个虚拟内存文件系统，它不同于传统的用块设备形式来实现的Ramdisk，也不同于针对物理内存的Ramfs。Tmpfs可以使用物理内存，也可以使用交换分区。在Linux内核中，虚拟内存资源由物理内存（RAM）和交换分区组成，这些资源是由内核中的虚拟内存子系统来负责分配和管理。Tmpfs向虚拟内存子系统请求页来存储文件，它同Linux的其它请求页的部分一样，不知道分配给自己的页是在内存中还是在交换分区中。同Ramfs一样，其大小也不是固定的，而是随着所需要的空间而动态的增减。在编译内核时须将File systems –&gt;&gt; pseudo filesystems –&gt;&gt; Virtual memory file system support支持选上。 12345# mkdir -p /mnt/tmpfs# mount tmpfs /mnt/tmpfs -t tmpfs同样可以在加载时指定tmpfs文件系统大小的最大限制:# mount tmpfs /mnt/tmpfs -t tmpfs -o size=32m use this memory virtual disk as the OSD deviceAfter all this OSD had been deployed, then we can configure the CRUSH ruleset of one pool to decide which data would use this OSD.]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>memstore</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ceph osd journal测试]]></title>
    <url>%2F2015%2F05%2F13%2Fceph-osd-journal-test%2F</url>
    <content type="text"><![CDATA[创建pool12345678To create a pool, execute:ceph osd pool create &#123;pool-name&#125; &#123;pg-num&#125; [&#123;pgp-num&#125;] [replicated] \ [crush-ruleset-name]ceph osd pool create &#123;pool-name&#125; &#123;pg-num&#125; &#123;pgp-num&#125; erasure \ [erasure-code-profile] [crush-ruleset-name]# rados mkpool testpool / ceph osd pool create testpool 1024 1024 replicated 测试journal文件rename put一个object rados put testobj ~/truck-dis-6.5.tar.gz --pool=testpool 获取object的map信息 ceph osd map testpool testobj 到对应的osd上把journal文件重命名为别的随意名字 mv journal bak.journal get这个object rados get testobj tst.tar.gz --pool=testpool 对比get的object和源object一致 diff tst.tar.gz ~/truck-dis-6.5.tar.gz 删除该object rados rm testobj --pool=testpool 重新put该object rados put testobj ~/truck-dis-6.5.tar.gz --pool=testpool get这个object rados get testobj tst.tar.gz --pool=testpool 对比get的object和源object一致 diff tst.tar.gz ~/truck-dis-6.5.tar.gz 结论测试发现即使把osd的journal mv到新的文件，osd的写操作还是会更新到这个journal文件的，感觉是osd一直持有journal的文件句柄。 FileJournal定义： 12345678910111213141516171819class FileJournal : public Journal &#123;... int fd;... Mutex writeq_lock; Cond writeq_cond; deque&lt;write_item&gt; writeq;... class Writer : public Thread &#123; FileJournal *journal; public: Writer(FileJournal *fj) : journal(fj) &#123;&#125; void *entry() &#123; journal-&gt;write_thread_entry(); return 0; &#125; &#125; write_thread;...&#125; journal初始化fd： 1OSD::init() -&gt; FileStore::mount() -&gt; JournalingObjectStore::journal_replay() -&gt; FileJournal::open() -&gt; FileJournal::_open() journal close fd： 1OSD::shutdown() -&gt; FileStore::umount() -&gt; JournalingObjectStore::journal_write_close() -&gt; FileJournal::close() journal写数据： 1FileJournal.Writer.entry() -&gt; FileJournal::write_thread_entry() -&gt; FileJournal::do_write() -&gt; FileJournal::write_bl() 模拟journal文件不可读后续把某个osd的journal文件配置在别的硬盘上，然后把journal在的硬盘offline，写object返回error，该osd变为down状态。 123456echo offline &gt; /sys/block/sdm/device/stateecho running &gt; /sys/block/sdm/device/statemount -t xfs /dev/sdm /data/cache/osd27 -o rw,noatime,inode64,logbsize=256k,delaylogservice ceph restart osd.27 分析journal文件journal文件其实就是创建的时候在文件最前面写一个header 12345678910111213struct header_t &#123; enum &#123; FLAG_CRC = (1&lt;&lt;0), // NOTE: remove kludgey weirdness in read_header() next time a flag is added. &#125;; uint64_t flags; uuid_d fsid; __u32 block_size; __u32 alignment; int64_t max_size; // max size of journal ring buffer int64_t start; // offset of first entry uint64_t committed_up_to; // committed up to 验证： 1234567891011OSD journal get fsid command:# ceph-osd -i 58 --get-journal-fsid4ac9b14d-652b-4480-a158-8837d67d1651# xxd -l 4096 journal myj# head -n 5 myj0000000: 0400 0000 4000 0000 0100 0000 0000 0000 ....@...........0000010: 4ac9 b14d 652b 4480 a158 8837 d67d 1651 J..Me+D..X.7.&#125;.Q0000020: 0010 0000 0010 0000 0000 0071 0200 0000 ...........q....0000030: 0090 9d4e 0000 0000 eeea 0000 0000 0000 ...N............0000040: efea 0000 0000 0000 0000 0000 0000 0000 ................ 重做journal重新make journal： 1ceph-osd -i 58 --mkjournal 这个命令可以创建一个osd的journal文件, 它会读取ceph配置，在osd的目录下生成journal文件，如果journal文件存在就失败 然后用新的journal文件重启osd service成功。 ceph journal其他命令osd journal flush command： 123# ceph-osd -i 58 --flush-journal2015-05-14 11:18:05.019196 7f175b62b800 -1 os/FileJournal.cc:95 journal FileJournal::_open: disabling aio for non-block journal. Use journal_force_aio to force use of aio anyway2015-05-14 11:18:05.212845 7f175b62b800 -1 ceph_osd.cc:271 flushed journal /data/cache/osd58/journal for object store /data/cache/osd58 OSD journal dump command: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677# ceph-osd -i 58 --dump-journal2015-05-14 11:28:55.490619 7f1125980800 -1 common/admin_socket.cc:512 asok(0x4ab0230) AdminSocketConfigObs::init: failed: AdminSocket::bind_and_listen: failed to bind the UNIX domain socket to '/var/run/ceph/ceph-osd.58.asok': (17) File exists2015-05-14 11:28:55.490695 7f1125980800 -1 os/FileJournal.cc:95 journal FileJournal::_open: disabling aio for non-block journal. Use journal_force_aio to force use of aio anyway[ &#123; "offset": 1411604480, "seq": 93784, "transactions": [ &#123; "trans_num": 0, "ops": [ &#123; "op_num": 0, "op_name": "omap_setkeys", "collection": "meta", "oid": "51ad36\/pglog_5.4ad\/0\/\/-1", "attr_lens": &#123; "0000000179.00000000000000000080": 186&#125;&#125;, &#123; "op_num": 1, "op_name": "omap_setkeys", "collection": "meta", "oid": "16ef7597\/infos\/head\/\/-1", "attr_lens": &#123; "5.4ad_epoch": 4, "5.4ad_info": 721&#125;&#125;, &#123; "op_num": 2, "op_name": "omap_rmkeys", "collection": "meta", "oid": "51ad36\/pglog_5.4ad\/0\/\/-1"&#125;, &#123; "op_num": 3, "op_name": "omap_setkeys", "collection": "meta", "oid": "51ad36\/pglog_5.4ad\/0\/\/-1", "attr_lens": &#123; "0000000179.00000000000000000080": 186, "can_rollback_to": 12&#125;&#125;, &#123; "op_num": 4, "op_name": "touch", "collection": "5.4ad_head", "oid": "546bdcad\/default.4250.345_health_check\/18_1431573984.96\/head\/\/5"&#125;, &#123; "op_num": 5, "op_name": "setattr", "collection": "5.4ad_head", "oid": "546bdcad\/default.4250.345_health_check\/18_1431573984.96\/head\/\/5", "name": "_user.rgw.idtag", "length": 19&#125;, &#123; "op_num": 6, "op_name": "write", "collection": "5.4ad_head", "oid": "546bdcad\/default.4250.345_health_check\/18_1431573984.96\/head\/\/5", "length": 12, "offset": 0, "bufferlist length": 12&#125;, &#123; "op_num": 7, "op_name": "setattr", "collection": "5.4ad_head", "oid": "546bdcad\/default.4250.345_health_check\/18_1431573984.96\/head\/\/5", "name": "_user.rgw.acl", "length": 39&#125;, &#123; "op_num": 8, "op_name": "setattr", "collection": "5.4ad_head", "oid": "546bdcad\/default.4250.345_health_check\/18_1431573984.96\/head\/\/5", "name": "_user.rgw.etag", "length": 33&#125;, &#123; "op_num": 9, "op_name": "setattr", "collection": "5.4ad_head", "oid": "546bdcad\/default.4250.345_health_check\/18_1431573984.96\/head\/\/5", "name": "_user.rgw.total_size", "length": 3&#125;, &#123; "op_num": 10, "op_name": "setattr", "collection": "5.4ad_head", "oid": "546bdcad\/default.4250.345_health_check\/18_1431573984.96\/head\/\/5", "name": "_", "length": 273&#125;, &#123; "op_num": 11, "op_name": "setattr", "collection": "5.4ad_head", "oid": "546bdcad\/default.4250.345_health_check\/18_1431573984.96\/head\/\/5", "name": "snapset", "length": 31&#125;]&#125;]&#125;2015-05-14 11:28:55.506607 7f1125980800 -1 ceph_osd.cc:285 dumped journal /data/cache/osd58/journal for object store /data/cache/osd58]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>osd</tag>
        <tag>journal</tag>
      </tags>
  </entry>
</search>
