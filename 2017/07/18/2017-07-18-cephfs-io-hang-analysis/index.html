<!doctype html>
<html class="theme-next use-motion theme-next-mala">
<head>
  

<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






  <link rel="stylesheet" type="text/css" href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5"/>



  <link href='//fonts.googleapis.com/css?family=Lato:300,400,700,400italic&subset=latin,latin-ext' rel='stylesheet' type='text/css'>


<link rel="stylesheet" type="text/css" href="/css/main.css?v=0.4.5.1"/>


    <meta name="description" content="学无止境" />



  <meta name="keywords" content="ceph,cephfs," />





  <link rel="shorticon icon" type="image/x-icon" href="/favicon.ico?v=0.4.5.1" />


<meta name="description" content="问题为了测试cephfs的可用性，我们对cephfs进行了几组stripe的测试，在跑自动化测试时，发现fio总是会跑一部分后hang住，而此时ceph -s输出显示read值太大，但实际环境中查看并没这么大的流量。 ceph输出123456789101112# ceph -w    cluster 16f59233-16da-4864-8c5a-1fec71d119ad     health H">
<meta name="keywords" content="ceph,cephfs">
<meta property="og:type" content="article">
<meta property="og:title" content="Cephfs测试中IO hang问题分析">
<meta property="og:url" content="http://www.ictfox.cn/2017/07/18/2017-07-18-cephfs-io-hang-analysis/index.html">
<meta property="og:site_name" content="ictfox blog">
<meta property="og:description" content="问题为了测试cephfs的可用性，我们对cephfs进行了几组stripe的测试，在跑自动化测试时，发现fio总是会跑一部分后hang住，而此时ceph -s输出显示read值太大，但实际环境中查看并没这么大的流量。 ceph输出123456789101112# ceph -w    cluster 16f59233-16da-4864-8c5a-1fec71d119ad     health H">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2017-07-18T06:12:09.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Cephfs测试中IO hang问题分析">
<meta name="twitter:description" content="问题为了测试cephfs的可用性，我们对cephfs进行了几组stripe的测试，在跑自动化测试时，发现fio总是会跑一部分后hang住，而此时ceph -s输出显示read值太大，但实际环境中查看并没这么大的流量。 ceph输出123456789101112# ceph -w    cluster 16f59233-16da-4864-8c5a-1fec71d119ad     health H">


<script type="text/javascript" id="hexo.configuration">
  var CONFIG = {
    scheme: 'Mala',
    sidebar: 'post'
  };
</script>

  <title> Cephfs测试中IO hang问题分析 | ictfox blog </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  <!--[if lte IE 8]>
  <div style=' clear: both; height: 59px; padding:0 0 0 15px; position: relative;margin:0 auto;'>
    <a href="http://windows.microsoft.com/en-US/internet-explorer/products/ie/home?ocid=ie6_countdown_bannercode">
      <img src="http://7u2nvr.com1.z0.glb.clouddn.com/picouterie.jpg" border="0" height="42" width="820"
           alt="You are using an outdated browser. For a faster, safer browsing experience, upgrade for free today or use other browser ,like chrome firefox safari."
           style='margin-left:auto;margin-right:auto;display: block;'/>
    </a>
  </div>
<![endif]-->
  



  <div class="container one-column page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><h1 class="site-meta">
  <span class="logo-line-before"><i></i></span>
  <a href="/" class="brand" rel="start">
      <span class="logo">
        <i class="icon-next-logo"></i>
      </span>
      <span class="site-title">ForTech</span>
  </a>
  <span class="logo-line-after"><i></i></span>
</h1>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu ">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            <i class="menu-item-icon icon-next-home"></i> <br />
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            <i class="menu-item-icon icon-next-categories"></i> <br />
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            <i class="menu-item-icon icon-next-archives"></i> <br />
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            <i class="menu-item-icon icon-next-tags"></i> <br />
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            <i class="menu-item-icon icon-next-about"></i> <br />
            About
          </a>
        </li>
      

      
      
    </ul>
  

  
</nav>

 </div>
    </header>

    

    <main id="main" class="main">
      <div class="main-inner">
        <div id="content" class="content"> 

  <div id="posts" class="posts-expand">
    

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              Cephfs测试中IO hang问题分析
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          Posted on
          <time itemprop="dateCreated" datetime="2017-07-18T14:11:17+08:00" content="2017-07-18">
            2017-07-18
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; In
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/ceph/" itemprop="url" rel="index">
                  <span itemprop="name">ceph</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        <span itemprop="articleBody"><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>为了测试cephfs的可用性，我们对cephfs进行了几组stripe的测试，在跑自动化测试时，发现fio总是会跑一部分后hang住，而此时ceph -s输出显示read值太大，但实际环境中查看并没这么大的流量。</p>
<h2 id="ceph输出"><a href="#ceph输出" class="headerlink" title="ceph输出"></a>ceph输出</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ceph -w</span></span><br><span class="line">    cluster 16f59233-16da-4864-8c5a-1fec71d119ad</span><br><span class="line">     health HEALTH_OK</span><br><span class="line">     monmap e1: 3 mons at &#123;cephcluster-server1=10.10.1.6:6789/0,cephcluster-server2=10.10.1.7:6789/0,cephcluster-server3=10.10.1.8:6789/0&#125;</span><br><span class="line">            election epoch 12, quorum 0,1,2 cephcluster-server1,cephcluster-server2,cephcluster-server3</span><br><span class="line">      fsmap e17: 1/1/1 up &#123;0=mds-daemon-38=up:active&#125;, 1 up:standby</span><br><span class="line">     osdmap e180: 30 osds: 30 up, 30 <span class="keyword">in</span></span><br><span class="line">            flags sortbitwise,require_jewel_osds</span><br><span class="line">      pgmap v55504: 1088 pgs, 3 pools, 81924 MB data, 10910 objects</span><br><span class="line">            241 GB used, 108 TB / 109 TB avail</span><br><span class="line">                1088 active+clean</span><br><span class="line">  client io 68228 MB/s rd, 1066 op/s rd, 0 op/s wr</span><br></pre></td></tr></table></figure>
<blockquote>
<p>分析：cephfs client io显示的信息是正确的。从后面的分析得知cephfs client一直loop着尝试read object，单次read就有64MB，object数据缓存在osd的内存里，所以ceph统计的read速率很大。</p>
</blockquote>
<h2 id="查看网络"><a href="#查看网络" class="headerlink" title="查看网络"></a>查看网络</h2><p>用iftop看网络的流量只有10MB/s</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># iftop -B -i eth4</span></span><br><span class="line">                           25.5MB                    50.9MB                    76.4MB                    102MB                    127MB</span><br><span class="line">└──────────────────────────┴─────────────────────────┴─────────────────────────┴─────────────────────────┴──────────────────────────</span><br><span class="line">cephcluster-server2.****.com                     =&gt; 10.10.1.18                                              4.46MB  4.51MB  4.52MB</span><br><span class="line">                                                 &lt;=                                                         220KB   221KB   222KB</span><br></pre></td></tr></table></figure>
<h2 id="对比几个stripe"><a href="#对比几个stripe" class="headerlink" title="对比几个stripe"></a>对比几个stripe</h2><p>发现对64MB的无条带化的object读时，会触发这个bug：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># getfattr -n ceph.file.layout dir4/tstfile</span></span><br><span class="line"><span class="comment"># file: dir4/tstfile</span></span><br><span class="line">ceph.file.layout=<span class="string">"stripe_unit=67108864 stripe_count=1 object_size=67108864 pool=cephfs_data”</span></span><br></pre></td></tr></table></figure>
<p>bs=64M的fio hang住</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># fio -filename=/home/yangguanjun/cephfs/dir4/tstfile -size=20G -thread -group_reporting -direct=1 -ioengine=libaio -bs=64M -rw=read -iodepth=64 -iodepth_batch_submit=8 -iodepth_batch_complete=8 -name=write_64k_64q</span></span><br><span class="line">write_64k_64q: (g=0): rw=<span class="built_in">read</span>, bs=64M-64M/64M-64M/64M-64M, ioengine=libaio, iodepth=64</span><br><span class="line">fio-2.2.8</span><br><span class="line">Starting 1 thread</span><br><span class="line">^Cbs: 1 (f=1): [R(1)] [inf% <span class="keyword">done</span>] [0KB/0KB/0KB /s] [0/0/0 iops] [eta 1158050441d:06h:59m:55s]</span><br><span class="line">fio: terminating on signal 2</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>bs=32M的fio hang住</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># fio -filename=/home/yangguanjun/cephfs/dir4/tstfile -size=20G -thread -group_reporting -direct=1 -ioengine=libaio -bs=32M -rw=read -iodepth=64 -iodepth_batch_submit=8 -iodepth_batch_complete=8 -name=write_64k_64q</span></span><br><span class="line">write_64k_64q: (g=0): rw=<span class="built_in">read</span>, bs=32M-32M/32M-32M/32M-32M, ioengine=libaio, iodepth=64</span><br><span class="line">fio-2.2.8</span><br><span class="line">Starting 1 thread</span><br><span class="line">^Cbs: 1 (f=1): [R(1)] [inf% <span class="keyword">done</span>] [0KB/0KB/0KB /s] [0/0/0 iops] [eta 1158050441d:07h:00m:09s]</span><br></pre></td></tr></table></figure>
<p>bs=16M的dd hang住</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dd if=tstfile of=/dev/null bs=16M count=200</span></span><br><span class="line"></span><br><span class="line">root     12763  0.0  0.0 124344   664 pts/0    D+   14:29   0:00 dd <span class="keyword">if</span>=tstfile of=/dev/null bs=16M count=200</span><br></pre></td></tr></table></figure>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ceph -w</span></span><br><span class="line">    cluster 16f59233-16da-4864-8c5a-1fec71d119ad</span><br><span class="line">     health HEALTH_OK</span><br><span class="line">     monmap e1: 3 mons at &#123;cephcluster-server1=10.10.1.6:6789/0,cephcluster-server2=10.10.1.7:6789/0,cephcluster-server3=10.10.1.8:6789/0&#125;</span><br><span class="line">            election epoch 12, quorum 0,1,2 cephcluster-server1,cephcluster-server2,cephcluster-server3</span><br><span class="line">      fsmap e23: 1/1/1 up &#123;0=mds-daemon-37=up:active&#125;</span><br><span class="line">     osdmap e183: 30 osds: 30 up, 30 <span class="keyword">in</span></span><br><span class="line">            flags sortbitwise,require_jewel_osds</span><br><span class="line">      pgmap v63806: 1088 pgs, 3 pools, 93200 MB data, 13729 objects</span><br><span class="line">            274 GB used, 108 TB / 109 TB avail</span><br><span class="line">                1088 active+clean</span><br><span class="line">  client io 1330 MB/s rd, 41 op/s rd, 0 op/s wr</span><br><span class="line"></span><br><span class="line">2017-07-05 14:30:11.483974 mon.0 [INF] pgmap v63806: 1088 pgs: 1088 active+clean; 93200 MB data, 274 GB used, 108 TB / 109 TB avail; 1330 MB/s rd, 41 op/s</span><br><span class="line">2017-07-05 14:30:16.484856 mon.0 [INF] pgmap v63807: 1088 pgs: 1088 active+clean; 93200 MB data, 274 GB used, 108 TB / 109 TB avail; 1212 MB/s rd, 37 op/s</span><br><span class="line">2017-07-05 14:30:21.485295 mon.0 [INF] pgmap v63808: 1088 pgs: 1088 active+clean; 93200 MB data, 274 GB used, 108 TB / 109 TB avail; 1206 MB/s rd, 37 op/s</span><br><span class="line">2017-07-05 14:30:26.485953 mon.0 [INF] pgmap v63809: 1088 pgs: 1088 active+clean; 93200 MB data, 274 GB used, 108 TB / 109 TB avail; 1212 MB/s rd, 37 op/s</span><br></pre></td></tr></table></figure>
<h2 id="ceph-cluster节点信息"><a href="#ceph-cluster节点信息" class="headerlink" title="ceph cluster节点信息"></a>ceph cluster节点信息</h2><p>在ceph cluster的一个节点上看到测试节点有读数据请求</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># iftop -n -i eth4</span></span><br><span class="line">10.10.1.7               =&gt; 10.10.1.18                       12.3Mb  11.6Mb  11.6Mb</span><br><span class="line">                        &lt;=                                  285Kb   284Kb   284Kb</span><br><span class="line"></span><br><span class="line">打开<span class="built_in">source</span> port后</span><br><span class="line">10.10.1.7:6810          =&gt; 10.10.1.18                       12.5Mb  11.5Mb  11.5Mb</span><br><span class="line">                        &lt;=                                  278Kb   255Kb   255Kb</span><br><span class="line"></span><br><span class="line"><span class="comment"># netstat -nap | grep 6810</span></span><br><span class="line">tcp        0      0 10.10.1.7:6810      0.0.0.0:*           LISTEN      79360/ceph-osd</span><br><span class="line">tcp        0      0 10.10.1.7:6810      0.0.0.0:*           LISTEN      79360/ceph-osd</span><br><span class="line">tcp        0      0 10.10.1.7:6810      10.10.1.7:57266     ESTABLISHED 79360/ceph-osd</span><br><span class="line"></span><br><span class="line"><span class="comment"># ps aux | grep -w 79360</span></span><br><span class="line">ceph       79360 85.9  0.5 4338588 709312 ?      Ssl  Jul04 1556:54 /usr/bin/ceph-osd -f --cluster ceph --id 15 --setuser ceph --setgroup ceph</span><br><span class="line"></span><br><span class="line"><span class="comment"># df</span></span><br><span class="line">/dev/sdg1      3905070088  9079428 3895990660   1% /var/lib/ceph/osd/ceph-15</span><br></pre></td></tr></table></figure>
<p>查看sdg没有io</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># iostat -kx 2 sdg</span></span><br><span class="line">Linux 3.10.0-514.10.2.el7.x86_64 (cephcluster-server2.****.com)     07/05/2017     _x86_64_    (32 CPU)</span><br><span class="line"></span><br><span class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</span><br><span class="line">          12.47    0.00    1.64    0.03    0.00   85.86</span><br><span class="line"></span><br><span class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">sdg               0.01     0.07    2.48    2.01    95.78   140.98   105.44     0.08   17.79    9.69   27.79   3.44   1.54</span><br><span class="line"></span><br><span class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</span><br><span class="line">           0.80    0.00    2.96    0.02    0.00   96.22</span><br><span class="line"></span><br><span class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">sdg               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00</span><br><span class="line"></span><br><span class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</span><br><span class="line">           0.86    0.00    2.99    0.00    0.00   96.15</span><br><span class="line"></span><br><span class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">sdg               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00</span><br></pre></td></tr></table></figure>
<h2 id="查看文件location"><a href="#查看文件location" class="headerlink" title="查看文件location"></a>查看文件location</h2><p>测试文件的location信息，正好第一块数据就在osd 15上</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cephfs tstfile show_location</span></span><br><span class="line">WARNING: This tool is deprecated.  Use the layout.* xattrs to query and modify layouts.</span><br><span class="line">location.file_offset:  0</span><br><span class="line">location.object_offset:0</span><br><span class="line">location.object_no:    0</span><br><span class="line">location.object_size:  67108864</span><br><span class="line">location.object_name:  10000000805.00000000</span><br><span class="line">location.block_offset: 0</span><br><span class="line">location.block_size:   67108864</span><br><span class="line">location.osd:          15</span><br><span class="line"></span><br><span class="line"><span class="comment"># ceph osd map cephfs_data 10000000805.00000000</span></span><br><span class="line">osdmap e188 pool <span class="string">'cephfs_data'</span> (1) object <span class="string">'10000000805.00000000'</span> -&gt; pg 1.12d00fd5 (1.1d5) -&gt; up ([15,0,26], p15) acting ([15,0,26], p15)</span><br></pre></td></tr></table></figure>
<p>停止ceph osd 15</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># systemctl stop ceph-osd@15.service</span></span><br></pre></td></tr></table></figure>
<p>过一会查看osd 15所在节点没有与测试节点的数据流量了<br>这时在ceph osd 0节点，出现了与测试节点的数据流量，查看发现正好是osd 0与测试节点的数据通信<br>查看此时测试文件第一个object的map信息如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ceph osd map cephfs_data 10000000805.00000000</span></span><br><span class="line">osdmap e185 pool <span class="string">'cephfs_data'</span> (1) object <span class="string">'10000000805.00000000'</span> -&gt; pg 1.12d00fd5 (1.1d5) -&gt; up ([0,26], p0) acting ([0,26], p0)</span><br></pre></td></tr></table></figure>
<h2 id="问题明确"><a href="#问题明确" class="headerlink" title="问题明确"></a>问题明确</h2><p>现在这个问题很好重现了</p>
<ul>
<li>创建一个 <code>stripe_unit=67108864 stripe_count=1 object_size=67108864</code> 的文件</li>
<li>dd if=/dev/zero of=foxfile bs=64M count=1 写成功</li>
<li>dd if=foxfile of=/dev/null bs=64M count=1 读就会hang住</li>
<li>查看测试节点与对应的object所在的osd有数据流量</li>
</ul>
<p>通过<code>strace</code>命令查看dd读数据时确实hang在read函数：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># strace dd if=foxfile of=/dev/null bs=64M count=1</span></span><br><span class="line">...</span><br><span class="line">open(<span class="string">"foxfile"</span>, O_RDONLY) = 3</span><br><span class="line">dup2(3, 0) = 0</span><br><span class="line">close(3) = 0</span><br><span class="line">lseek(0, 0, SEEK_CUR) = 0</span><br><span class="line">open(<span class="string">"/dev/null"</span>, O_WRONLY|O_CREAT|O_TRUNC, 0666) = 3</span><br><span class="line">dup2(3, 1) = 1</span><br><span class="line">close(3) = 0</span><br><span class="line">mmap(NULL, 67121152, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f01712d8000</span><br><span class="line"><span class="built_in">read</span>(0,</span><br></pre></td></tr></table></figure>
<h2 id="收集client-log"><a href="#收集client-log" class="headerlink" title="收集client log"></a>收集client log</h2><p>参考文章 <a href="http://www.yangguanjun.com/2017/06/08/cephfs-client-debug/" target="_blank" rel="noopener">cephfs kernel client debug</a></p>
<h2 id="收集osd-log"><a href="#收集osd-log" class="headerlink" title="收集osd log"></a>收集osd log</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ceph daemon /var/run/ceph/ceph-osd.&lt;osdid&gt;.asok config set debug_osd "20/20"</span></span><br></pre></td></tr></table></figure>
<h2 id="分析log"><a href="#分析log" class="headerlink" title="分析log"></a>分析log</h2><p>在cephfs client的log中能循环发现如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Jul  6 11:30:29 cephfs-client kernel: [ 2010.231008] ceph_sock_data_ready on ffff88202c987030 state = 5, queueing work</span><br><span class="line">Jul  6 11:30:29 cephfs-client kernel: [ 2010.231012] get_osd ffff88202c987000 3 -&gt; 4</span><br><span class="line">Jul  6 11:30:29 cephfs-client kernel: [ 2010.231015] queue_con_delay ffff88202c987030 0</span><br><span class="line">Jul  6 11:30:29 cephfs-client kernel: [ 2010.231034] try_read start on ffff88202c987030 state 5</span><br><span class="line">Jul  6 11:30:29 cephfs-client kernel: [ 2010.231035] try_read tag 1 in_base_pos 0</span><br><span class="line">Jul  6 11:30:29 cephfs-client kernel: [ 2010.231036] try_read got tag 8</span><br><span class="line">Jul  6 11:30:29 cephfs-client kernel: [ 2010.231036] prepare_read_ack ffff88202c987030</span><br><span class="line">Jul  6 11:30:29 cephfs-client kernel: [ 2010.231041] ceph_sock_data_ready on ffff88202c987030 state = 5, queueing work</span><br><span class="line">Jul  6 11:30:29 cephfs-client kernel: [ 2010.231041] get_osd ffff88202c987000 4 -&gt; 5</span><br><span class="line">Jul  6 11:30:29 cephfs-client kernel: [ 2010.231053] queue_con_delay ffff88202c987030 0</span><br><span class="line">Jul  6 11:30:29 cephfs-client kernel: [ 2010.231062] got ack <span class="keyword">for</span> seq 1 <span class="built_in">type</span> 42 at ffff881008668100</span><br><span class="line">Jul  6 11:30:29 cephfs-client kernel: [ 2010.231066] ceph_msg_put ffff881008668100 (was 2)</span><br><span class="line">Jul  6 11:30:29 cephfs-client kernel: [ 2010.231081] prepare_read_tag ffff88202c987030</span><br><span class="line">Jul  6 11:30:29 cephfs-client kernel: [ 2010.231083] try_read start on ffff88202c987030 state 5</span><br><span class="line">Jul  6 11:30:29 cephfs-client kernel: [ 2010.231086] try_read tag 1 in_base_pos 0</span><br><span class="line">Jul  6 11:30:29 cephfs-client kernel: [ 2010.231088] try_read got tag 7</span><br><span class="line">Jul  6 11:30:29 cephfs-client kernel: [ 2010.231088] prepare_read_message ffff88202c987030</span><br><span class="line">Jul  6 11:30:29 cephfs-client kernel: [ 2010.231089] read_partial_message con ffff88202c987030 msg           (null)</span><br><span class="line">Jul  6 11:30:29 cephfs-client kernel: [ 2010.231090] try_read <span class="keyword">done</span> on ffff88202c987030 ret -5</span><br></pre></td></tr></table></figure>
<p>证明cephfs client一直在尝试读取osd数据，但收到数据后却认为msg为<code>null</code></p>
<p>在ceph osd的log中能循环发现如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">2017-07-06 11:30:29.185056 7f1788eb2700  1 osd.20 188 ms_handle_reset con 0x7f1748c39d80 session 0x7f170f4f31c0</span><br><span class="line">2017-07-06 11:30:29.185683 7f17719fc700 10 osd.20 188  new session 0x7f174d435380 con=0x7f1748c39f00 addr=10.10.1.18:0/144324133</span><br><span class="line">2017-07-06 11:30:29.185839 7f17719fc700 20 osd.20 188 should_share_map client.24357 10.10.1.18:0/144324133 188</span><br><span class="line">2017-07-06 11:30:29.185854 7f17719fc700 15 osd.20 188 enqueue_op 0x7f174d42f100 prio 127 cost 0 latency 0.000047 osd_op(client.24357.1:3 1.1bdf9243 1000000095b.00000000 [<span class="built_in">read</span> 0~67108864 [1@-1]] snapc 0=[] RETRY=6 ondisk+retry+<span class="built_in">read</span>+known_if_redirected e188) v4</span><br><span class="line">2017-07-06 11:30:29.185891 7f1777bff700 10 osd.20 188 dequeue_op 0x7f174d42f100 prio 127 cost 0 latency 0.000085 osd_op(client.24357.1:3 1.1bdf9243 1000000095b.00000000 [<span class="built_in">read</span> 0~67108864 [1@-1]] snapc 0=[] RETRY=6 ondisk+retry+<span class="built_in">read</span>+known_if_redirected e188) v4 pg pg[1.43( v 188<span class="string">'6597 (180'</span>3500,188<span class="string">'6597] local-les=157 n=10 ec=136 les/c/f 157/157/0 151/156/136) [20,10,7] r=0 lpr=156 crt=188'</span>6595 lcod 188<span class="string">'6596 mlcod 188'</span>6596 active+clean]</span><br><span class="line">2017-07-06 11:30:29.185937 7f1777bff700 10 osd.20 pg_epoch: 188 pg[1.43( v 188<span class="string">'6597 (180'</span>3500,188<span class="string">'6597] local-les=157 n=10 ec=136 les/c/f 157/157/0 151/156/136) [20,10,7] r=0 lpr=156 crt=188'</span>6595 lcod 188<span class="string">'6596 mlcod 188'</span>6596 active+clean] handle_message: 0x7f174d42f100</span><br><span class="line">2017-07-06 11:30:29.185951 7f1777bff700 20 osd.20 pg_epoch: 188 pg[1.43( v 188<span class="string">'6597 (180'</span>3500,188<span class="string">'6597] local-les=157 n=10 ec=136 les/c/f 157/157/0 151/156/136) [20,10,7] r=0 lpr=156 crt=188'</span>6595 lcod 188<span class="string">'6596 mlcod 188'</span>6596 active+clean] op_has_sufficient_caps pool=1 (cephfs_data ) owner=0 need_read_cap=1 need_write_cap=0 need_class_read_cap=0 need_class_write_cap=0 -&gt; yes</span><br><span class="line">2017-07-06 11:30:29.185970 7f1777bff700 10 osd.20 pg_epoch: 188 pg[1.43( v 188<span class="string">'6597 (180'</span>3500,188<span class="string">'6597] local-les=157 n=10 ec=136 les/c/f 157/157/0 151/156/136) [20,10,7] r=0 lpr=156 crt=188'</span>6595 lcod 188<span class="string">'6596 mlcod 188'</span>6596 active+clean] do_op osd_op(client.24357.1:3 1.1bdf9243 1000000095b.00000000 [<span class="built_in">read</span> 0~67108864 [1@-1]] snapc 0=[] RETRY=6 ondisk+retry+<span class="built_in">read</span>+known_if_redirected e188) v4 may_read -&gt; <span class="built_in">read</span>-ordered flags ondisk+retry+<span class="built_in">read</span>+known_if_redirected</span><br><span class="line">2017-07-06 11:30:29.185990 7f1777bff700 10 osd.20 pg_epoch: 188 pg[1.43( v 188<span class="string">'6597 (180'</span>3500,188<span class="string">'6597] local-les=157 n=10 ec=136 les/c/f 157/157/0 151/156/136) [20,10,7] r=0 lpr=156 crt=188'</span>6595 lcod 188<span class="string">'6596 mlcod 188'</span>6596 active+clean] get_object_context: found obc <span class="keyword">in</span> cache: 0x7f171e423180</span><br><span class="line">2017-07-06 11:30:29.185997 7f1777bff700 10 osd.20 pg_epoch: 188 pg[1.43( v 188<span class="string">'6597 (180'</span>3500,188<span class="string">'6597] local-les=157 n=10 ec=136 les/c/f 157/157/0 151/156/136) [20,10,7] r=0 lpr=156 crt=188'</span>6595 lcod 188<span class="string">'6596 mlcod 188'</span>6596 active+clean] get_object_context: 0x7f171e423180 1:c249fbd8:::1000000095b.00000000:head rwstate(none n=0 w=0) oi: 1:c249fbd8:::1000000095b.00000000:head(188<span class="string">'4499 mds.0.20:66259 dirty|data_digest|omap_digest s 67108864 uv 4499 dd cdba94a2 od ffffffff) ssc: 0x7f172dcfce40 snapset: 1=[]:[]+head</span></span><br><span class="line"><span class="string">2017-07-06 11:30:29.186010 7f1777bff700 10 osd.20 pg_epoch: 188 pg[1.43( v 188'</span>6597 (180<span class="string">'3500,188'</span>6597] <span class="built_in">local</span>-les=157 n=10 ec=136 les/c/f 157/157/0 151/156/136) [20,10,7] r=0 lpr=156 crt=188<span class="string">'6595 lcod 188'</span>6596 mlcod 188<span class="string">'6596 active+clean] find_object_context 1:c249fbd8:::1000000095b.00000000:head @head oi=1:c249fbd8:::1000000095b.00000000:head(188'</span>4499 mds.0.20:66259 dirty|data_digest|omap_digest s 67108864 uv 4499 dd cdba94a2 od ffffffff)</span><br><span class="line">2017-07-06 11:30:29.186026 7f1777bff700 10 osd.20 pg_epoch: 188 pg[1.43( v 188<span class="string">'6597 (180'</span>3500,188<span class="string">'6597] local-les=157 n=10 ec=136 les/c/f 157/157/0 151/156/136) [20,10,7] r=0 lpr=156 crt=188'</span>6595 lcod 188<span class="string">'6596 mlcod 188'</span>6596 active+clean] execute_ctx 0x7f1776beed00</span><br><span class="line">2017-07-06 11:30:29.186035 7f1777bff700 10 osd.20 pg_epoch: 188 pg[1.43( v 188<span class="string">'6597 (180'</span>3500,188<span class="string">'6597] local-les=157 n=10 ec=136 les/c/f 157/157/0 151/156/136) [20,10,7] r=0 lpr=156 crt=188'</span>6595 lcod 188<span class="string">'6596 mlcod 188'</span>6596 active+clean] do_op 1:c249fbd8:::1000000095b.00000000:head [<span class="built_in">read</span> 0~67108864 [1@-1]] ov 188<span class="string">'4499</span></span><br><span class="line"><span class="string">2017-07-06 11:30:29.186041 7f1777bff700 10 osd.20 pg_epoch: 188 pg[1.43( v 188'</span>6597 (180<span class="string">'3500,188'</span>6597] <span class="built_in">local</span>-les=157 n=10 ec=136 les/c/f 157/157/0 151/156/136) [20,10,7] r=0 lpr=156 crt=188<span class="string">'6595 lcod 188'</span>6596 mlcod 188<span class="string">'6596 active+clean]  taking ondisk_read_lock</span></span><br><span class="line"><span class="string">2017-07-06 11:30:29.186047 7f1777bff700 10 osd.20 pg_epoch: 188 pg[1.43( v 188'</span>6597 (180<span class="string">'3500,188'</span>6597] <span class="built_in">local</span>-les=157 n=10 ec=136 les/c/f 157/157/0 151/156/136) [20,10,7] r=0 lpr=156 crt=188<span class="string">'6595 lcod 188'</span>6596 mlcod 188<span class="string">'6596 active+clean] do_osd_op 1:c249fbd8:::1000000095b.00000000:head [read 0~67108864 [1@-1]]</span></span><br><span class="line"><span class="string">2017-07-06 11:30:29.186056 7f1777bff700 10 osd.20 pg_epoch: 188 pg[1.43( v 188'</span>6597 (180<span class="string">'3500,188'</span>6597] <span class="built_in">local</span>-les=157 n=10 ec=136 les/c/f 157/157/0 151/156/136) [20,10,7] r=0 lpr=156 crt=188<span class="string">'6595 lcod 188'</span>6596 mlcod 188<span class="string">'6596 active+clean] do_osd_op  read 0~67108864 [1@-1]</span></span><br></pre></td></tr></table></figure>
<p>分析看出cephfs client发送read 64MB的request，ceph osd读64M数据return，但是cephfs client认为msg为null，然后重试<br>一直重复上面的请求，所以cephfs client的IO就hang住了</p>
<h2 id="提交bug"><a href="#提交bug" class="headerlink" title="提交bug"></a>提交bug</h2><p>提交该bug到ceph社区：<br><a href="http://tracker.ceph.com/issues/20528" target="_blank" rel="noopener">http://tracker.ceph.com/issues/20528</a></p>
<p>cephfs client端的配置限制了read message的最大size为16M。</p>
<p>in net/ceph/messenger.c</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">static int read_partial_message(struct ceph_connection *con)</span><br><span class="line">&#123;</span><br><span class="line">...</span><br><span class="line">        front_len = le32_to_cpu(con-&gt;in_hdr.front_len);</span><br><span class="line">        <span class="keyword">if</span> (front_len &gt; CEPH_MSG_MAX_FRONT_LEN)</span><br><span class="line">                <span class="built_in">return</span> -EIO;</span><br><span class="line">        middle_len = le32_to_cpu(con-&gt;in_hdr.middle_len);</span><br><span class="line">        <span class="keyword">if</span> (middle_len &gt; CEPH_MSG_MAX_MIDDLE_LEN)</span><br><span class="line">                <span class="built_in">return</span> -EIO;</span><br><span class="line">        data_len = le32_to_cpu(con-&gt;in_hdr.data_len);</span><br><span class="line">        <span class="keyword">if</span> (data_len &gt; CEPH_MSG_MAX_DATA_LEN)</span><br><span class="line">                <span class="built_in">return</span> -EIO;</span><br></pre></td></tr></table></figure>
<p>所以我们使用cephfs中，不能配置文件的<code>stripe_unit</code>大于16M。</p>
</span>
      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/ceph/" rel="tag">#ceph</a>
          
            <a href="/tags/cephfs/" rel="tag">#cephfs</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/07/27/2017-07-27-compare-mysql-on-localdisk-rbddisk/" rel="prev">Mysql performance test on localdisk and rbd</a>
            
          </div>

          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/07/16/2017-07-16-cephfs-test-method-lite/" rel="next">cephfs的测试简报</a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div>
      
        <style type="text/css">

    .donate_bar {
        text-align: center;
        margin-top : 5%;
    }

    .donate_bar.hidden {
        display:none;
    }
/*
    .donate_bar a.btn_donate {
        display: inline-block;
        width: 82px;
        height: 82px;
        margin-left:auto;
        margin-right:auto;

        background: url("http://img.t.sinajs.cn/t5/style/images/apps_PRF/e_media/btn_reward.gif") no-repeat;
        _background: url("http://img.t.sinajs.cn/t5/style/images/apps_PRF/e_media/btn_reward.gif") no-repeat; 

        -webkit-transition: background 0s;
        -moz-transition: background 0s;
        -o-transition: background 0s;
        -ms-transition: background 0s;
        transition: background 0s;
    }
*/
    .donate_bar a.btn_donate:hover { 
        // background-position: 0px -82px;
        color: #87daff;
    }

    .donate_bar .donate_txt {
        display: block;
        color: #9d9d9d;
        font: 14px/2 "Microsoft Yahei";
    }

    .bold { 
        font-weight: bold; 
    }

    .post-donate a {
        border-bottom: 0px;
    }

    #donate_guide table {
        border: none;
    }

    #donate_guide td {
        border-bottom: none;
        border-right: none;
        // background: #333333;
        valign: top;
    }

</style>



    

    <div class ="post-donate">
        <div id="donate_board" class="donate_bar center">
              <a id="btn_donate" class="btn_donate" href="javascript:;" title="打赏">赏</a>
              <span id="donate_txt" class="donate_txt">
                   
                        仅仅是一个功能
                   
              </span>
            <br>
        </div>  
  
        <div id="donate_guide" class="donate_bar center hidden">
            <!--
            
                <a href="http://o7keinrz4.bkt.clouddn.com/alipay.jpg" title="支付宝打赏" class="fancybox" rel="article0" 
                    style="float:left;margin-left:25%;margin-right:10px;">
                    <img src="http://o7keinrz4.bkt.clouddn.com/alipay.jpg" title="" height="164px" width="164px">
                </a> 
              

            
                <a href="http://o7keinrz4.bkt.clouddn.com/wechat.jpg" title="微信打赏" class="fancybox" rel="article0"
                    style="margin-right:30%">
                    <img src="http://o7keinrz4.bkt.clouddn.com/wechat.jpg" title="" height="164px" width="164px">
                </a>
            
            -->
            <table>
                <tr>
                    <td>
                        
                            <a href="http://o7keinrz4.bkt.clouddn.com/alipay.jpg" title="支付宝打赏" class="fancybox" rel="article0" 
                                style="float:left;margin-left:25%;margin-right:10px;">
                                <img src="http://o7keinrz4.bkt.clouddn.com/alipay.jpg" title="" height="164px" width="164px">
                            </a> 
                         
                    </td>
                    <td>
                        
                            <a href="http://o7keinrz4.bkt.clouddn.com/wechat.jpg" title="微信打赏" class="fancybox" rel="article0"
                                style="margin-right:30%">
                                <img src="http://o7keinrz4.bkt.clouddn.com/wechat.jpg" title="" height="164px" width="164px">
                            </a>
                        
                    </td>
                </tr>
            </table>

        </div>

        <script type="text/javascript">
            document.getElementById('btn_donate').onclick = function() {
                $('#donate_board').addClass('hidden');
                // $('#donate_guide').removeClass('hidden');
                $('#donate_guide').show(2000);
            }

        </script>
    </div>

    


      
    </div>

    <div class="post-spread">
      
        <div class="bdsharebuttonbox">
	<a href="#" class="bds_more" data-cmd="more"></a>
	<a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a>
	<a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
	<a href="#" class="bds_sqq" data-cmd="sqq" title="分享到QQ好友"></a>
	<a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a>
	<a href="#" class="bds_tqq" data-cmd="tqq" title="分享到腾讯微博"></a>
	<a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a>
</div>
<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"16"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];</script>

      
    </div>
  </div>

 </div>

        

        
          <div class="comments" id="comments">
            
          </div>
        
      </div>

      
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table Of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" src="/images/default_avatar.jpg" alt="ictfox" itemprop="image"/>
          <p class="site-author-name" itemprop="name">ictfox</p>
        </div>
        <p class="site-description motion-element" itemprop="description">学无止境</p>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">71</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          <div class="site-state-item site-state-categories">
            <a href="/categories">
              <span class="site-state-item-count">10</span>
              <span class="site-state-item-name">categories</span>
              </a>
          </div>

          <div class="site-state-item site-state-tags">
            <a href="/tags">
              <span class="site-state-item-count">48</span>
              <span class="site-state-item-name">tags</span>
              </a>
          </div>

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        <div class="links-of-friendly motion-element">
          
        </div>

        
        

      </section>

      
        <section class="post-toc-wrap sidebar-panel-active">
          <div class="post-toc-indicator-top post-toc-indicator"></div>
          <div class="post-toc">
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#问题"><span class="nav-number">1.</span> <span class="nav-text">问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ceph输出"><span class="nav-number">2.</span> <span class="nav-text">ceph输出</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#查看网络"><span class="nav-number">3.</span> <span class="nav-text">查看网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#对比几个stripe"><span class="nav-number">4.</span> <span class="nav-text">对比几个stripe</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ceph-cluster节点信息"><span class="nav-number">5.</span> <span class="nav-text">ceph cluster节点信息</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#查看文件location"><span class="nav-number">6.</span> <span class="nav-text">查看文件location</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#问题明确"><span class="nav-number">7.</span> <span class="nav-text">问题明确</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#收集client-log"><span class="nav-number">8.</span> <span class="nav-text">收集client log</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#收集osd-log"><span class="nav-number">9.</span> <span class="nav-text">收集osd log</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#分析log"><span class="nav-number">10.</span> <span class="nav-text">分析log</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#提交bug"><span class="nav-number">11.</span> <span class="nav-text">提交bug</span></a></li></ol></div>
            
          </div>
          <div class="post-toc-indicator-bottom post-toc-indicator"></div>
        </section>
      

    </div>
  </aside>


    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner"> <div class="copyright" >
  
  &copy; &nbsp; 
  <span itemprop="copyrightYear">2018
  </span>
  <span class="with-love">
    <i class="icon-next-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ictfox
  </span>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="http://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme by <a class="theme-link" href="http://blog.idhyt.com">idhyt</a>.<a class="theme-link" href="https://github.com/idhyt/hexo-theme-next/tree/magiclamp">Mala</a>
</div>

<!-- busuanzi -->



 </div>
    </footer>

    <div class="back-to-top"></div>
  </div>

  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  
  
    
    

  


  
  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/fancy-box.js?v=0.4.5.1"></script>


  <script type="text/javascript" src="/js/helpers.js?v=0.4.5.1"></script>
  

  <script type="text/javascript" src="/vendors/velocity/velocity.min.js"></script>
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js"></script>

  <script type="text/javascript" src="/js/motion_global.js?v=0.4.5.1" id="motion.global"></script>




  <script type="text/javascript" src="/js/nav-toggle.js?v=0.4.5.1"></script>
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  
<script type="text/javascript" src="/js/bootstrap.scrollspy.js?v=0.4.5.1" id="bootstrap.scrollspy.custom"></script>


<script type="text/javascript" id="sidebar.toc.highlight">
  $(document).ready(function () {
    var tocSelector = '.post-toc';
    var $tocSelector = $(tocSelector);
    var activeCurrentSelector = '.active-current';

    $tocSelector
      .on('activate.bs.scrollspy', function () {
        var $currentActiveElement = $(tocSelector + ' .active').last();

        removeCurrentActiveClass();
        $currentActiveElement.addClass('active-current');

        $tocSelector[0].scrollTop = $currentActiveElement.position().top;
      })
      .on('clear.bs.scrollspy', function () {
        removeCurrentActiveClass();
      });

    function removeCurrentActiveClass () {
      $(tocSelector + ' ' + activeCurrentSelector)
        .removeClass(activeCurrentSelector.substring(1));
    }

    function processTOC () {
      getTOCMaxHeight();
      toggleTOCOverflowIndicators();
    }

    function getTOCMaxHeight () {
      var height = $('.sidebar').height() -
                   $tocSelector.position().top -
                   $('.post-toc-indicator-bottom').height();

      $tocSelector.css('height', height);

      return height;
    }

    function toggleTOCOverflowIndicators () {
      tocOverflowIndicator(
        '.post-toc-indicator-top',
        $tocSelector.scrollTop() > 0 ? 'show' : 'hide'
      );

      tocOverflowIndicator(
        '.post-toc-indicator-bottom',
        $tocSelector.scrollTop() >= $tocSelector.find('ol').height() - $tocSelector.height() ? 'hide' : 'show'
      )
    }

    $(document).on('sidebar.motion.complete', function () {
      processTOC();
    });

    $('body').scrollspy({ target: tocSelector });
    $(window).on('resize', function () {
      if ( $('.sidebar').hasClass('sidebar-active') ) {
        processTOC();
      }
    });

    onScroll($tocSelector);

    function onScroll (element) {
      element.on('mousewheel DOMMouseScroll', function (event) {
          var oe = event.originalEvent;
          var delta = oe.wheelDelta || -oe.detail;

          this.scrollTop += ( delta < 0 ? 1 : -1 ) * 30;
          event.preventDefault();

          toggleTOCOverflowIndicators();
      });
    }

    function tocOverflowIndicator (indicator, action) {
      var $indicator = $(indicator);
      var opacity = action === 'show' ? 0.4 : 0;
      $indicator.velocity ?
        $indicator.velocity('stop').velocity({
          opacity: opacity
        }, { duration: 100 }) :
        $indicator.stop().animate({
          opacity: opacity
        }, 100);
    }

  });
</script>

<script type="text/javascript" id="sidebar.nav">
  $(document).ready(function () {
    var html = $('html');
    var TAB_ANIMATE_DURATION = 200;
    var hasVelocity = $.isFunction(html.velocity);

    $('.sidebar-nav li').on('click', function () {
      var item = $(this);
      var activeTabClassName = 'sidebar-nav-active';
      var activePanelClassName = 'sidebar-panel-active';
      if (item.hasClass(activeTabClassName)) {
        return;
      }

      var currentTarget = $('.' + activePanelClassName);
      var target = $('.' + item.data('target'));

      hasVelocity ?
        currentTarget.velocity('transition.slideUpOut', TAB_ANIMATE_DURATION, function () {
          target
            .velocity('stop')
            .velocity('transition.slideDownIn', TAB_ANIMATE_DURATION)
            .addClass(activePanelClassName);
        }) :
        currentTarget.animate({ opacity: 0 }, TAB_ANIMATE_DURATION, function () {
          currentTarget.hide();
          target
            .stop()
            .css({'opacity': 0, 'display': 'block'})
            .animate({ opacity: 1 }, TAB_ANIMATE_DURATION, function () {
              currentTarget.removeClass(activePanelClassName);
              target.addClass(activePanelClassName);
            });
        });

      item.siblings().removeClass(activeTabClassName);
      item.addClass(activeTabClassName);
    });

    $('.post-toc a').on('click', function (e) {
      e.preventDefault();
      var targetSelector = escapeSelector(this.getAttribute('href'));
      var offset = $(targetSelector).offset().top;
      hasVelocity ?
        html.velocity('stop').velocity('scroll', {
          offset: offset  + 'px',
          mobileHA: false
        }) :
        $('html, body').stop().animate({
          scrollTop: offset
        }, 500);
    });

    // Expand sidebar on post detail page by default, when post has a toc.
    var $tocContent = $('.post-toc-content');
    if (isDesktop() && CONFIG.sidebar === 'post') {
      if ($tocContent.length > 0 && $tocContent.html().trim().length > 0) {
        displaySidebar();
      }
    }
  });
</script>



  <script type="text/javascript">
    $(document).ready(function () {
      if (CONFIG.sidebar === 'always') {
        displaySidebar();
      }
      if (isMobile()) {
        FastClick.attach(document.body);
      }
    });
  </script>

  

  
  

  
  <script type="text/javascript" src="/js/lazyload.js"></script>
  <script type="text/javascript">
    $(function () {
      $("#posts").find('img').lazyload({
        placeholder: "/images/loading.gif",
        effect: "fadeIn"
      });
    });
  </script>
</body>
</html>
